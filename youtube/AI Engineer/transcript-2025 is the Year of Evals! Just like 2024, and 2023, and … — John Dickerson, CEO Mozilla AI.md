# 2025 is the Year of Evals! Just like 2024, and 2023, and … — John Dickerson, CEO Mozilla AI

**Video URL:** https://www.youtube.com/watch?v=CQGuvf6gSrM

---

## Full Transcript

### [00:00 - 01:00]

**[00:17]** Thanks everyone for being here. Um I'm

**[00:17]** Thanks everyone for being here. Um I'm going to give this talk mostly from the

**[00:19]** going to give this talk mostly from the

**[00:19]** going to give this talk mostly from the uh point of view of being a co-founder

**[00:21]** uh point of view of being a co-founder

**[00:21]** uh point of view of being a co-founder and chief scientist at Arthur AAI where

**[00:23]** and chief scientist at Arthur AAI where

**[00:23]** and chief scientist at Arthur AAI where for six years prior to joining Mosilla

**[00:25]** for six years prior to joining Mosilla

**[00:25]** for six years prior to joining Mosilla AI as CEO. Um, I do want to say Mozilla

**[00:28]** AI as CEO. Um, I do want to say Mozilla

**[00:28]** AI as CEO. Um, I do want to say Mozilla AI operates in the open source world

**[00:30]** AI operates in the open source world

**[00:30]** AI operates in the open source world where we're providing open source AI

**[00:32]** where we're providing open source AI

**[00:32]** where we're providing open source AI tooling and we're supporting the open

**[00:33]** tooling and we're supporting the open

**[00:33]** tooling and we're supporting the open source AI stack. Our end goal is to uh

**[00:36]** source AI stack. Our end goal is to uh

**[00:36]** source AI stack. Our end goal is to uh enable the open source community to be

**[00:37]** enable the open source community to be

**[00:37]** enable the open source community to be at the same table as a Sam Alman when

**[00:39]** at the same table as a Sam Alman when

**[00:40]** at the same table as a Sam Alman when talking about AI moving forward. So, if

**[00:42]** talking about AI moving forward. So, if

**[00:42]** talking about AI moving forward. So, if you're interested in that, that's not

**[00:43]** you're interested in that, that's not

**[00:43]** you're interested in that, that's not what this talk is going to be about, but

**[00:44]** what this talk is going to be about, but

**[00:44]** what this talk is going to be about, but we can talk about that offline. This

**[00:46]** we can talk about that offline. This

**[00:46]** we can talk about that offline. This talk is going to be about 2025 finally

**[00:48]** talk is going to be about 2025 finally

**[00:48]** talk is going to be about 2025 finally being the year of the evals. And as was

**[00:51]** being the year of the evals. And as was

**[00:52]** being the year of the evals. And as was written uh written as was spoken uh by

**[00:54]** written uh written as was spoken uh by

**[00:54]** written uh written as was spoken uh by my introduction, I've been in the space

**[00:55]** my introduction, I've been in the space

**[00:55]** my introduction, I've been in the space for a very long time. Arthur AI for

**[00:57]** for a very long time. Arthur AI for

**[00:57]** for a very long time. Arthur AI for example was in and still is in obser


### [01:00 - 02:00]

**[01:00]** example was in and still is in obser

**[01:00]** example was in and still is in obser observability evaluation and security in

**[01:02]** observability evaluation and security in

**[01:02]** observability evaluation and security in both traditional ML and AI and then into

**[01:05]** both traditional ML and AI and then into

**[01:05]** both traditional ML and AI and then into the deep learning revolution and then

**[01:06]** the deep learning revolution and then

**[01:06]** the deep learning revolution and then into the genai revolution and then into

**[01:08]** into the genai revolution and then into

**[01:08]** into the genai revolution and then into the agentic revolution. And I think

**[01:09]** the agentic revolution. And I think

**[01:09]** the agentic revolution. And I think we're finally at the point where uh all

**[01:11]** we're finally at the point where uh all

**[01:11]** we're finally at the point where uh all of these companies are going to start

**[01:12]** of these companies are going to start

**[01:12]** of these companies are going to start seeing hockey stick growth which is

**[01:14]** seeing hockey stick growth which is

**[01:14]** seeing hockey stick growth which is exciting.

**[01:16]** exciting.

**[01:16]** exciting. So the thesis of this talk, one thing is

**[01:18]** So the thesis of this talk, one thing is

**[01:18]** So the thesis of this talk, one thing is that uh I I see a IML monitoring and

**[01:20]** that uh I I see a IML monitoring and

**[01:20]** that uh I I see a IML monitoring and evaluation as two sides of the same

**[01:23]** evaluation as two sides of the same

**[01:23]** evaluation as two sides of the same sword or ruler, right? You can't do

**[01:24]** sword or ruler, right? You can't do

**[01:24]** sword or ruler, right? You can't do monitoring or observability without

**[01:26]** monitoring or observability without

**[01:26]** monitoring or observability without being able to measure and measurement is

**[01:29]** being able to measure and measurement is

**[01:29]** being able to measure and measurement is uh the core functionality for

**[01:30]** uh the core functionality for

**[01:30]** uh the core functionality for evaluation.

**[01:32]** evaluation.

**[01:32]** evaluation. This was not top of mind really with the

**[01:35]** This was not top of mind really with the

**[01:35]** This was not top of mind really with the seuite uh until two things happened

**[01:38]** seuite uh until two things happened

**[01:38]** seuite uh until two things happened concurrently.

**[01:39]** concurrently.

**[01:39]** concurrently. One is AI became a thing that people who

**[01:43]** One is AI became a thing that people who

**[01:43]** One is AI became a thing that people who aren't a CIO or CTO could understand. So

**[01:46]** aren't a CIO or CTO could understand. So

**[01:46]** aren't a CIO or CTO could understand. So the CEOs, the CFOs, the CISOs began to

**[01:50]** the CEOs, the CFOs, the CISOs began to

**[01:50]** the CEOs, the CFOs, the CISOs began to understand it basically when ChatGBT

**[01:52]** understand it basically when ChatGBT

**[01:52]** understand it basically when ChatGBT came out. And simultaneously

**[01:54]** came out. And simultaneously

**[01:54]** came out. And simultaneously there was a perfectly timed budget

**[01:55]** there was a perfectly timed budget

**[01:56]** there was a perfectly timed budget freeze across enterprise at least in the

**[01:57]** freeze across enterprise at least in the

**[01:57]** freeze across enterprise at least in the US that happened due to a fear of an

**[01:59]** US that happened due to a fear of an

**[01:59]** US that happened due to a fear of an impending recession. So this is right


### [02:00 - 03:00]

**[02:01]** impending recession. So this is right

**[02:01]** impending recession. So this is right before chat GBT launched. This is like

**[02:03]** before chat GBT launched. This is like

**[02:03]** before chat GBT launched. This is like October, November when most enterprises

**[02:05]** October, November when most enterprises

**[02:05]** October, November when most enterprises would set up budget for the next year.

**[02:07]** would set up budget for the next year.

**[02:07]** would set up budget for the next year. At that time there was a freeze except

**[02:10]** At that time there was a freeze except

**[02:10]** At that time there was a freeze except for money that could be opened up for a

**[02:13]** for money that could be opened up for a

**[02:13]** for money that could be opened up for a specific pet project and that PET

**[02:14]** specific pet project and that PET

**[02:14]** specific pet project and that PET project because CEOs and CFOs then knew

**[02:16]** project because CEOs and CFOs then knew

**[02:16]** project because CEOs and CFOs then knew about it uh was Gen AI.

**[02:19]** about it uh was Gen AI.

**[02:19]** about it uh was Gen AI. So that happened and then now we have

**[02:22]** So that happened and then now we have

**[02:22]** So that happened and then now we have the final sort of uh vertex on this

**[02:24]** the final sort of uh vertex on this

**[02:24]** the final sort of uh vertex on this triangle which is going to force

**[02:26]** triangle which is going to force

**[02:26]** triangle which is going to force evaluation to be top of mind this year

**[02:28]** evaluation to be top of mind this year

**[02:28]** evaluation to be top of mind this year which is that we have systems that are

**[02:29]** which is that we have systems that are

**[02:29]** which is that we have systems that are now acting for

**[02:32]** now acting for

**[02:32]** now acting for uh humans acting for teams as opposed to

**[02:35]** uh humans acting for teams as opposed to

**[02:35]** uh humans acting for teams as opposed to just providing inputs into larger

**[02:37]** just providing inputs into larger

**[02:37]** just providing inputs into larger systems. So these three things together

**[02:39]** systems. So these three things together

**[02:39]** systems. So these three things together are as you saw with Brain Trust, as

**[02:41]** are as you saw with Brain Trust, as

**[02:41]** are as you saw with Brain Trust, as we're seeing at Arthur, as you're seeing

**[02:42]** we're seeing at Arthur, as you're seeing

**[02:42]** we're seeing at Arthur, as you're seeing with like Arise AI or Galileo, other big

**[02:45]** with like Arise AI or Galileo, other big

**[02:45]** with like Arise AI or Galileo, other big players in this space, we're starting to

**[02:47]** players in this space, we're starting to

**[02:47]** players in this space, we're starting to see big takeoff because of this.

**[02:50]** see big takeoff because of this.

**[02:50]** see big takeoff because of this. Cool. So right now, this is what's

**[02:52]** Cool. So right now, this is what's

**[02:52]** Cool. So right now, this is what's happening. Year of the agent, we all

**[02:54]** happening. Year of the agent, we all

**[02:54]** happening. Year of the agent, we all hear about it. We're hearing about it

**[02:55]** hear about it. We're hearing about it

**[02:55]** hear about it. We're hearing about it here at this conference. Agents are

**[02:57]** here at this conference. Agents are

**[02:57]** here at this conference. Agents are starting to make decisions and take

**[02:58]** starting to make decisions and take

**[02:58]** starting to make decisions and take actions, complex steps that lead toward


### [03:00 - 04:00]

**[03:01]** actions, complex steps that lead toward

**[03:01]** actions, complex steps that lead toward an action, either autonomously or

**[03:03]** an action, either autonomously or

**[03:03]** an action, either autonomously or semi-autonomously. uh as a question on

**[03:06]** semi-autonomously. uh as a question on

**[03:06]** semi-autonomously. uh as a question on the last talk um um uh brought up you

**[03:08]** the last talk um um uh brought up you

**[03:08]** the last talk um um uh brought up you know bringing humans into the loop is

**[03:10]** know bringing humans into the loop is

**[03:10]** know bringing humans into the loop is still obviously a very good idea on many

**[03:11]** still obviously a very good idea on many

**[03:11]** still obviously a very good idea on many systems but we're getting closer and

**[03:13]** systems but we're getting closer and

**[03:13]** systems but we're getting closer and closer to full automation

**[03:15]** closer to full automation

**[03:15]** closer to full automation and agentic systems are going into

**[03:17]** and agentic systems are going into

**[03:17]** and agentic systems are going into deployment now okay and that's in

**[03:19]** deployment now okay and that's in

**[03:19]** deployment now okay and that's in enterprises that's in SMBs that's in pet

**[03:21]** enterprises that's in SMBs that's in pet

**[03:21]** enterprises that's in SMBs that's in pet projects and so on and what that means

**[03:23]** projects and so on and what that means

**[03:23]** projects and so on and what that means is by that last slide this is also the

**[03:26]** is by that last slide this is also the

**[03:26]** is by that last slide this is also the year that we need

**[03:29]** year that we need

**[03:29]** year that we need flash back uh to then up to like a year

**[03:32]** flash back uh to then up to like a year

**[03:32]** flash back uh to then up to like a year ago where every year we were asking hey

**[03:34]** ago where every year we were asking hey

**[03:34]** ago where every year we were asking hey is this the year of ML? Is this the year

**[03:36]** is this the year of ML? Is this the year

**[03:36]** is this the year of ML? Is this the year of evaluations? And prior to sort of

**[03:40]** of evaluations? And prior to sort of

**[03:40]** of evaluations? And prior to sort of these agentic systems coming out, we

**[03:42]** these agentic systems coming out, we

**[03:42]** these agentic systems coming out, we would have machine learning models

**[03:43]** would have machine learning models

**[03:43]** would have machine learning models basically spitting out numbers that

**[03:44]** basically spitting out numbers that

**[03:44]** basically spitting out numbers that would then be ingested into a more

**[03:46]** would then be ingested into a more

**[03:46]** would then be ingested into a more complex system. And that complexity uh

**[03:49]** complex system. And that complexity uh

**[03:49]** complex system. And that complexity uh would sort of erase uh the top of mind

**[03:52]** would sort of erase uh the top of mind

**[03:52]** would sort of erase uh the top of mind need to think about what's coming out of

**[03:53]** need to think about what's coming out of

**[03:53]** need to think about what's coming out of the model itself. Except for people in

**[03:55]** the model itself. Except for people in

**[03:55]** the model itself. Except for people in this audience, we know that's very

**[03:56]** this audience, we know that's very

**[03:56]** this audience, we know that's very important. But when it comes to decision

**[03:57]** important. But when it comes to decision

**[03:58]** important. But when it comes to decision makers, it would often get wrapped up

**[03:59]** makers, it would often get wrapped up

**[03:59]** makers, it would often get wrapped up into this sort of opaque box.


### [04:00 - 05:00]

**[04:02]** into this sort of opaque box.

**[04:02]** into this sort of opaque box. And that meant that the ML part didn't

**[04:04]** And that meant that the ML part didn't

**[04:04]** And that meant that the ML part didn't really bubble up beyond like the CIO or

**[04:06]** really bubble up beyond like the CIO or

**[04:06]** really bubble up beyond like the CIO or whatever or the system was going into

**[04:08]** whatever or the system was going into

**[04:08]** whatever or the system was going into which meant that typically that year was

**[04:10]** which meant that typically that year was

**[04:10]** which meant that typically that year was not the year for EVEL because the need

**[04:12]** not the year for EVEL because the need

**[04:12]** not the year for EVEL because the need for eval was not obvious to the entire

**[04:14]** for eval was not obvious to the entire

**[04:14]** for eval was not obvious to the entire seuite.

**[04:17]** seuite.

**[04:17]** seuite. Cool. So let's take a quick step back in

**[04:19]** Cool. So let's take a quick step back in

**[04:19]** Cool. So let's take a quick step back in time. Before November 30th of 2022, the

**[04:22]** time. Before November 30th of 2022, the

**[04:22]** time. Before November 30th of 2022, the chatbt launch, uh, ML monitoring was

**[04:24]** chatbt launch, uh, ML monitoring was

**[04:24]** chatbt launch, uh, ML monitoring was certainly a thing, right? Like data

**[04:25]** certainly a thing, right? Like data

**[04:25]** certainly a thing, right? Like data science teams have long used statistical

**[04:27]** science teams have long used statistical

**[04:27]** science teams have long used statistical methods as part of larger systems to

**[04:29]** methods as part of larger systems to

**[04:29]** methods as part of larger systems to understand what's going on, right? This

**[04:30]** understand what's going on, right? This

**[04:30]** understand what's going on, right? This is this is core. Um, like I mentioned

**[04:33]** is this is core. Um, like I mentioned

**[04:33]** is this is core. Um, like I mentioned though, there's a tenuous connection to

**[04:34]** though, there's a tenuous connection to

**[04:34]** though, there's a tenuous connection to sort of downstream business KPIs. And at

**[04:37]** sort of downstream business KPIs. And at

**[04:37]** sort of downstream business KPIs. And at the end of the day, that's what gets

**[04:38]** the end of the day, that's what gets

**[04:38]** the end of the day, that's what gets your product bought in the enterprise is

**[04:39]** your product bought in the enterprise is

**[04:39]** your product bought in the enterprise is being able to make a sale about dollars

**[04:41]** being able to make a sale about dollars

**[04:41]** being able to make a sale about dollars saved or about dollars earned. So being

**[04:43]** saved or about dollars earned. So being

**[04:43]** saved or about dollars earned. So being able to connect machine learning the

**[04:45]** able to connect machine learning the

**[04:45]** able to connect machine learning the components specifically to a downstream

**[04:47]** components specifically to a downstream

**[04:47]** components specifically to a downstream business KPI

**[04:50]** business KPI

**[04:50]** business KPI there was a lot of lip service around a

**[04:52]** there was a lot of lip service around a

**[04:52]** there was a lot of lip service around a IML around the ROI from the seauite

**[04:55]** IML around the ROI from the seauite

**[04:55]** IML around the ROI from the seauite including CIO CEOs um but that was just

**[04:58]** including CIO CEOs um but that was just

**[04:58]** including CIO CEOs um but that was just lip service in our experience at least


### [05:00 - 06:00]

**[05:00]** lip service in our experience at least

**[05:00]** lip service in our experience at least uh it was still basically selling into

**[05:01]** uh it was still basically selling into

**[05:01]** uh it was still basically selling into the CIO so basically it made it hard to

**[05:04]** the CIO so basically it made it hard to

**[05:04]** the CIO so basically it made it hard to sell outside of that now obviously this

**[05:07]** sell outside of that now obviously this

**[05:08]** sell outside of that now obviously this is a large space this has been happening

**[05:09]** is a large space this has been happening

**[05:09]** is a large space this has been happening since you know about 2012 I would say is

**[05:11]** since you know about 2012 I would say is

**[05:11]** since you know about 2012 I would say is when a IML monitoring really started up

**[05:13]** when a IML monitoring really started up

**[05:13]** when a IML monitoring really started up with like H2O and algorithmia and Seldon

**[05:15]** with like H2O and algorithmia and Seldon

**[05:15]** with like H2O and algorithmia and Seldon sort of the first generation of these

**[05:16]** sort of the first generation of these

**[05:16]** sort of the first generation of these companies coming around Y labs Aporeia

**[05:19]** companies coming around Y labs Aporeia

**[05:19]** companies coming around Y labs Aporeia Arise Arthur Galileo Fiddler protect AAI

**[05:22]** Arise Arthur Galileo Fiddler protect AAI

**[05:22]** Arise Arthur Galileo Fiddler protect AAI and so on and so on and so on. I've put

**[05:24]** and so on and so on and so on. I've put

**[05:24]** and so on and so on and so on. I've put the cuto off here at uh like mid 2022

**[05:28]** the cuto off here at uh like mid 2022

**[05:28]** the cuto off here at uh like mid 2022 sort of like before the genai revolution

**[05:30]** sort of like before the genai revolution

**[05:30]** sort of like before the genai revolution happened. There have obviously been

**[05:31]** happened. There have obviously been

**[05:32]** happened. There have obviously been companies founded after that. You know,

**[05:33]** companies founded after that. You know,

**[05:33]** companies founded after that. You know, we just saw Brain Trust talking as well.

**[05:35]** we just saw Brain Trust talking as well.

**[05:35]** we just saw Brain Trust talking as well. And then, you know, the big players here

**[05:37]** And then, you know, the big players here

**[05:37]** And then, you know, the big players here as well, right? Snowflake, data bricks,

**[05:38]** as well, right? Snowflake, data bricks,

**[05:38]** as well, right? Snowflake, data bricks, data dog, SageMaker, Vert.ex, you know,

**[05:40]** data dog, SageMaker, Vert.ex, you know,

**[05:40]** data dog, SageMaker, Vert.ex, you know, Microsoft's products and so on. So,

**[05:42]** Microsoft's products and so on. So,

**[05:42]** Microsoft's products and so on. So, people have been thinking about it,

**[05:45]** people have been thinking about it,

**[05:45]** people have been thinking about it, but it was never the thing. Again,

**[05:47]** but it was never the thing. Again,

**[05:47]** but it was never the thing. Again, rarely top of mind for the CEO, the CFO,

**[05:49]** rarely top of mind for the CEO, the CFO,

**[05:49]** rarely top of mind for the CEO, the CFO, and the CISO. It's never the issue. So

**[05:52]** and the CISO. It's never the issue. So

**[05:52]** and the CISO. It's never the issue. So when we would talk to people, it's

**[05:53]** when we would talk to people, it's

**[05:53]** when we would talk to people, it's always yes, we understand that we need

**[05:54]** always yes, we understand that we need

**[05:54]** always yes, we understand that we need this, but security is going to be a

**[05:57]** this, but security is going to be a

**[05:57]** this, but security is going to be a bigger issue or latency is going to be a

**[05:59]** bigger issue or latency is going to be a

**[05:59]** bigger issue or latency is going to be a bigger issue or some of these more


### [06:00 - 07:00]

**[06:00]** bigger issue or some of these more

**[06:00]** bigger issue or some of these more traditional technology sort of problems

**[06:02]** traditional technology sort of problems

**[06:02]** traditional technology sort of problems are going to be the issue. It wasn't the

**[06:03]** are going to be the issue. It wasn't the

**[06:04]** are going to be the issue. It wasn't the machine learning model itself. So

**[06:06]** machine learning model itself. So

**[06:06]** machine learning model itself. So basically, you know, I do a lot of due

**[06:07]** basically, you know, I do a lot of due

**[06:08]** basically, you know, I do a lot of due diligence for venture capitalists as

**[06:09]** diligence for venture capitalists as

**[06:09]** diligence for venture capitalists as well now as a you know um a multi-time

**[06:12]** well now as a you know um a multi-time

**[06:12]** well now as a you know um a multi-time founder and so on. And basically every

**[06:14]** founder and so on. And basically every

**[06:14]** founder and so on. And basically every pitch deck in this space from like the

**[06:16]** pitch deck in this space from like the

**[06:16]** pitch deck in this space from like the mid2010s onward had a slide that said

**[06:19]** mid2010s onward had a slide that said

**[06:19]** mid2010s onward had a slide that said this is the year that a CEO is going to

**[06:20]** this is the year that a CEO is going to

**[06:20]** this is the year that a CEO is going to get fired because of an ML related

**[06:22]** get fired because of an ML related

**[06:22]** get fired because of an ML related screw-up. Uh and to my knowledge it just

**[06:24]** screw-up. Uh and to my knowledge it just

**[06:24]** screw-up. Uh and to my knowledge it just still hasn't happened. There have been

**[06:26]** still hasn't happened. There have been

**[06:26]** still hasn't happened. There have been some forwardinking leaders. So I have

**[06:29]** some forwardinking leaders. So I have

**[06:29]** some forwardinking leaders. So I have here a um the annual report from Jamie

**[06:31]** here a um the annual report from Jamie

**[06:31]** here a um the annual report from Jamie Diamond head of JPMC. Uh this came out

**[06:34]** Diamond head of JPMC. Uh this came out

**[06:34]** Diamond head of JPMC. Uh this came out in April of 2022. So it covered

**[06:36]** in April of 2022. So it covered

**[06:36]** in April of 2022. So it covered basically JPMC up through their fiscal

**[06:37]** basically JPMC up through their fiscal

**[06:37]** basically JPMC up through their fiscal year in 2021. Uh and he's talking about

**[06:39]** year in 2021. Uh and he's talking about

**[06:39]** year in 2021. Uh and he's talking about the spend that they have going into AI.

**[06:41]** the spend that they have going into AI.

**[06:41]** the spend that they have going into AI. But if you squint and you look at these

**[06:43]** But if you squint and you look at these

**[06:43]** But if you squint and you look at these numbers, they're still like comically

**[06:44]** numbers, they're still like comically

**[06:44]** numbers, they're still like comically small. So basically, one of these is in

**[06:46]** small. So basically, one of these is in

**[06:46]** small. So basically, one of these is in the consumer world, he makes the

**[06:47]** the consumer world, he makes the

**[06:47]** the consumer world, he makes the statement that from 2017 up through the

**[06:50]** statement that from 2017 up through the

**[06:50]** statement that from 2017 up through the end of 2021, they had put $100 million

**[06:52]** end of 2021, they had put $100 million

**[06:52]** end of 2021, they had put $100 million into a IMO, right? That's not a huge

**[06:54]** into a IMO, right? That's not a huge

**[06:54]** into a IMO, right? That's not a huge amount of money uh for JPMC.

**[06:58]** amount of money uh for JPMC.

**[06:58]** amount of money uh for JPMC. So keep sitting back in time pre-hat GPT


### [07:00 - 08:00]

**[07:01]** So keep sitting back in time pre-hat GPT

**[07:01]** So keep sitting back in time pre-hat GPT and so on, but let's now flip to the

**[07:03]** and so on, but let's now flip to the

**[07:03]** and so on, but let's now flip to the macroeconomic side of things.

**[07:06]** macroeconomic side of things.

**[07:06]** macroeconomic side of things. So the economy started getting pretty

**[07:08]** So the economy started getting pretty

**[07:08]** So the economy started getting pretty dicey um right up until about chatbt um

**[07:12]** dicey um right up until about chatbt um

**[07:12]** dicey um right up until about chatbt um uh um launched right so that was the end

**[07:15]** uh um launched right so that was the end

**[07:16]** uh um launched right so that was the end of November for chatbt uh a lot of

**[07:18]** of November for chatbt uh a lot of

**[07:18]** of November for chatbt uh a lot of enterprise budgets are set in October

**[07:20]** enterprise budgets are set in October

**[07:20]** enterprise budgets are set in October November for the following year and

**[07:22]** November for the following year and

**[07:22]** November for the following year and toward mid to the end of 2022 there were

**[07:24]** toward mid to the end of 2022 there were

**[07:24]** toward mid to the end of 2022 there were very deep fears about an impending

**[07:26]** very deep fears about an impending

**[07:26]** very deep fears about an impending recession that didn't end up happening

**[07:28]** recession that didn't end up happening

**[07:28]** recession that didn't end up happening but those fears basically made it so

**[07:29]** but those fears basically made it so

**[07:29]** but those fears basically made it so that most enterprises either froze or

**[07:32]** that most enterprises either froze or

**[07:32]** that most enterprises either froze or shrunk their IT budgets for 20 uh 23

**[07:35]** shrunk their IT budgets for 20 uh 23

**[07:35]** shrunk their IT budgets for 20 uh 23 three. Okay. So, what that meant is were

**[07:38]** three. Okay. So, what that meant is were

**[07:38]** three. Okay. So, what that meant is were it not for particular tailwinds called

**[07:40]** it not for particular tailwinds called

**[07:40]** it not for particular tailwinds called JBT, we probably wouldn't have seen a

**[07:42]** JBT, we probably wouldn't have seen a

**[07:42]** JBT, we probably wouldn't have seen a lot of new technology being developed in

**[07:44]** lot of new technology being developed in

**[07:44]** lot of new technology being developed in the IT departments at these large

**[07:46]** the IT departments at these large

**[07:46]** the IT departments at these large enterprises in 2023.

**[07:49]** enterprises in 2023.

**[07:49]** enterprises in 2023. Right? So, this is sort of a bittersweet

**[07:52]** Right? So, this is sort of a bittersweet

**[07:52]** Right? So, this is sort of a bittersweet uh mix. I guess I just talked about a

**[07:53]** uh mix. I guess I just talked about a

**[07:53]** uh mix. I guess I just talked about a lot of this. Um, this was sort of a

**[07:55]** lot of this. Um, this was sort of a

**[07:55]** lot of this. Um, this was sort of a bittersweet mix in the sense that it did

**[07:57]** bittersweet mix in the sense that it did

**[07:57]** bittersweet mix in the sense that it did set us up to put the eye of Sauron on a


### [08:00 - 09:00]

**[08:00]** set us up to put the eye of Sauron on a

**[08:00]** set us up to put the eye of Sauron on a particular specific small pet project

**[08:04]** particular specific small pet project

**[08:04]** particular specific small pet project where uh a small amount of budget could

**[08:07]** where uh a small amount of budget could

**[08:07]** where uh a small amount of budget could be applied and that small pet project uh

**[08:10]** be applied and that small pet project uh

**[08:10]** be applied and that small pet project uh came from our friends at OpenAI

**[08:12]** came from our friends at OpenAI

**[08:12]** came from our friends at OpenAI launching chatbt right before the

**[08:14]** launching chatbt right before the

**[08:14]** launching chatbt right before the holiday break and I'm convinced just

**[08:16]** holiday break and I'm convinced just

**[08:16]** holiday break and I'm convinced just from talking to some seuite folks across

**[08:18]** from talking to some seuite folks across

**[08:18]** from talking to some seuite folks across the enterprises here in the US that

**[08:19]** the enterprises here in the US that

**[08:19]** the enterprises here in the US that basically what happened is now CEOs and

**[08:22]** basically what happened is now CEOs and

**[08:22]** basically what happened is now CEOs and CFOs and you know less technical and the

**[08:24]** CFOs and you know less technical and the

**[08:24]** CFOs and you know less technical and the computer science sense of the word were

**[08:26]** computer science sense of the word were

**[08:26]** computer science sense of the word were able to interact swiftly and easily with

**[08:28]** able to interact swiftly and easily with

**[08:28]** able to interact swiftly and easily with a single UI on the internet and get

**[08:30]** a single UI on the internet and get

**[08:30]** a single UI on the internet and get wowed by AI. Right? So I was also wowed

**[08:34]** wowed by AI. Right? So I was also wowed

**[08:34]** wowed by AI. Right? So I was also wowed by AI. In fact, we were hosting um

**[08:36]** by AI. In fact, we were hosting um

**[08:36]** by AI. In fact, we were hosting um Arthur was hosting with our um series A

**[08:39]** Arthur was hosting with our um series A

**[08:39]** Arthur was hosting with our um series A leaders index ventures an event at

**[08:40]** leaders index ventures an event at

**[08:40]** leaders index ventures an event at Nurups, the major machine learning

**[08:42]** Nurups, the major machine learning

**[08:42]** Nurups, the major machine learning conference the night that chatbt came

**[08:44]** conference the night that chatbt came

**[08:44]** conference the night that chatbt came out and it basically took over a bunch

**[08:46]** out and it basically took over a bunch

**[08:46]** out and it basically took over a bunch of nerds in a room being like wow this

**[08:47]** of nerds in a room being like wow this

**[08:47]** of nerds in a room being like wow this is very very impressive and that

**[08:49]** is very very impressive and that

**[08:49]** is very very impressive and that happened to everybody else right flip

**[08:50]** happened to everybody else right flip

**[08:50]** happened to everybody else right flip back to November 30th you know you can

**[08:52]** back to November 30th you know you can

**[08:52]** back to November 30th you know you can make Eminem rap like Taylor Swift oh hey

**[08:54]** make Eminem rap like Taylor Swift oh hey

**[08:54]** make Eminem rap like Taylor Swift oh hey isn't this funny oh hey my mom did this

**[08:56]** isn't this funny oh hey my mom did this

**[08:56]** isn't this funny oh hey my mom did this sort of like joke about you know I want

**[08:57]** sort of like joke about you know I want

**[08:57]** sort of like joke about you know I want to have poetry but in the uh uh written


### [09:00 - 10:00]

**[09:00]** to have poetry but in the uh uh written

**[09:00]** to have poetry but in the uh uh written in the you know the words of a rapper or

**[09:02]** in the you know the words of a rapper or

**[09:02]** in the you know the words of a rapper or whatever this started to happen over and

**[09:03]** whatever this started to happen over and

**[09:03]** whatever this started to happen over and over again And what that meant is that

**[09:05]** over again And what that meant is that

**[09:05]** over again And what that meant is that that discretionary budget uh which

**[09:08]** that discretionary budget uh which

**[09:08]** that discretionary budget uh which exists uh was unlocked specifically for

**[09:10]** exists uh was unlocked specifically for

**[09:10]** exists uh was unlocked specifically for now the CEO's pet projects which were

**[09:12]** now the CEO's pet projects which were

**[09:12]** now the CEO's pet projects which were called Genai.

**[09:15]** called Genai.

**[09:15]** called Genai. So 2023 uh we still had austerity forced

**[09:18]** So 2023 uh we still had austerity forced

**[09:18]** So 2023 uh we still had austerity forced on us because of those frozen budgets

**[09:21]** on us because of those frozen budgets

**[09:21]** on us because of those frozen budgets um or even reduced budgets. But the

**[09:24]** um or even reduced budgets. But the

**[09:24]** um or even reduced budgets. But the thing that was happening here is that

**[09:25]** thing that was happening here is that

**[09:25]** thing that was happening here is that now the only money going around that

**[09:27]** now the only money going around that

**[09:27]** now the only money going around that could be allocated was going to

**[09:28]** could be allocated was going to

**[09:28]** could be allocated was going to specifically genai. And so everybody

**[09:30]** specifically genai. And so everybody

**[09:30]** specifically genai. And so everybody focused on this. A it's a cool

**[09:31]** focused on this. A it's a cool

**[09:31]** focused on this. A it's a cool technology. Everybody focused on this

**[09:33]** technology. Everybody focused on this

**[09:33]** technology. Everybody focused on this and the science projects started to sort

**[09:34]** and the science projects started to sort

**[09:34]** and the science projects started to sort of float around within enterprise.

**[09:41]** 2024 we started to see genai based

**[09:41]** 2024 we started to see genai based applications going into production right

**[09:43]** applications going into production right

**[09:43]** applications going into production right chat applications are the obvious one

**[09:44]** chat applications are the obvious one

**[09:44]** chat applications are the obvious one internal chat applications internal

**[09:46]** internal chat applications internal

**[09:46]** internal chat applications internal hiring tools things like that

**[09:49]** hiring tools things like that

**[09:49]** hiring tools things like that and that's because basically the only

**[09:50]** and that's because basically the only

**[09:50]** and that's because basically the only budget going into new projects in 2023

**[09:52]** budget going into new projects in 2023

**[09:52]** budget going into new projects in 2023 was going to genai and now as things go

**[09:55]** was going to genai and now as things go

**[09:55]** was going to genai and now as things go into production primarily internally in

**[09:57]** into production primarily internally in

**[09:57]** into production primarily internally in 2024 uh we have the folks who tend to

**[09:59]** 2024 uh we have the folks who tend to

**[09:59]** 2024 uh we have the folks who tend to dress in business suits uh asking


### [10:00 - 11:00]

**[10:01]** dress in business suits uh asking

**[10:02]** dress in business suits uh asking questions around ROI governance risk

**[10:04]** questions around ROI governance risk

**[10:04]** questions around ROI governance risk compliance brand optics and that kind of

**[10:06]** compliance brand optics and that kind of

**[10:06]** compliance brand optics and that kind of thing so now we're starting to get a

**[10:07]** thing so now we're starting to get a

**[10:08]** thing so now we're starting to get a little bit closer to people outside of

**[10:09]** little bit closer to people outside of

**[10:09]** little bit closer to people outside of the machine learning, the data science,

**[10:10]** the machine learning, the data science,

**[10:10]** the machine learning, the data science, the computer science world, the CIO's

**[10:12]** the computer science world, the CIO's

**[10:12]** the computer science world, the CIO's office caring about evaluation, right?

**[10:14]** office caring about evaluation, right?

**[10:14]** office caring about evaluation, right? If I need to quantit quant uh if I need

**[10:17]** If I need to quantit quant uh if I need

**[10:17]** If I need to quantit quant uh if I need to have a quantitative estimate of risk,

**[10:19]** to have a quantitative estimate of risk,

**[10:19]** to have a quantitative estimate of risk, then I need to do evaluation.

**[10:25]** 2025, we've seen, you know, scaleups,

**[10:25]** 2025, we've seen, you know, scaleups, right? Look look at the revenue numbers

**[10:26]** right? Look look at the revenue numbers

**[10:26]** right? Look look at the revenue numbers for any frontier model provider. Look at

**[10:28]** for any frontier model provider. Look at

**[10:28]** for any frontier model provider. Look at the revenue numbers for a lot of us in

**[10:29]** the revenue numbers for a lot of us in

**[10:29]** the revenue numbers for a lot of us in this room. Just everything is really

**[10:30]** this room. Just everything is really

**[10:30]** this room. Just everything is really going up right now. And that's because

**[10:31]** going up right now. And that's because

**[10:31]** going up right now. And that's because of usage, which is great.

**[10:34]** of usage, which is great.

**[10:34]** of usage, which is great. That also means that's a function of the

**[10:35]** That also means that's a function of the

**[10:35]** That also means that's a function of the seauite basically becoming comfortable

**[10:37]** seauite basically becoming comfortable

**[10:37]** seauite basically becoming comfortable basically talking about and putting

**[10:38]** basically talking about and putting

**[10:38]** basically talking about and putting large real budget into AI right so

**[10:41]** large real budget into AI right so

**[10:41]** large real budget into AI right so 2023's IT budgets 2024's IT budgets uh

**[10:45]** 2023's IT budgets 2024's IT budgets uh

**[10:45]** 2023's IT budgets 2024's IT budgets uh set for the following year those weren't

**[10:46]** set for the following year those weren't

**[10:46]** set for the following year those weren't frozen right those are earmarked

**[10:47]** frozen right those are earmarked

**[10:47]** frozen right those are earmarked specifically for AI applications and

**[10:49]** specifically for AI applications and

**[10:49]** specifically for AI applications and things along those lines so we had

**[10:51]** things along those lines so we had

**[10:51]** things along those lines so we had science projects in 2023 go into

**[10:55]** science projects in 2023 go into

**[10:55]** science projects in 2023 go into production in 2024 and they're now

**[10:57]** production in 2024 and they're now

**[10:57]** production in 2024 and they're now shipping and scaling uh in 2025 and also


### [11:00 - 12:00]

**[11:00]** shipping and scaling uh in 2025 and also

**[11:00]** shipping and scaling uh in 2025 and also like frankly the the technologies just

**[11:02]** like frankly the the technologies just

**[11:02]** like frankly the the technologies just gotten really amazing Right? Like all of

**[11:03]** gotten really amazing Right? Like all of

**[11:03]** gotten really amazing Right? Like all of us in this room are are technical, but

**[11:05]** us in this room are are technical, but

**[11:05]** us in this room are are technical, but even I'm just amazed every time a new

**[11:07]** even I'm just amazed every time a new

**[11:07]** even I'm just amazed every time a new model is dropped.

**[11:09]** model is dropped.

**[11:09]** model is dropped. The community has also really gotten

**[11:10]** The community has also really gotten

**[11:10]** The community has also really gotten behind this. You know, open source has

**[11:11]** behind this. You know, open source has

**[11:12]** behind this. You know, open source has gotten behind this. Venture capital, big

**[11:13]** gotten behind this. Venture capital, big

**[11:13]** gotten behind this. Venture capital, big tech is writing huge checks into uh into

**[11:16]** tech is writing huge checks into uh into

**[11:16]** tech is writing huge checks into uh into frontier model providers and so on. So

**[11:18]** frontier model providers and so on. So

**[11:18]** frontier model providers and so on. So everything is sort of coming together in

**[11:19]** everything is sort of coming together in

**[11:19]** everything is sort of coming together in 2025.

**[11:21]** 2025.

**[11:21]** 2025. And also remember that third vertex, we

**[11:23]** And also remember that third vertex, we

**[11:23]** And also remember that third vertex, we have machine learning systems now moving

**[11:25]** have machine learning systems now moving

**[11:25]** have machine learning systems now moving toward autonomy. Okay, so 2025 we all

**[11:28]** toward autonomy. Okay, so 2025 we all

**[11:28]** toward autonomy. Okay, so 2025 we all hear it. It's year of the agent. I no

**[11:30]** hear it. It's year of the agent. I no

**[11:30]** hear it. It's year of the agent. I no longer is a question mark needed here.

**[11:32]** longer is a question mark needed here.

**[11:32]** longer is a question mark needed here. It's clearly the the year of the agent.

**[11:35]** It's clearly the the year of the agent.

**[11:35]** It's clearly the the year of the agent. Now, quick 30 secondond definition of an

**[11:38]** Now, quick 30 secondond definition of an

**[11:38]** Now, quick 30 secondond definition of an agent. As defined, you know, in the late

**[11:40]** agent. As defined, you know, in the late

**[11:40]** agent. As defined, you know, in the late 50s onward, um agents need to perceive

**[11:43]** 50s onward, um agents need to perceive

**[11:43]** 50s onward, um agents need to perceive the environment. They need to learn.

**[11:44]** the environment. They need to learn.

**[11:44]** the environment. They need to learn. They need to abstract and generalize.

**[11:46]** They need to abstract and generalize.

**[11:46]** They need to abstract and generalize. And unlike traditional machine learning,

**[11:48]** And unlike traditional machine learning,

**[11:48]** And unlike traditional machine learning, they're going to reason and act, right?

**[11:50]** they're going to reason and act, right?

**[11:50]** they're going to reason and act, right? We have reasoning models out there. We

**[11:52]** We have reasoning models out there. We

**[11:52]** We have reasoning models out there. We have systems that are acting in virtual

**[11:54]** have systems that are acting in virtual

**[11:54]** have systems that are acting in virtual environments or cyber physical

**[11:55]** environments or cyber physical

**[11:56]** environments or cyber physical environments. And what that means is you

**[11:58]** environments. And what that means is you

**[11:58]** environments. And what that means is you have a lot of complexity introduced into

**[11:59]** have a lot of complexity introduced into

**[11:59]** have a lot of complexity introduced into the system and you have a lot of risk


### [12:00 - 13:00]

**[12:01]** the system and you have a lot of risk

**[12:01]** the system and you have a lot of risk introduced into the system and that's

**[12:03]** introduced into the system and that's

**[12:03]** introduced into the system and that's great for those of us in uh ebo.

**[12:08]** great for those of us in uh ebo.

**[12:08]** great for those of us in uh ebo. So, at the end of the day, the thing

**[12:10]** So, at the end of the day, the thing

**[12:10]** So, at the end of the day, the thing that really matters, like I mentioned,

**[12:11]** that really matters, like I mentioned,

**[12:11]** that really matters, like I mentioned, is connecting when you're selling any

**[12:12]** is connecting when you're selling any

**[12:12]** is connecting when you're selling any product, not just our products in this

**[12:14]** product, not just our products in this

**[12:14]** product, not just our products in this room. When you're selling any product

**[12:15]** room. When you're selling any product

**[12:15]** room. When you're selling any product into an enterprise or SMB, is being able

**[12:18]** into an enterprise or SMB, is being able

**[12:18]** into an enterprise or SMB, is being able to attach uh your product into some sort

**[12:20]** to attach uh your product into some sort

**[12:20]** to attach uh your product into some sort of downstream business KPI, risk

**[12:22]** of downstream business KPI, risk

**[12:22]** of downstream business KPI, risk mitigation, revenue gains, uh you know,

**[12:25]** mitigation, revenue gains, uh you know,

**[12:25]** mitigation, revenue gains, uh you know, losing less money, whatever. So, now

**[12:29]** losing less money, whatever. So, now

**[12:29]** losing less money, whatever. So, now evaluations, you need to be able to do

**[12:31]** evaluations, you need to be able to do

**[12:31]** evaluations, you need to be able to do this because you're quantifying things.

**[12:32]** this because you're quantifying things.

**[12:32]** this because you're quantifying things. And they're finally a first class

**[12:34]** And they're finally a first class

**[12:34]** And they're finally a first class discussion point, which is fantastic. So

**[12:36]** discussion point, which is fantastic. So

**[12:36]** discussion point, which is fantastic. So we have the CEO like I mentioned

**[12:39]** we have the CEO like I mentioned

**[12:39]** we have the CEO like I mentioned November 30th 2022 and onward now at

**[12:42]** November 30th 2022 and onward now at

**[12:42]** November 30th 2022 and onward now at least knows what the tech is and I'm not

**[12:43]** least knows what the tech is and I'm not

**[12:43]** least knows what the tech is and I'm not saying they know you know what attention

**[12:45]** saying they know you know what attention

**[12:45]** saying they know you know what attention means uh right but I am saying that they

**[12:47]** means uh right but I am saying that they

**[12:48]** means uh right but I am saying that they know some of the capabilities around

**[12:49]** know some of the capabilities around

**[12:49]** know some of the capabilities around these generative models they know some

**[12:51]** these generative models they know some

**[12:51]** these generative models they know some of the capabilities around agentic and

**[12:52]** of the capabilities around agentic and

**[12:52]** of the capabilities around agentic and multi-agent systems uh and they're

**[12:54]** multi-agent systems uh and they're

**[12:54]** multi-agent systems uh and they're comfortable um you know talking to

**[12:56]** comfortable um you know talking to

**[12:56]** comfortable um you know talking to experts about it uh allocating budget

**[12:58]** experts about it uh allocating budget

**[12:58]** experts about it uh allocating budget for it uh and talking to their board of


### [13:00 - 14:00]

**[13:00]** for it uh and talking to their board of

**[13:00]** for it uh and talking to their board of directors and shareholders about it as

**[13:01]** directors and shareholders about it as

**[13:01]** directors and shareholders about it as well.

**[13:03]** well.

**[13:03]** well. Uh, we have the CFO who because the CEO

**[13:06]** Uh, we have the CFO who because the CEO

**[13:06]** Uh, we have the CFO who because the CEO cares about this stuff, uh, also

**[13:07]** cares about this stuff, uh, also

**[13:08]** cares about this stuff, uh, also obviously needs to care about it, but

**[13:09]** obviously needs to care about it, but

**[13:09]** obviously needs to care about it, but she's going to care about the impact to

**[13:10]** she's going to care about the impact to

**[13:10]** she's going to care about the impact to the bottom line, right? That's what a

**[13:12]** the bottom line, right? That's what a

**[13:12]** the bottom line, right? That's what a CFO does. They're doing allocation,

**[13:13]** CFO does. They're doing allocation,

**[13:13]** CFO does. They're doing allocation, they're doing budget planning, uh, and

**[13:15]** they're doing budget planning, uh, and

**[13:15]** they're doing budget planning, uh, and they're going to need to basically write

**[13:16]** they're going to need to basically write

**[13:16]** they're going to need to basically write some numbers into an Excel spreadsheet,

**[13:18]** some numbers into an Excel spreadsheet,

**[13:18]** some numbers into an Excel spreadsheet, and those numbers have to come from, in

**[13:19]** and those numbers have to come from, in

**[13:19]** and those numbers have to come from, in part quantitative evaluation.

**[13:23]** part quantitative evaluation.

**[13:23]** part quantitative evaluation. Uh, CISOs now see this as, you know, a

**[13:25]** Uh, CISOs now see this as, you know, a

**[13:25]** Uh, CISOs now see this as, you know, a huge security risk and opportunity. And

**[13:27]** huge security risk and opportunity. And

**[13:28]** huge security risk and opportunity. And for those who haven't sold into

**[13:29]** for those who haven't sold into

**[13:29]** for those who haven't sold into enterprises before, slicers are

**[13:30]** enterprises before, slicers are

**[13:30]** enterprises before, slicers are typically willing to write checks uh

**[13:32]** typically willing to write checks uh

**[13:32]** typically willing to write checks uh smaller checks especially for startups

**[13:34]** smaller checks especially for startups

**[13:34]** smaller checks especially for startups uh more quickly and with less overhead

**[13:36]** uh more quickly and with less overhead

**[13:36]** uh more quickly and with less overhead than like a CIO would. CIOS tend to have

**[13:39]** than like a CIO would. CIOS tend to have

**[13:39]** than like a CIO would. CIOS tend to have a bigger org for one and also tend to

**[13:40]** a bigger org for one and also tend to

**[13:40]** a bigger org for one and also tend to have a lot more process. CISOs tend to

**[13:42]** have a lot more process. CISOs tend to

**[13:42]** have a lot more process. CISOs tend to be a little bit more scrappy and willing

**[13:44]** be a little bit more scrappy and willing

**[13:44]** be a little bit more scrappy and willing to try out tools and so on. And so this

**[13:47]** to try out tools and so on. And so this

**[13:47]** to try out tools and so on. And so this actually happened before the agentic uh

**[13:48]** actually happened before the agentic uh

**[13:48]** actually happened before the agentic uh revolution. This actually happened when

**[13:50]** revolution. This actually happened when

**[13:50]** revolution. This actually happened when Genai started coming up where CISOs were

**[13:51]** Genai started coming up where CISOs were

**[13:51]** Genai started coming up where CISOs were like, hey, hallucination detection,

**[13:53]** like, hey, hallucination detection,

**[13:53]** like, hey, hallucination detection, prompt injection, things like that.

**[13:54]** prompt injection, things like that.

**[13:54]** prompt injection, things like that. that's firmly in the security space,

**[13:56]** that's firmly in the security space,

**[13:56]** that's firmly in the security space, which is why you've seen a lot of

**[13:57]** which is why you've seen a lot of

**[13:57]** which is why you've seen a lot of guardrail products, including Arthur,

**[13:58]** guardrail products, including Arthur,

**[13:58]** guardrail products, including Arthur, including the ones that come from our

**[13:59]** including the ones that come from our

**[13:59]** including the ones that come from our competitors going into the CISO's office


### [14:00 - 15:00]

**[14:01]** competitors going into the CISO's office

**[14:02]** competitors going into the CISO's office and and basically being able to sign a

**[14:03]** and and basically being able to sign a

**[14:03]** and and basically being able to sign a lot of deals.

**[14:06]** lot of deals.

**[14:06]** lot of deals. Uh the CIO, corporate,

**[14:13]** they're still on board. Uh they want to

**[14:13]** they're still on board. Uh they want to keep their job. And the CTO's now, uh

**[14:16]** keep their job. And the CTO's now, uh

**[14:16]** keep their job. And the CTO's now, uh they always want standard, right? They

**[14:17]** they always want standard, right? They

**[14:17]** they always want standard, right? They need to make these decisions based on

**[14:19]** need to make these decisions based on

**[14:19]** need to make these decisions based on numbers. Those numbers are coming in

**[14:20]** numbers. Those numbers are coming in

**[14:20]** numbers. Those numbers are coming in part from like hotel standards standards

**[14:23]** part from like hotel standards standards

**[14:23]** part from like hotel standards standards like that

**[14:25]** like that

**[14:25]** like that and that's great right so I've listed a

**[14:27]** and that's great right so I've listed a

**[14:27]** and that's great right so I've listed a lot of the seauite here I haven't talked

**[14:29]** lot of the seauite here I haven't talked

**[14:29]** lot of the seauite here I haven't talked about chief strategy officers or

**[14:30]** about chief strategy officers or

**[14:30]** about chief strategy officers or otherwise but like the CEO the CFO the

**[14:32]** otherwise but like the CEO the CFO the

**[14:32]** otherwise but like the CEO the CFO the CTO the CIO the CISO they control a lot

**[14:34]** CTO the CIO the CISO they control a lot

**[14:34]** CTO the CIO the CISO they control a lot of budget and now they are all willing

**[14:37]** of budget and now they are all willing

**[14:37]** of budget and now they are all willing to talk about and they're all aligned

**[14:39]** to talk about and they're all aligned

**[14:39]** to talk about and they're all aligned about basically the need to understand

**[14:41]** about basically the need to understand

**[14:41]** about basically the need to understand evaluation from AI

**[14:45]** evaluation from AI

**[14:45]** evaluation from AI great so uh quick remind me you know you

**[14:48]** great so uh quick remind me you know you

**[14:48]** great so uh quick remind me you know you should hold me truthful

**[14:49]** should hold me truthful

**[14:49]** should hold me truthful All the evaluation companies,

**[14:51]** All the evaluation companies,

**[14:51]** All the evaluation companies, observability companies, monitoring

**[14:52]** observability companies, monitoring

**[14:52]** observability companies, monitoring companies, security companies, whatever

**[14:54]** companies, security companies, whatever

**[14:54]** companies, security companies, whatever you want to call them, have shifted into

**[14:56]** you want to call them, have shifted into

**[14:56]** you want to call them, have shifted into a multi-agent systems monitoring, right?

**[14:58]** a multi-agent systems monitoring, right?

**[14:58]** a multi-agent systems monitoring, right? The point around you should monitor the


### [15:00 - 16:00]

**[15:00]** The point around you should monitor the

**[15:00]** The point around you should monitor the whole system. You shouldn't just monitor

**[15:01]** whole system. You shouldn't just monitor

**[15:01]** whole system. You shouldn't just monitor the one model that is being used by one

**[15:03]** the one model that is being used by one

**[15:03]** the one model that is being used by one particular agent. That's well taken and

**[15:05]** particular agent. That's well taken and

**[15:05]** particular agent. That's well taken and well understood in industry and in

**[15:06]** well understood in industry and in

**[15:06]** well understood in industry and in government. And I think that's that's

**[15:07]** government. And I think that's that's

**[15:07]** government. And I think that's that's great to have that at that topline

**[15:09]** great to have that at that topline

**[15:09]** great to have that at that topline discussion point.

**[15:11]** discussion point.

**[15:11]** discussion point. But, you know, keep me honest here. Uh

**[15:13]** But, you know, keep me honest here. Uh

**[15:13]** But, you know, keep me honest here. Uh there was an article that came out in

**[15:15]** there was an article that came out in

**[15:15]** there was an article that came out in midappril uh in the information uh

**[15:17]** midappril uh in the information uh

**[15:17]** midappril uh in the information uh showing some leaked revenue numbers for

**[15:19]** showing some leaked revenue numbers for

**[15:19]** showing some leaked revenue numbers for a variety of startups in the evaluation

**[15:20]** a variety of startups in the evaluation

**[15:20]** a variety of startups in the evaluation space and biases Galileo Brain Trust and

**[15:23]** space and biases Galileo Brain Trust and

**[15:23]** space and biases Galileo Brain Trust and so on but they were lagged by about 6

**[15:24]** so on but they were lagged by about 6

**[15:24]** so on but they were lagged by about 6 months or eight months um and just from

**[15:27]** months or eight months um and just from

**[15:27]** months or eight months um and just from talking to friends in the space uh those

**[15:30]** talking to friends in the space uh those

**[15:30]** talking to friends in the space uh those numbers are no longer representative of

**[15:31]** numbers are no longer representative of

**[15:31]** numbers are no longer representative of what folks in this area are making. And

**[15:33]** what folks in this area are making. And

**[15:33]** what folks in this area are making. And so let's see what the information leaks

**[15:35]** so let's see what the information leaks

**[15:35]** so let's see what the information leaks in early 2026 about this and maybe we'll

**[15:38]** in early 2026 about this and maybe we'll

**[15:38]** in early 2026 about this and maybe we'll see something like revenue no longer

**[15:39]** see something like revenue no longer

**[15:39]** see something like revenue no longer lags at AI evaluation startups because

**[15:42]** lags at AI evaluation startups because

**[15:42]** lags at AI evaluation startups because this is the year for AI evaluation.

**[15:46]** this is the year for AI evaluation.

**[15:46]** this is the year for AI evaluation. Great. So I'll leave it with some time

**[15:47]** Great. So I'll leave it with some time

**[15:47]** Great. So I'll leave it with some time for questions. Um I did mention Mozilla

**[15:49]** for questions. Um I did mention Mozilla

**[15:49]** for questions. Um I did mention Mozilla is not firmly in the evaluation space.

**[15:50]** is not firmly in the evaluation space.

**[15:50]** is not firmly in the evaluation space. We do have a very nice open source not

**[15:52]** We do have a very nice open source not

**[15:52]** We do have a very nice open source not monetized at all what we're calling a

**[15:54]** monetized at all what we're calling a

**[15:54]** monetized at all what we're calling a light LLM for multi-agent systems. So if

**[15:56]** light LLM for multi-agent systems. So if

**[15:56]** light LLM for multi-agent systems. So if you're playing around with different

**[15:57]** you're playing around with different

**[15:57]** you're playing around with different multi-agent system frameworks check out

**[15:58]** multi-agent system frameworks check out

**[15:58]** multi-agent system frameworks check out any agent. We implement a lot of them

**[15:59]** any agent. We implement a lot of them


### [16:00 - 17:00]

**[16:00]** any agent. We implement a lot of them for you under a unified interface. So

**[16:01]** for you under a unified interface. So

**[16:01]** for you under a unified interface. So for people in this room that might be a

**[16:02]** for people in this room that might be a

**[16:02]** for people in this room that might be a fun project to play around with. So

**[16:04]** fun project to play around with. So

**[16:04]** fun project to play around with. So thank you. I'll uh have three three

**[16:06]** thank you. I'll uh have three three

**[16:06]** thank you. I'll uh have three three minutes for questions.

**[16:18]** >> So okay thanks uh thanks great

**[16:18]** >> So okay thanks uh thanks great presentation. I just have a question

**[16:20]** presentation. I just have a question

**[16:20]** presentation. I just have a question really about the enterprise value about

**[16:23]** really about the enterprise value about

**[16:23]** really about the enterprise value about the most of the evaluations in Genead

**[16:25]** the most of the evaluations in Genead

**[16:25]** the most of the evaluations in Genead require domain expertise. So for

**[16:27]** require domain expertise. So for

**[16:27]** require domain expertise. So for example, if you're building a multi-

**[16:29]** example, if you're building a multi-

**[16:29]** example, if you're building a multi- agent system to do financial investment

**[16:32]** agent system to do financial investment

**[16:32]** agent system to do financial investment analysis to do something called a

**[16:33]** analysis to do something called a

**[16:33]** analysis to do something called a discounted cash flow spreadsheet, is the

**[16:35]** discounted cash flow spreadsheet, is the

**[16:36]** discounted cash flow spreadsheet, is the agent doing it correctly or not? Uh I'm

**[16:38]** agent doing it correctly or not? Uh I'm

**[16:38]** agent doing it correctly or not? Uh I'm just trying to understand is how is that

**[16:40]** just trying to understand is how is that

**[16:40]** just trying to understand is how is that problem getting solved? Because most of

**[16:41]** problem getting solved? Because most of

**[16:41]** problem getting solved? Because most of them are, you know, coming from an ML

**[16:43]** them are, you know, coming from an ML

**[16:43]** them are, you know, coming from an ML background where it was structured data,

**[16:45]** background where it was structured data,

**[16:45]** background where it was structured data, but this is a lot of unstructured data

**[16:46]** but this is a lot of unstructured data

**[16:46]** but this is a lot of unstructured data and you have to measure the quality like

**[16:48]** and you have to measure the quality like

**[16:48]** and you have to measure the quality like is it in acting like a human, right?

**[16:51]** is it in acting like a human, right?

**[16:51]** is it in acting like a human, right? >> Yeah. Yeah. It's it's great. Actually, I

**[16:52]** >> Yeah. Yeah. It's it's great. Actually, I

**[16:52]** >> Yeah. Yeah. It's it's great. Actually, I have a paper in nature machine

**[16:53]** have a paper in nature machine

**[16:53]** have a paper in nature machine intelligence talking about some of the

**[16:54]** intelligence talking about some of the

**[16:54]** intelligence talking about some of the problems that can come around when you

**[16:55]** problems that can come around when you

**[16:55]** problems that can come around when you do persona based agents where I say act

**[16:58]** do persona based agents where I say act

**[16:58]** do persona based agents where I say act like a farmer in Ohio in your mid-40s

**[16:59]** like a farmer in Ohio in your mid-40s

**[16:59]** like a farmer in Ohio in your mid-40s and so on. There's value in that, but


### [17:00 - 18:00]

**[17:01]** and so on. There's value in that, but

**[17:01]** and so on. There's value in that, but you can't do it perfectly. And my gut

**[17:03]** you can't do it perfectly. And my gut

**[17:03]** you can't do it perfectly. And my gut reaction to this is um there was a

**[17:04]** reaction to this is um there was a

**[17:04]** reaction to this is um there was a leaked spreadsheet from Merkor, which is

**[17:07]** leaked spreadsheet from Merkor, which is

**[17:07]** leaked spreadsheet from Merkor, which is a company that uh can hire in experts

**[17:09]** a company that uh can hire in experts

**[17:09]** a company that uh can hire in experts showing, you know, $50 an hour, $100 an

**[17:11]** showing, you know, $50 an hour, $100 an

**[17:11]** showing, you know, $50 an hour, $100 an hour, $200 an hour for experts to be

**[17:13]** hour, $200 an hour for experts to be

**[17:13]** hour, $200 an hour for experts to be hired by, for example, Google or for

**[17:15]** hired by, for example, Google or for

**[17:15]** hired by, for example, Google or for example, Meta or for example, large

**[17:17]** example, Meta or for example, large

**[17:17]** example, Meta or for example, large banks to do kind of what you're saying,

**[17:18]** banks to do kind of what you're saying,

**[17:18]** banks to do kind of what you're saying, which is you're going to have an expert

**[17:20]** which is you're going to have an expert

**[17:20]** which is you're going to have an expert sitting alongside the multi- aent

**[17:22]** sitting alongside the multi- aent

**[17:22]** sitting alongside the multi- aent system. Uh basically sitting next to,

**[17:24]** system. Uh basically sitting next to,

**[17:24]** system. Uh basically sitting next to, you know, the intern who's going to come

**[17:25]** you know, the intern who's going to come

**[17:25]** you know, the intern who's going to come in and take your job in like a year or

**[17:26]** in and take your job in like a year or

**[17:26]** in and take your job in like a year or two or change your job. Maybe take is

**[17:28]** two or change your job. Maybe take is

**[17:28]** two or change your job. Maybe take is not the right word, but they're

**[17:29]** not the right word, but they're

**[17:29]** not the right word, but they're basically doing that expensive human

**[17:32]** basically doing that expensive human

**[17:32]** basically doing that expensive human validation and lock step with the multi-

**[17:33]** validation and lock step with the multi-

**[17:33]** validation and lock step with the multi- aent system, which you know, if you're

**[17:35]** aent system, which you know, if you're

**[17:35]** aent system, which you know, if you're going to be doing discounted cash flow

**[17:36]** going to be doing discounted cash flow

**[17:36]** going to be doing discounted cash flow analysis, right, the kind of thing where

**[17:38]** analysis, right, the kind of thing where

**[17:38]** analysis, right, the kind of thing where a you can either make or lose a lot of

**[17:39]** a you can either make or lose a lot of

**[17:39]** a you can either make or lose a lot of money and b lose your job if you get it

**[17:41]** money and b lose your job if you get it

**[17:41]** money and b lose your job if you get it wrong. Uh it's worth spending that large

**[17:43]** wrong. Uh it's worth spending that large

**[17:43]** wrong. Uh it's worth spending that large amount of money doing the human

**[17:44]** amount of money doing the human

**[17:44]** amount of money doing the human validation. It's a question for everyone

**[17:46]** validation. It's a question for everyone

**[17:46]** validation. It's a question for everyone though is like what does that look like

**[17:47]** though is like what does that look like

**[17:47]** though is like what does that look like in five years once that data is

**[17:48]** in five years once that data is

**[17:48]** in five years once that data is incorporated into the systems themselves

**[17:50]** incorporated into the systems themselves

**[17:50]** incorporated into the systems themselves and that can be a mode as well, right?

**[17:52]** and that can be a mode as well, right?

**[17:52]** and that can be a mode as well, right? when you talk to anyone in the eval

**[17:53]** when you talk to anyone in the eval

**[17:53]** when you talk to anyone in the eval space like it's the data set creation

**[17:54]** space like it's the data set creation

**[17:54]** space like it's the data set creation and the environment creation that

**[17:56]** and the environment creation that

**[17:56]** and the environment creation that matters more than anything which is the

**[17:57]** matters more than anything which is the

**[17:57]** matters more than anything which is the point you're getting at as well. So if I

**[17:59]** point you're getting at as well. So if I

**[17:59]** point you're getting at as well. So if I if I spend a bunch of money to have a


### [18:00 - 19:00]

**[18:01]** if I spend a bunch of money to have a

**[18:01]** if I spend a bunch of money to have a very good competitive like uh DCF

**[18:05]** very good competitive like uh DCF

**[18:05]** very good competitive like uh DCF environment or whatever that can help me

**[18:06]** environment or whatever that can help me

**[18:06]** environment or whatever that can help me versus my competitors. So there is like

**[18:08]** versus my competitors. So there is like

**[18:08]** versus my competitors. So there is like um there is capex going into that.

**[18:12]** um there is capex going into that.

**[18:12]** um there is capex going into that. >> Uh yeah

**[18:13]** >> Uh yeah

**[18:13]** >> Uh yeah >> thanks for the presentation. Um do you

**[18:15]** >> thanks for the presentation. Um do you

**[18:15]** >> thanks for the presentation. Um do you have a rough uh maybe timeline on when

**[18:17]** have a rough uh maybe timeline on when

**[18:17]** have a rough uh maybe timeline on when you think um eval will primarily be

**[18:20]** you think um eval will primarily be

**[18:20]** you think um eval will primarily be driven by uh maybe gen AI or LMS?

**[18:23]** driven by uh maybe gen AI or LMS?

**[18:23]** driven by uh maybe gen AI or LMS? >> Yeah, the LLM as a judge paradigm is and

**[18:25]** >> Yeah, the LLM as a judge paradigm is and

**[18:26]** >> Yeah, the LLM as a judge paradigm is and I think this was talked about in in the

**[18:27]** I think this was talked about in in the

**[18:27]** I think this was talked about in in the previous talk as well. Uh we see it

**[18:28]** previous talk as well. Uh we see it

**[18:28]** previous talk as well. Uh we see it getting used in practice because there

**[18:30]** getting used in practice because there

**[18:30]** getting used in practice because there are issues with it, right? We have a

**[18:32]** are issues with it, right? We have a

**[18:32]** are issues with it, right? We have a paper in iClar uh from last month

**[18:34]** paper in iClar uh from last month

**[18:34]** paper in iClar uh from last month talking about some of the biases that

**[18:35]** talking about some of the biases that

**[18:35]** talking about some of the biases that LLMs as judges have versus humans and

**[18:37]** LLMs as judges have versus humans and

**[18:37]** LLMs as judges have versus humans and things like conciseness or helpfulness

**[18:38]** things like conciseness or helpfulness

**[18:38]** things like conciseness or helpfulness and some of those anthropic words. But

**[18:40]** and some of those anthropic words. But

**[18:40]** and some of those anthropic words. But the long and short of it is that like it

**[18:42]** the long and short of it is that like it

**[18:42]** the long and short of it is that like it it solves the data set creation problem

**[18:44]** it solves the data set creation problem

**[18:44]** it solves the data set creation problem in some sense and that like you can ask

**[18:46]** in some sense and that like you can ask

**[18:46]** in some sense and that like you can ask you you give a persona to an LLM and it

**[18:48]** you you give a persona to an LLM and it

**[18:48]** you you give a persona to an LLM and it it it is like a poor man's version of

**[18:50]** it it is like a poor man's version of

**[18:50]** it it is like a poor man's version of like a human doing the judging and so we

**[18:52]** like a human doing the judging and so we

**[18:52]** like a human doing the judging and so we see a lot of people using that as a

**[18:54]** see a lot of people using that as a

**[18:54]** see a lot of people using that as a product right now but you need to toward

**[18:55]** product right now but you need to toward

**[18:56]** product right now but you need to toward the toward the last question that was

**[18:57]** the toward the last question that was

**[18:57]** the toward the last question that was asked you do need to make sure you're

**[18:58]** asked you do need to make sure you're

**[18:58]** asked you do need to make sure you're validating this and and making sure that

**[18:59]** validating this and and making sure that

**[18:59]** validating this and and making sure that you're not going off in some weird bias


### [19:00 - 20:00]

**[19:00]** you're not going off in some weird bias

**[19:00]** you're not going off in some weird bias direction

**[19:06]** that's time. Oh. Uh, happy to chat

**[19:06]** that's time. Oh. Uh, happy to chat offline.


