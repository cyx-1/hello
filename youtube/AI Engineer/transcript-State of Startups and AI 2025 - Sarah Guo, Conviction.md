# State of Startups and AI 2025 - Sarah Guo, Conviction

**Video URL:** https://www.youtube.com/watch?v=3MZS5gNElZM

---

## Full Transcript

### [00:00 - 01:00]

**[00:19]** [Music]

**[00:19]** [Music] [Applause]

**[00:22]** [Applause]

**[00:22]** [Applause] So first question for you uh what is

**[00:26]** So first question for you uh what is

**[00:26]** So first question for you uh what is definitely happening by the end of 2026

**[00:29]** definitely happening by the end of 2026

**[00:29]** definitely happening by the end of 2026 AI agents ship code directly to prod in

**[00:32]** AI agents ship code directly to prod in

**[00:32]** AI agents ship code directly to prod in your environment, right? Not in like

**[00:34]** your environment, right? Not in like

**[00:34]** your environment, right? Not in like some uh playground. Uh voice AI replaces

**[00:37]** some uh playground. Uh voice AI replaces

**[00:38]** some uh playground. Uh voice AI replaces text for most business communication.

**[00:41]** text for most business communication.

**[00:41]** text for most business communication. Inference cost dropped below a cent per

**[00:44]** Inference cost dropped below a cent per

**[00:44]** Inference cost dropped below a cent per million tokens or wall-ally like we're

**[00:47]** million tokens or wall-ally like we're

**[00:48]** million tokens or wall-ally like we're all chilling.

**[00:51]** all chilling.

**[00:51]** all chilling. Any of these

**[00:56]** first one ship ship code directly to

**[00:56]** first one ship ship code directly to prod. Okay, this is a hopeful set of

**[00:58]** prod. Okay, this is a hopeful set of

**[00:58]** prod. Okay, this is a hopeful set of engineers.


### [01:00 - 02:00]

**[01:01]** engineers.

**[01:01]** engineers. All of you want to get rid of your own

**[01:03]** All of you want to get rid of your own

**[01:03]** All of you want to get rid of your own jobs. I love that.

**[01:10]** The good thing is I also don't have

**[01:10]** The good thing is I also don't have internet so I can't look at my next

**[01:11]** internet so I can't look at my next

**[01:11]** internet so I can't look at my next question.

**[01:17]** No, it's going to be good. It's going to

**[01:17]** No, it's going to be good. It's going to be good.

**[01:19]** be good.

**[01:19]** be good. Um

**[01:25]** I present from your phone. Uh, no. I was

**[01:25]** I present from your phone. Uh, no. I was going to go through poll questions while

**[01:27]** going to go through poll questions while

**[01:27]** going to go through poll questions while we're trying to do AV setup. This one.

**[01:44]** While this is happening, I'm actually

**[01:44]** While this is happening, I'm actually just going to introduce myself so we're

**[01:45]** just going to introduce myself so we're

**[01:45]** just going to introduce myself so we're not wasting the time. Um, my name is

**[01:47]** not wasting the time. Um, my name is

**[01:47]** not wasting the time. Um, my name is Sarah Goa. I, uh, helped start a AI

**[01:51]** Sarah Goa. I, uh, helped start a AI

**[01:51]** Sarah Goa. I, uh, helped start a AI native venture fund. It's called

**[01:52]** native venture fund. It's called

**[01:52]** native venture fund. It's called Conviction. And we got going about two

**[01:55]** Conviction. And we got going about two

**[01:55]** Conviction. And we got going about two and a half almost three years ago now

**[01:57]** and a half almost three years ago now

**[01:57]** and a half almost three years ago now just before the starting gun of chat

**[01:59]** just before the starting gun of chat

**[01:59]** just before the starting gun of chat GPT. Um as always in technology


### [02:00 - 03:00]

**[02:02]** GPT. Um as always in technology

**[02:02]** GPT. Um as always in technology investing most of life it's better to be

**[02:04]** investing most of life it's better to be

**[02:04]** investing most of life it's better to be lucky than right. Hopefully you can be a

**[02:06]** lucky than right. Hopefully you can be a

**[02:06]** lucky than right. Hopefully you can be a little of both. Um uh and and the point

**[02:10]** little of both. Um uh and and the point

**[02:10]** little of both. Um uh and and the point of having a new venture firm I I worked

**[02:13]** of having a new venture firm I I worked

**[02:13]** of having a new venture firm I I worked at Greylock. It's kind of a

**[02:14]** at Greylock. It's kind of a

**[02:14]** at Greylock. It's kind of a traditionalist venture firm a great one.

**[02:16]** traditionalist venture firm a great one.

**[02:16]** traditionalist venture firm a great one. My partner Mike Vernal used to work at

**[02:17]** My partner Mike Vernal used to work at

**[02:17]** My partner Mike Vernal used to work at Sequoia. You guys have probably heard of

**[02:19]** Sequoia. You guys have probably heard of

**[02:19]** Sequoia. You guys have probably heard of them. uh was that we think like actually

**[02:23]** them. uh was that we think like actually

**[02:23]** them. uh was that we think like actually you know at risk of sounding like those

**[02:24]** you know at risk of sounding like those

**[02:24]** you know at risk of sounding like those people this time it's different right um

**[02:26]** people this time it's different right um

**[02:26]** people this time it's different right um that this is the largest technology

**[02:28]** that this is the largest technology

**[02:28]** that this is the largest technology revolution that we get to be a part of

**[02:30]** revolution that we get to be a part of

**[02:30]** revolution that we get to be a part of and that there's so much change in the

**[02:33]** and that there's so much change in the

**[02:33]** and that there's so much change in the technology the types of businesses you

**[02:35]** technology the types of businesses you

**[02:35]** technology the types of businesses you can build the product decisions you make

**[02:37]** can build the product decisions you make

**[02:37]** can build the product decisions you make what challenges these startups and big

**[02:39]** what challenges these startups and big

**[02:39]** what challenges these startups and big companies face that you know maybe

**[02:41]** companies face that you know maybe

**[02:41]** companies face that you know maybe there's opportunity for like a startup

**[02:43]** there's opportunity for like a startup

**[02:43]** there's opportunity for like a startup VC as well and so um you know I'm I'm

**[02:47]** VC as well and so um you know I'm I'm

**[02:47]** VC as well and so um you know I'm I'm thrilled to be working with like really

**[02:49]** thrilled to be working with like really

**[02:49]** thrilled to be working with like really interesting people in the industry so

**[02:50]** interesting people in the industry so

**[02:50]** interesting people in the industry so far. Uh Mike and I are investors in

**[02:53]** far. Uh Mike and I are investors in

**[02:53]** far. Uh Mike and I are investors in companies like cursor, cognition,

**[02:55]** companies like cursor, cognition,

**[02:55]** companies like cursor, cognition, mistral, thinking machines, Harvey, open

**[02:58]** mistral, thinking machines, Harvey, open

**[02:58]** mistral, thinking machines, Harvey, open evidence. So a mix of um base 10 like a


### [03:00 - 04:00]

**[03:01]** evidence. So a mix of um base 10 like a

**[03:01]** evidence. So a mix of um base 10 like a mix of uh infrastructure model and

**[03:03]** mix of uh infrastructure model and

**[03:04]** mix of uh infrastructure model and application level companies and you know

**[03:06]** application level companies and you know

**[03:06]** application level companies and you know one more are my kids coming up yet?

**[03:09]** one more are my kids coming up yet?

**[03:09]** one more are my kids coming up yet? Okay, cool. Um one more uh just

**[03:13]** Okay, cool. Um one more uh just

**[03:13]** Okay, cool. Um one more uh just observation from the last two and a half

**[03:15]** observation from the last two and a half

**[03:15]** observation from the last two and a half three years of doing venture. I I was an

**[03:17]** three years of doing venture. I I was an

**[03:17]** three years of doing venture. I I was an investor for about 10 years before that

**[03:19]** investor for about 10 years before that

**[03:20]** investor for about 10 years before that is I have never seen the like just the

**[03:23]** is I have never seen the like just the

**[03:23]** is I have never seen the like just the uptake from users that has been possible

**[03:26]** uptake from users that has been possible

**[03:26]** uptake from users that has been possible in the last couple years. I'm sure all

**[03:28]** in the last couple years. I'm sure all

**[03:28]** in the last couple years. I'm sure all of you have experienced that it is not

**[03:30]** of you have experienced that it is not

**[03:30]** of you have experienced that it is not trivial. Um you know AI product and AI

**[03:33]** trivial. Um you know AI product and AI

**[03:33]** trivial. Um you know AI product and AI engineering uh and this is kind of the

**[03:36]** engineering uh and this is kind of the

**[03:36]** engineering uh and this is kind of the theme of my talk so I'm sorry to give

**[03:37]** theme of my talk so I'm sorry to give

**[03:37]** theme of my talk so I'm sorry to give away the punch line but it's quite a bit

**[03:39]** away the punch line but it's quite a bit

**[03:39]** away the punch line but it's quite a bit harder than people had hoped. Um but the

**[03:42]** harder than people had hoped. Um but the

**[03:42]** harder than people had hoped. Um but the the value creation is massive. Um, we

**[03:44]** the value creation is massive. Um, we

**[03:44]** the value creation is massive. Um, we see companies going from 0 to 10, 50,

**[03:47]** see companies going from 0 to 10, 50,

**[03:48]** see companies going from 0 to 10, 50, 100 million in run rate very, very

**[03:50]** 100 million in run rate very, very

**[03:50]** 100 million in run rate very, very quickly, faster than we've ever seen in

**[03:51]** quickly, faster than we've ever seen in

**[03:51]** quickly, faster than we've ever seen in any technology revolution before. Um,

**[03:54]** any technology revolution before. Um,

**[03:54]** any technology revolution before. Um, and I get asked a lot like where are we

**[03:57]** and I get asked a lot like where are we

**[03:57]** and I get asked a lot like where are we in the AI hype cycle? Is the winter

**[03:59]** in the AI hype cycle? Is the winter

**[03:59]** in the AI hype cycle? Is the winter coming? Is this like infinite AI summer?


### [04:00 - 05:00]

**[04:02]** coming? Is this like infinite AI summer?

**[04:02]** coming? Is this like infinite AI summer? And I would say um having actually been

**[04:05]** And I would say um having actually been

**[04:05]** And I would say um having actually been an investor or an operator through a

**[04:07]** an investor or an operator through a

**[04:07]** an investor or an operator through a macro cycle at this point like I try to

**[04:10]** macro cycle at this point like I try to

**[04:10]** macro cycle at this point like I try to pay very little attention to what the

**[04:12]** pay very little attention to what the

**[04:12]** pay very little attention to what the marketing world is saying or even what

**[04:13]** marketing world is saying or even what

**[04:13]** marketing world is saying or even what the markets are saying, right? Because

**[04:15]** the markets are saying, right? Because

**[04:16]** the markets are saying, right? Because you know if you're if you're an operator

**[04:17]** you know if you're if you're an operator

**[04:17]** you know if you're if you're an operator or an investor

**[04:19]** or an investor

**[04:19]** or an investor maybe you care about what the stock

**[04:21]** maybe you care about what the stock

**[04:21]** maybe you care about what the stock price does every day, but really you

**[04:23]** price does every day, but really you

**[04:23]** price does every day, but really you want to figure out if the company you're

**[04:24]** want to figure out if the company you're

**[04:24]** want to figure out if the company you're working for or starting is going to work

**[04:26]** working for or starting is going to work

**[04:26]** working for or starting is going to work long term, right? And if the products

**[04:27]** long term, right? And if the products

**[04:28]** long term, right? And if the products are going to work long term. And the

**[04:29]** are going to work long term. And the

**[04:29]** are going to work long term. And the things that I get most excited about are

**[04:31]** things that I get most excited about are

**[04:31]** things that I get most excited about are seeing like crazy usage numbers. Okay.

**[04:37]** seeing like crazy usage numbers. Okay.

**[04:37]** seeing like crazy usage numbers. Okay. Thank you, amazing AV team.

**[04:41]** Thank you, amazing AV team.

**[04:41]** Thank you, amazing AV team. Okay, I'm gonna I'm gonna go real quick.

**[04:43]** Okay, I'm gonna I'm gonna go real quick.

**[04:43]** Okay, I'm gonna I'm gonna go real quick. Um,

**[04:45]** Um,

**[04:46]** Um, where are my presenter notes?

**[04:50]** where are my presenter notes?

**[04:50]** where are my presenter notes? Okay, we're we're just going to keep

**[04:51]** Okay, we're we're just going to keep

**[04:51]** Okay, we're we're just going to keep going. It's cool. It's cool. Um, so I

**[04:54]** going. It's cool. It's cool. Um, so I

**[04:54]** going. It's cool. It's cool. Um, so I want to talk really quickly about uh

**[04:56]** want to talk really quickly about uh

**[04:56]** want to talk really quickly about uh just a few things today. I think we lost

**[04:58]** just a few things today. I think we lost

**[04:58]** just a few things today. I think we lost a little bit of time, but let's let's

**[04:59]** a little bit of time, but let's let's

**[04:59]** a little bit of time, but let's let's say let's talk about capabilities, what


### [05:00 - 06:00]

**[05:01]** say let's talk about capabilities, what

**[05:01]** say let's talk about capabilities, what we're seeing work in the market, and

**[05:03]** we're seeing work in the market, and

**[05:03]** we're seeing work in the market, and then um uh maybe some advice on like

**[05:07]** then um uh maybe some advice on like

**[05:07]** then um uh maybe some advice on like what to build if those are, you know, a

**[05:09]** what to build if those are, you know, a

**[05:09]** what to build if those are, you know, a question you're considering. Uh I think

**[05:11]** question you're considering. Uh I think

**[05:11]** question you're considering. Uh I think the shorthand that we're going to use in

**[05:13]** the shorthand that we're going to use in

**[05:13]** the shorthand that we're going to use in this presentation is like cursor for X,

**[05:16]** this presentation is like cursor for X,

**[05:16]** this presentation is like cursor for X, right? Uh and I do think that's a really

**[05:17]** right? Uh and I do think that's a really

**[05:17]** right? Uh and I do think that's a really massive opportunity. Uh the first thing

**[05:20]** massive opportunity. Uh the first thing

**[05:20]** massive opportunity. Uh the first thing in capability for this past year is

**[05:22]** in capability for this past year is

**[05:22]** in capability for this past year is clearly reasoning. Um, reasoning is a

**[05:25]** clearly reasoning. Um, reasoning is a

**[05:25]** clearly reasoning. Um, reasoning is a new vector for scaling intelligence with

**[05:26]** new vector for scaling intelligence with

**[05:26]** new vector for scaling intelligence with more compute. The labs are really

**[05:28]** more compute. The labs are really

**[05:28]** more compute. The labs are really excited about this because they get to

**[05:29]** excited about this because they get to

**[05:29]** excited about this because they get to spend more money and get more output.

**[05:31]** spend more money and get more output.

**[05:31]** spend more money and get more output. Um, but we should also be really excited

**[05:33]** Um, but we should also be really excited

**[05:34]** Um, but we should also be really excited about this in terms of unlocking new

**[05:36]** about this in terms of unlocking new

**[05:36]** about this in terms of unlocking new capabilities. Right? If you just put

**[05:38]** capabilities. Right? If you just put

**[05:38]** capabilities. Right? If you just put aside how it works, it's a confidence

**[05:40]** aside how it works, it's a confidence

**[05:40]** aside how it works, it's a confidence boosting implementation detail. Um, but

**[05:43]** boosting implementation detail. Um, but

**[05:43]** boosting implementation detail. Um, but we should expect more capability. You're

**[05:45]** we should expect more capability. You're

**[05:45]** we should expect more capability. You're unlocking a new set of use cases like

**[05:48]** unlocking a new set of use cases like

**[05:48]** unlocking a new set of use cases like transparent highstakes decisions where

**[05:51]** transparent highstakes decisions where

**[05:51]** transparent highstakes decisions where showing the work matters. uh sequential

**[05:53]** showing the work matters. uh sequential

**[05:53]** showing the work matters. uh sequential problems, problems where you need to do

**[05:55]** problems, problems where you need to do

**[05:55]** problems, problems where you need to do systematic search. I I think this looks

**[05:57]** systematic search. I I think this looks

**[05:57]** systematic search. I I think this looks like a lot of problems that we're

**[05:58]** like a lot of problems that we're

**[05:58]** like a lot of problems that we're excited about and um face in knowledge


### [06:00 - 07:00]

**[06:01]** excited about and um face in knowledge

**[06:01]** excited about and um face in knowledge work every day. Uh as you have just seen

**[06:04]** work every day. Uh as you have just seen

**[06:04]** work every day. Uh as you have just seen demos of and I'm sure are working on

**[06:06]** demos of and I'm sure are working on

**[06:06]** demos of and I'm sure are working on given reasoning, people are really

**[06:08]** given reasoning, people are really

**[06:08]** given reasoning, people are really excited about agents. um to put a you

**[06:12]** excited about agents. um to put a you

**[06:12]** excited about agents. um to put a you know I want to do like the Steve Balmer

**[06:14]** know I want to do like the Steve Balmer

**[06:14]** know I want to do like the Steve Balmer impression that's like agents agents

**[06:15]** impression that's like agents agents

**[06:15]** impression that's like agents agents agents agents agents agents but uh I um

**[06:19]** agents agents agents agents but uh I um

**[06:19]** agents agents agents agents but uh I um you have to give me more than 12 minutes

**[06:21]** you have to give me more than 12 minutes

**[06:21]** you have to give me more than 12 minutes to like get that sweaty

**[06:24]** to like get that sweaty

**[06:24]** to like get that sweaty uh but but like the non-marketing

**[06:26]** uh but but like the non-marketing

**[06:26]** uh but but like the non-marketing definition that I think of is it's

**[06:28]** definition that I think of is it's

**[06:28]** definition that I think of is it's software that um uh it takes some set of

**[06:33]** software that um uh it takes some set of

**[06:33]** software that um uh it takes some set of steps it like plans it includes AI it

**[06:36]** steps it like plans it includes AI it

**[06:36]** steps it like plans it includes AI it takes ownership of a task and it can

**[06:38]** takes ownership of a task and it can

**[06:38]** takes ownership of a task and it can hold a goal in memory

**[06:39]** hold a goal in memory

**[06:39]** hold a goal in memory you know, try different hypotheses,

**[06:41]** you know, try different hypotheses,

**[06:41]** you know, try different hypotheses, backtrack. It ranges from super

**[06:43]** backtrack. It ranges from super

**[06:43]** backtrack. It ranges from super sophisticated to super simple. Um, some

**[06:45]** sophisticated to super simple. Um, some

**[06:45]** sophisticated to super simple. Um, some of the tools that might use to

**[06:47]** of the tools that might use to

**[06:47]** of the tools that might use to accomplish a task include other models

**[06:49]** accomplish a task include other models

**[06:49]** accomplish a task include other models or search. And largely, it's just like

**[06:52]** or search. And largely, it's just like

**[06:52]** or search. And largely, it's just like AI systems that do something. Um, and

**[06:54]** AI systems that do something. Um, and

**[06:54]** AI systems that do something. Um, and that's not a chatbot that looks more

**[06:56]** that's not a chatbot that looks more

**[06:56]** that's not a chatbot that looks more like a colleague. Uh, and you know, one

**[06:59]** like a colleague. Uh, and you know, one

**[06:59]** like a colleague. Uh, and you know, one thing that I think we have a really


### [07:00 - 08:00]

**[07:00]** thing that I think we have a really

**[07:00]** thing that I think we have a really unique vantage point on is, uh, we back

**[07:03]** unique vantage point on is, uh, we back

**[07:03]** unique vantage point on is, uh, we back a small number of companies at

**[07:04]** a small number of companies at

**[07:04]** a small number of companies at conviction, but we also run a grant

**[07:06]** conviction, but we also run a grant

**[07:06]** conviction, but we also run a grant program for AI startups. It's called

**[07:08]** program for AI startups. It's called

**[07:08]** program for AI startups. It's called Embed. We get thousands of applications

**[07:09]** Embed. We get thousands of applications

**[07:09]** Embed. We get thousands of applications every year. Um, and includes like user

**[07:11]** every year. Um, and includes like user

**[07:12]** every year. Um, and includes like user data and revenue data and like really

**[07:13]** data and revenue data and like really

**[07:13]** data and revenue data and like really amazing people and the number of agent

**[07:16]** amazing people and the number of agent

**[07:16]** amazing people and the number of agent startups has gone up 50% over the last

**[07:18]** startups has gone up 50% over the last

**[07:18]** startups has gone up 50% over the last year and a lot of them are working like

**[07:20]** year and a lot of them are working like

**[07:20]** year and a lot of them are working like we do see stuff that's working in the

**[07:21]** we do see stuff that's working in the

**[07:22]** we do see stuff that's working in the real world and uh that's super exciting.

**[07:24]** real world and uh that's super exciting.

**[07:24]** real world and uh that's super exciting. Uh, other modalities are progressing

**[07:26]** Uh, other modalities are progressing

**[07:26]** Uh, other modalities are progressing too. I'm sure a lot of people are using

**[07:28]** too. I'm sure a lot of people are using

**[07:28]** too. I'm sure a lot of people are using voice, video, image generation um, even

**[07:31]** voice, video, image generation um, even

**[07:31]** voice, video, image generation um, even beyond you know studio gibli. But you

**[07:34]** beyond you know studio gibli. But you

**[07:34]** beyond you know studio gibli. But you have companies like Hey Genen and 11 and

**[07:36]** have companies like Hey Genen and 11 and

**[07:36]** have companies like Hey Genen and 11 and Midjourney that are rocketing past 50

**[07:38]** Midjourney that are rocketing past 50

**[07:38]** Midjourney that are rocketing past 50 million of AR. These are real businesses

**[07:40]** million of AR. These are real businesses

**[07:40]** million of AR. These are real businesses now. Um, I want to see if I can quickly

**[07:43]** now. Um, I want to see if I can quickly

**[07:43]** now. Um, I want to see if I can quickly play for you. They told me to express

**[07:45]** play for you. They told me to express

**[07:45]** play for you. They told me to express myself, so I did. They told me to

**[07:47]** myself, so I did. They told me to

**[07:47]** myself, so I did. They told me to express myself, so I did. Now I'm banned

**[07:50]** express myself, so I did. Now I'm banned

**[07:50]** express myself, so I did. Now I'm banned from three coffee shops. Hands can hurt

**[07:53]** from three coffee shops. Hands can hurt

**[07:53]** from three coffee shops. Hands can hurt or heal. That's the difference between

**[07:55]** or heal. That's the difference between

**[07:55]** or heal. That's the difference between chaos and creation. So if you're

**[07:57]** chaos and creation. So if you're

**[07:57]** chaos and creation. So if you're wondering where Q3 is headed, So if

**[07:59]** wondering where Q3 is headed, So if

**[07:59]** wondering where Q3 is headed, So if you're wondering where Q3 is headed,


### [08:00 - 09:00]

**[08:01]** you're wondering where Q3 is headed,

**[08:01]** you're wondering where Q3 is headed, here's the thing. Consistency always

**[08:03]** here's the thing. Consistency always

**[08:03]** here's the thing. Consistency always beats urgency. We've got the projections

**[08:06]** beats urgency. We've got the projections

**[08:06]** beats urgency. We've got the projections ready and let's just say it's looking

**[08:08]** ready and let's just say it's looking

**[08:08]** ready and let's just say it's looking solid. I would definitely recommend it

**[08:10]** solid. I would definitely recommend it

**[08:10]** solid. I would definitely recommend it to anyone. I would definitely recommend

**[08:12]** to anyone. I would definitely recommend

**[08:12]** to anyone. I would definitely recommend it to So I I think like if you just are

**[08:15]** it to So I I think like if you just are

**[08:15]** it to So I I think like if you just are looking for artifacts of improvement,

**[08:17]** looking for artifacts of improvement,

**[08:17]** looking for artifacts of improvement, this is from a company called Hey Jen.

**[08:19]** this is from a company called Hey Jen.

**[08:19]** this is from a company called Hey Jen. Um you can make clones of yourself of

**[08:21]** Um you can make clones of yourself of

**[08:22]** Um you can make clones of yourself of fake people and like you have gestures

**[08:24]** fake people and like you have gestures

**[08:24]** fake people and like you have gestures and expressions that uh reflect emotion

**[08:27]** and expressions that uh reflect emotion

**[08:27]** and expressions that uh reflect emotion and content now, right? So these models

**[08:29]** and content now, right? So these models

**[08:30]** and content now, right? So these models work together and like I don't know

**[08:31]** work together and like I don't know

**[08:31]** work together and like I don't know about you guys but looking at that last

**[08:33]** about you guys but looking at that last

**[08:33]** about you guys but looking at that last gal like I feel influenced. I don't know

**[08:34]** gal like I feel influenced. I don't know

**[08:34]** gal like I feel influenced. I don't know what the bunny is but I would buy it. Um

**[08:37]** what the bunny is but I would buy it. Um

**[08:37]** what the bunny is but I would buy it. Um and and and so I think like huge swaths

**[08:39]** and and and so I think like huge swaths

**[08:39]** and and and so I think like huge swaths of the economy are going to be affected

**[08:40]** of the economy are going to be affected

**[08:40]** of the economy are going to be affected by this sort of multimodality. Um some

**[08:43]** by this sort of multimodality. Um some

**[08:43]** by this sort of multimodality. Um some investors or operators would say

**[08:45]** investors or operators would say

**[08:45]** investors or operators would say multimodality would just be for niche

**[08:47]** multimodality would just be for niche

**[08:47]** multimodality would just be for niche verticals that enterprises don't have

**[08:49]** verticals that enterprises don't have

**[08:49]** verticals that enterprises don't have you know your average enterprise doesn't

**[08:51]** you know your average enterprise doesn't

**[08:51]** you know your average enterprise doesn't have that much voice video image data

**[08:53]** have that much voice video image data

**[08:53]** have that much voice video image data today. Um, but I think that changes,

**[08:55]** today. Um, but I think that changes,

**[08:55]** today. Um, but I think that changes, right? When you can do stuff with this

**[08:56]** right? When you can do stuff with this

**[08:56]** right? When you can do stuff with this data, when it is structured and

**[08:58]** data, when it is structured and

**[08:58]** data, when it is structured and understood, there's more reason to


### [09:00 - 10:00]

**[09:00]** understood, there's more reason to

**[09:00]** understood, there's more reason to capture it. And I think of like how much

**[09:03]** capture it. And I think of like how much

**[09:03]** capture it. And I think of like how much video do all of us watch every day? It's

**[09:04]** video do all of us watch every day? It's

**[09:04]** video do all of us watch every day? It's one of the highest bandwidth

**[09:06]** one of the highest bandwidth

**[09:06]** one of the highest bandwidth communication methods, and we're just

**[09:07]** communication methods, and we're just

**[09:07]** communication methods, and we're just going to use more of it. Um, we think

**[09:09]** going to use more of it. Um, we think

**[09:09]** going to use more of it. Um, we think voice is where we're going to see uh

**[09:11]** voice is where we're going to see uh

**[09:11]** voice is where we're going to see uh applications first in business workflows

**[09:14]** applications first in business workflows

**[09:14]** applications first in business workflows um because it's already a very natural

**[09:16]** um because it's already a very natural

**[09:16]** um because it's already a very natural communication mode. So, uh, everything

**[09:18]** communication mode. So, uh, everything

**[09:18]** communication mode. So, uh, everything from medical consults to lead

**[09:20]** from medical consults to lead

**[09:20]** from medical consults to lead generation, places you already had

**[09:23]** generation, places you already had

**[09:23]** generation, places you already had business voice, you just couldn't scale

**[09:24]** business voice, you just couldn't scale

**[09:24]** business voice, you just couldn't scale it before. Uh, I I think that's where

**[09:26]** it before. Uh, I I think that's where

**[09:26]** it before. Uh, I I think that's where we're going to see it first. But as

**[09:27]** we're going to see it first. But as

**[09:27]** we're going to see it first. But as these other modalities become more

**[09:30]** these other modalities become more

**[09:30]** these other modalities become more controllable and also less costly, we

**[09:32]** controllable and also less costly, we

**[09:32]** controllable and also less costly, we should see all of them. Uh, I I think

**[09:34]** should see all of them. Uh, I I think

**[09:34]** should see all of them. Uh, I I think it's safe to say you can expect

**[09:35]** it's safe to say you can expect

**[09:35]** it's safe to say you can expect capability improvement in every part of

**[09:39]** capability improvement in every part of

**[09:39]** capability improvement in every part of the model layer, which is really

**[09:40]** the model layer, which is really

**[09:40]** the model layer, which is really exciting. A lot of people were talking

**[09:41]** exciting. A lot of people were talking

**[09:41]** exciting. A lot of people were talking about the uh the data wall or like the

**[09:44]** about the uh the data wall or like the

**[09:44]** about the uh the data wall or like the end of AI summer, but for anybody who's

**[09:46]** end of AI summer, but for anybody who's

**[09:46]** end of AI summer, but for anybody who's building applications, I I'm at least to

**[09:49]** building applications, I I'm at least to

**[09:49]** building applications, I I'm at least to tell you one person's opinion is uh it's

**[09:51]** tell you one person's opinion is uh it's

**[09:51]** tell you one person's opinion is uh it's not coming. Um and and then usefully for

**[09:55]** not coming. Um and and then usefully for

**[09:55]** not coming. Um and and then usefully for all of us, uh that market for model

**[09:58]** all of us, uh that market for model

**[09:58]** all of us, uh that market for model capabilities is getting more


### [10:00 - 11:00]

**[10:00]** capabilities is getting more

**[10:00]** capabilities is getting more competitive, not less. Um Sam Alman

**[10:03]** competitive, not less. Um Sam Alman

**[10:03]** competitive, not less. Um Sam Alman himself, I think, said it best. Last

**[10:05]** himself, I think, said it best. Last

**[10:05]** himself, I think, said it best. Last year's model is a commodity, which is a

**[10:07]** year's model is a commodity, which is a

**[10:07]** year's model is a commodity, which is a scary thing for a model provider to say,

**[10:09]** scary thing for a model provider to say,

**[10:09]** scary thing for a model provider to say, because last year's model is now pretty

**[10:10]** because last year's model is now pretty

**[10:10]** because last year's model is now pretty damn good, right? The numbers tell the

**[10:12]** damn good, right? The numbers tell the

**[10:12]** damn good, right? The numbers tell the story. GPT4 went from $30 per million

**[10:15]** story. GPT4 went from $30 per million

**[10:15]** story. GPT4 went from $30 per million tokens to $2 in about 18 months. The

**[10:18]** tokens to $2 in about 18 months. The

**[10:18]** tokens to $2 in about 18 months. The distilled versions of that are like now

**[10:20]** distilled versions of that are like now

**[10:20]** distilled versions of that are like now 10 cents. So, we can really use them

**[10:21]** 10 cents. So, we can really use them

**[10:21]** 10 cents. So, we can really use them very broadly. Um, if you look at this

**[10:24]** very broadly. Um, if you look at this

**[10:24]** very broadly. Um, if you look at this chart, uh, green is Google, yellow is

**[10:27]** chart, uh, green is Google, yellow is

**[10:27]** chart, uh, green is Google, yellow is anthropic. So, you see, you know, it's a

**[10:29]** anthropic. So, you see, you know, it's a

**[10:29]** anthropic. So, you see, you know, it's a real mix. This is data from Open Router.

**[10:31]** real mix. This is data from Open Router.

**[10:31]** real mix. This is data from Open Router. So, thank you Open Router for that. But

**[10:34]** So, thank you Open Router for that. But

**[10:34]** So, thank you Open Router for that. But um you really saw Claude cut into

**[10:36]** um you really saw Claude cut into

**[10:36]** um you really saw Claude cut into OpenAI's market share and Google come

**[10:39]** OpenAI's market share and Google come

**[10:39]** OpenAI's market share and Google come roaring back with Gemini. Uh this data

**[10:41]** roaring back with Gemini. Uh this data

**[10:41]** roaring back with Gemini. Uh this data is obviously a little biased because a

**[10:42]** is obviously a little biased because a

**[10:42]** is obviously a little biased because a lot of people just go direct to OpenAI,

**[10:43]** lot of people just go direct to OpenAI,

**[10:44]** lot of people just go direct to OpenAI, but if you're into multimodel that there

**[10:45]** but if you're into multimodel that there

**[10:45]** but if you're into multimodel that there really is a mix and you do have credible

**[10:47]** really is a mix and you do have credible

**[10:47]** really is a mix and you do have credible new players like SSI and thinking

**[10:49]** new players like SSI and thinking

**[10:49]** new players like SSI and thinking machines, some of the best researchers

**[10:51]** machines, some of the best researchers

**[10:51]** machines, some of the best researchers in the business with orthogonal

**[10:53]** in the business with orthogonal

**[10:53]** in the business with orthogonal technical approaches um entering the

**[10:55]** technical approaches um entering the

**[10:55]** technical approaches um entering the frey as well. And I'm sure many of you

**[10:57]** frey as well. And I'm sure many of you

**[10:57]** frey as well. And I'm sure many of you have experimented with DeepSeek uh

**[10:59]** have experimented with DeepSeek uh

**[10:59]** have experimented with DeepSeek uh coming out with releases of you know


### [11:00 - 12:00]

**[11:02]** coming out with releases of you know

**[11:02]** coming out with releases of you know both base and reasoning models that are

**[11:05]** both base and reasoning models that are

**[11:05]** both base and reasoning models that are uh reasonably competitive with a claimed

**[11:07]** uh reasonably competitive with a claimed

**[11:07]** uh reasonably competitive with a claimed fraction of the training cost like we

**[11:09]** fraction of the training cost like we

**[11:09]** fraction of the training cost like we should just assume that open source will

**[11:11]** should just assume that open source will

**[11:11]** should just assume that open source will do as open source does and we can rely

**[11:13]** do as open source does and we can rely

**[11:13]** do as open source does and we can rely on the model market to compete for our

**[11:15]** on the model market to compete for our

**[11:15]** on the model market to compete for our business which is really exciting. Um

**[11:16]** business which is really exciting. Um

**[11:16]** business which is really exciting. Um and so the view is plan for a world that

**[11:18]** and so the view is plan for a world that

**[11:18]** and so the view is plan for a world that is multimodel. um tools like open router

**[11:21]** is multimodel. um tools like open router

**[11:21]** is multimodel. um tools like open router or inference platforms like base 10 help

**[11:23]** or inference platforms like base 10 help

**[11:23]** or inference platforms like base 10 help that uh and uh I think like be

**[11:25]** that uh and uh I think like be

**[11:25]** that uh and uh I think like be comfortable with that I I am okay so we

**[11:28]** comfortable with that I I am okay so we

**[11:28]** comfortable with that I I am okay so we have all this capability let's ship uh

**[11:30]** have all this capability let's ship uh

**[11:30]** have all this capability let's ship uh shift quickly to the application layer

**[11:32]** shift quickly to the application layer

**[11:32]** shift quickly to the application layer we have to start with cursor uh a

**[11:34]** we have to start with cursor uh a

**[11:34]** we have to start with cursor uh a million to 100 million of AR in 12

**[11:36]** million to 100 million of AR in 12

**[11:36]** million to 100 million of AR in 12 months and half a million developers I

**[11:38]** months and half a million developers I

**[11:38]** months and half a million developers I assume all of you uh zero sales people

**[11:41]** assume all of you uh zero sales people

**[11:41]** assume all of you uh zero sales people to start that's not growth that is a

**[11:43]** to start that's not growth that is a

**[11:43]** to start that's not growth that is a killer application um cognition which

**[11:45]** killer application um cognition which

**[11:45]** killer application um cognition which started with more autonomy is already

**[11:47]** started with more autonomy is already

**[11:47]** started with more autonomy is already the top committer in many companies

**[11:49]** the top committer in many companies

**[11:49]** the top committer in many companies feeling a little threatened but also

**[11:51]** feeling a little threatened but also

**[11:51]** feeling a little threatened but also excited because recruiting is hard. And

**[11:52]** excited because recruiting is hard. And

**[11:52]** excited because recruiting is hard. And then Windsurf who's on a tear itself and

**[11:55]** then Windsurf who's on a tear itself and

**[11:55]** then Windsurf who's on a tear itself and really beloved is being acquired by

**[11:56]** really beloved is being acquired by

**[11:56]** really beloved is being acquired by OpenAI for $3 billion. So we know for

**[11:59]** OpenAI for $3 billion. So we know for

**[11:59]** OpenAI for $3 billion. So we know for sure that the labs don't think that they


### [12:00 - 13:00]

**[12:02]** sure that the labs don't think that they

**[12:02]** sure that the labs don't think that they can just you know steamroll everyone

**[12:04]** can just you know steamroll everyone

**[12:04]** can just you know steamroll everyone right lovable and bolt hit 30 million of

**[12:07]** right lovable and bolt hit 30 million of

**[12:07]** right lovable and bolt hit 30 million of AR each in a handful of weeks uh helping

**[12:11]** AR each in a handful of weeks uh helping

**[12:11]** AR each in a handful of weeks uh helping non-engineers vibe as well. So you know

**[12:14]** non-engineers vibe as well. So you know

**[12:14]** non-engineers vibe as well. So you know our our our ranks are expanding. Um and

**[12:17]** our our our ranks are expanding. Um and

**[12:17]** our our our ranks are expanding. Um and I think it's useful to just like analyze

**[12:19]** I think it's useful to just like analyze

**[12:19]** I think it's useful to just like analyze a little bit why code was first. Uh

**[12:21]** a little bit why code was first. Uh

**[12:21]** a little bit why code was first. Uh fundamentally it is text with it's log

**[12:25]** fundamentally it is text with it's log

**[12:25]** fundamentally it is text with it's log it's like logical language with

**[12:26]** it's like logical language with

**[12:26]** it's like logical language with structure right so much of coding is

**[12:29]** structure right so much of coding is

**[12:29]** structure right so much of coding is sophisticated boilerplate like we all

**[12:31]** sophisticated boilerplate like we all

**[12:31]** sophisticated boilerplate like we all love engineering but some of it is like

**[12:33]** love engineering but some of it is like

**[12:33]** love engineering but some of it is like craft work not new algorithm work um you

**[12:36]** craft work not new algorithm work um you

**[12:36]** craft work not new algorithm work um you don't need AGI to write a like uh an API

**[12:39]** don't need AGI to write a like uh an API

**[12:39]** don't need AGI to write a like uh an API endpoint or um a react component.

**[12:42]** endpoint or um a react component.

**[12:42]** endpoint or um a react component. Second, you have deterministic

**[12:44]** Second, you have deterministic

**[12:44]** Second, you have deterministic validation. You can automatically check

**[12:46]** validation. You can automatically check

**[12:46]** validation. You can automatically check if code works, run tests, compile,

**[12:49]** if code works, run tests, compile,

**[12:49]** if code works, run tests, compile, execute, do things developers would do.

**[12:51]** execute, do things developers would do.

**[12:51]** execute, do things developers would do. And third, researchers believe code is

**[12:54]** And third, researchers believe code is

**[12:54]** And third, researchers believe code is crucial for AGI, right? So, they poured

**[12:56]** crucial for AGI, right? So, they poured

**[12:56]** crucial for AGI, right? So, they poured resources into it. Um, and uh code


### [13:00 - 14:00]

**[13:00]** resources into it. Um, and uh code

**[13:00]** resources into it. Um, and uh code became a key benchmark and a training

**[13:01]** became a key benchmark and a training

**[13:02]** became a key benchmark and a training priority and an area for data

**[13:03]** priority and an area for data

**[13:03]** priority and an area for data collection. But I think the last point

**[13:05]** collection. But I think the last point

**[13:05]** collection. But I think the last point is um the money point to me. Uh

**[13:08]** is um the money point to me. Uh

**[13:08]** is um the money point to me. Uh engineers built tools for engineers.

**[13:11]** engineers built tools for engineers.

**[13:11]** engineers built tools for engineers. They understood the workflow intimately

**[13:12]** They understood the workflow intimately

**[13:12]** They understood the workflow intimately and that made all the difference. And

**[13:14]** and that made all the difference. And

**[13:14]** and that made all the difference. And that last part is the playbook for every

**[13:16]** that last part is the playbook for every

**[13:16]** that last part is the playbook for every other industry. I'm sure people are

**[13:17]** other industry. I'm sure people are

**[13:17]** other industry. I'm sure people are building things that serve beyond

**[13:19]** building things that serve beyond

**[13:19]** building things that serve beyond engineers. And I don't think the winners

**[13:21]** engineers. And I don't think the winners

**[13:21]** engineers. And I don't think the winners will just be AI experts learning those

**[13:24]** will just be AI experts learning those

**[13:24]** will just be AI experts learning those domains. They'll be customer centric

**[13:26]** domains. They'll be customer centric

**[13:26]** domains. They'll be customer centric like problem centric builders who

**[13:29]** like problem centric builders who

**[13:29]** like problem centric builders who understand AI and then redesign

**[13:30]** understand AI and then redesign

**[13:30]** understand AI and then redesign workflows from first principles around

**[13:32]** workflows from first principles around

**[13:32]** workflows from first principles around manipulating those models. Um and so I

**[13:34]** manipulating those models. Um and so I

**[13:34]** manipulating those models. Um and so I think that's really the opportunity to

**[13:35]** think that's really the opportunity to

**[13:35]** think that's really the opportunity to build cursor for X. Um let's think a

**[13:38]** build cursor for X. Um let's think a

**[13:38]** build cursor for X. Um let's think a little bit about what that means. Cursor

**[13:41]** little bit about what that means. Cursor

**[13:41]** little bit about what that means. Cursor is not a single model. Uh you know one

**[13:44]** is not a single model. Uh you know one

**[13:44]** is not a single model. Uh you know one model's doing diffs, one's doing merge,

**[13:46]** model's doing diffs, one's doing merge,

**[13:46]** model's doing diffs, one's doing merge, one's embedding the files. They

**[13:47]** one's embedding the files. They

**[13:47]** one's embedding the files. They manipulate and package up the context.

**[13:50]** manipulate and package up the context.

**[13:50]** manipulate and package up the context. They prompt the models very skillfully.

**[13:52]** They prompt the models very skillfully.

**[13:52]** They prompt the models very skillfully. They let engineers avoid repetitive

**[13:54]** They let engineers avoid repetitive

**[13:54]** They let engineers avoid repetitive tasks and standardize with things like

**[13:56]** tasks and standardize with things like

**[13:56]** tasks and standardize with things like um cursor rules. And then if you're

**[13:58]** um cursor rules. And then if you're

**[13:58]** um cursor rules. And then if you're using cursor in a team or even yourself


### [14:00 - 15:00]

**[14:00]** using cursor in a team or even yourself

**[14:00]** using cursor in a team or even yourself regularly, retrieval accuracy gets

**[14:02]** regularly, retrieval accuracy gets

**[14:02]** regularly, retrieval accuracy gets better the more you use it with coverage

**[14:04]** better the more you use it with coverage

**[14:04]** better the more you use it with coverage and freshness. And so all of this

**[14:06]** and freshness. And so all of this

**[14:06]** and freshness. And so all of this happens in a UX that makes sense, right?

**[14:08]** happens in a UX that makes sense, right?

**[14:08]** happens in a UX that makes sense, right? Like I, you know, I use VS Code. I'm

**[14:10]** Like I, you know, I use VS Code. I'm

**[14:10]** Like I, you know, I use VS Code. I'm familiar with it. My shortcuts work. Um,

**[14:12]** familiar with it. My shortcuts work. Um,

**[14:12]** familiar with it. My shortcuts work. Um, and I make it safe to say yes, right?

**[14:15]** and I make it safe to say yes, right?

**[14:15]** and I make it safe to say yes, right? Like green for add and red for subtract

**[14:18]** Like green for add and red for subtract

**[14:18]** Like green for add and red for subtract makes sense. I can scroll through it.

**[14:19]** makes sense. I can scroll through it.

**[14:20]** makes sense. I can scroll through it. Um, and it's fast enough that I don't

**[14:21]** Um, and it's fast enough that I don't

**[14:21]** Um, and it's fast enough that I don't get frustrated. So my my view is cursor

**[14:24]** get frustrated. So my my view is cursor

**[14:24]** get frustrated. So my my view is cursor if it's a wrapper, it's like a very nice

**[14:26]** if it's a wrapper, it's like a very nice

**[14:26]** if it's a wrapper, it's like a very nice thick perhaps 14 or 15 billion dollar

**[14:29]** thick perhaps 14 or 15 billion dollar

**[14:29]** thick perhaps 14 or 15 billion dollar wrapper, right? It's like if your

**[14:30]** wrapper, right? It's like if your

**[14:30]** wrapper, right? It's like if your burrito was 80% wrap and 20% fill, but

**[14:35]** burrito was 80% wrap and 20% fill, but

**[14:35]** burrito was 80% wrap and 20% fill, but you got to choose the fill and there's

**[14:36]** you got to choose the fill and there's

**[14:36]** you got to choose the fill and there's like an empty like an open market for

**[14:38]** like an empty like an open market for

**[14:38]** like an empty like an open market for fill, right? Um, and so where's the pro

**[14:41]** fill, right? Um, and so where's the pro

**[14:41]** fill, right? Um, and so where's the pro where's the value now? It may not be in

**[14:43]** where's the value now? It may not be in

**[14:43]** where's the value now? It may not be in the protein. It's kind of in the

**[14:44]** the protein. It's kind of in the

**[14:44]** the protein. It's kind of in the company. Um, so like if we try to

**[14:47]** company. Um, so like if we try to

**[14:47]** company. Um, so like if we try to generalize that recipe a little bit, if

**[14:50]** generalize that recipe a little bit, if

**[14:50]** generalize that recipe a little bit, if you are building a generic text box like

**[14:54]** you are building a generic text box like

**[14:54]** you are building a generic text box like unless you're just like learning to do

**[14:55]** unless you're just like learning to do

**[14:55]** unless you're just like learning to do this, please don't like OpenAI already

**[14:58]** this, please don't like OpenAI already

**[14:58]** this, please don't like OpenAI already one that or it's just not very valuable


### [15:00 - 16:00]

**[15:00]** one that or it's just not very valuable

**[15:00]** one that or it's just not very valuable to do. So your domain knowledge, your

**[15:02]** to do. So your domain knowledge, your

**[15:02]** to do. So your domain knowledge, your workflow knowledge can be the bootstrap.

**[15:04]** workflow knowledge can be the bootstrap.

**[15:04]** workflow knowledge can be the bootstrap. If you already know what users in your

**[15:07]** If you already know what users in your

**[15:07]** If you already know what users in your industry need, don't make them explain

**[15:09]** industry need, don't make them explain

**[15:09]** industry need, don't make them explain it. Uh, build products that show up

**[15:11]** it. Uh, build products that show up

**[15:11]** it. Uh, build products that show up informed. They collect and package

**[15:13]** informed. They collect and package

**[15:13]** informed. They collect and package context automatically including from

**[15:14]** context automatically including from

**[15:14]** context automatically including from other sources not just natural language

**[15:17]** other sources not just natural language

**[15:17]** other sources not just natural language presented to the models use the right

**[15:18]** presented to the models use the right

**[15:18]** presented to the models use the right models at the right time now known as

**[15:20]** models at the right time now known as

**[15:20]** models at the right time now known as orchestration and present the outputs to

**[15:23]** orchestration and present the outputs to

**[15:23]** orchestration and present the outputs to the users thoughtfully right um so I do

**[15:25]** the users thoughtfully right um so I do

**[15:25]** the users thoughtfully right um so I do not think this is the end of the guey uh

**[15:27]** not think this is the end of the guey uh

**[15:27]** not think this is the end of the guey uh I I think you can capture and enable

**[15:29]** I I think you can capture and enable

**[15:29]** I I think you can capture and enable workflow with these models and all this

**[15:31]** workflow with these models and all this

**[15:31]** workflow with these models and all this requires taste and a ton of work I' I'd

**[15:33]** requires taste and a ton of work I' I'd

**[15:33]** requires taste and a ton of work I' I'd argue that like some version of this

**[15:35]** argue that like some version of this

**[15:35]** argue that like some version of this recipe is much of the work each of us is

**[15:37]** recipe is much of the work each of us is

**[15:37]** recipe is much of the work each of us is going to do so don't listen to the labs

**[15:40]** going to do so don't listen to the labs

**[15:40]** going to do so don't listen to the labs from a user experience perspective The

**[15:42]** from a user experience perspective The

**[15:42]** from a user experience perspective The prompt is a bug, not a feature. I think

**[15:44]** prompt is a bug, not a feature. I think

**[15:44]** prompt is a bug, not a feature. I think it's like a stepping stone. Don't make

**[15:45]** it's like a stepping stone. Don't make

**[15:45]** it's like a stepping stone. Don't make me think as a user. The best AI

**[15:48]** me think as a user. The best AI

**[15:48]** me think as a user. The best AI products, they feel like mind readading

**[15:50]** products, they feel like mind readading

**[15:50]** products, they feel like mind readading because they are. Um, there's enormous

**[15:52]** because they are. Um, there's enormous

**[15:52]** because they are. Um, there's enormous headroom in building these products. And

**[15:54]** headroom in building these products. And

**[15:54]** headroom in building these products. And I I think that's really exciting because

**[15:55]** I I think that's really exciting because

**[15:55]** I I think that's really exciting because that's what most of us in this room have

**[15:56]** that's what most of us in this room have

**[15:56]** that's what most of us in this room have alpha on. Uh, what is a software company

**[15:59]** alpha on. Uh, what is a software company

**[15:59]** alpha on. Uh, what is a software company if not a very thick like workflow


### [16:00 - 17:00]

**[16:02]** if not a very thick like workflow

**[16:02]** if not a very thick like workflow wrapper most of the time? That's true in

**[16:04]** wrapper most of the time? That's true in

**[16:04]** wrapper most of the time? That's true in 2015. It's true in 2025.

**[16:08]** 2015. It's true in 2025.

**[16:08]** 2015. It's true in 2025. Um, besides code, where might you go

**[16:11]** Um, besides code, where might you go

**[16:11]** Um, besides code, where might you go apply this? We think the opportunities

**[16:14]** apply this? We think the opportunities

**[16:14]** apply this? We think the opportunities to build value around the LLMs exist in

**[16:16]** to build value around the LLMs exist in

**[16:16]** to build value around the LLMs exist in every vertical and profession. Uh, but

**[16:19]** every vertical and profession. Uh, but

**[16:19]** every vertical and profession. Uh, but here's something counterintuitive.

**[16:21]** here's something counterintuitive.

**[16:21]** here's something counterintuitive. Beyond coding, one of the things that

**[16:23]** Beyond coding, one of the things that

**[16:23]** Beyond coding, one of the things that I've been surprised by is that the most

**[16:25]** I've been surprised by is that the most

**[16:25]** I've been surprised by is that the most conservative low tech industries seem to

**[16:27]** conservative low tech industries seem to

**[16:27]** conservative low tech industries seem to be adopting AI fastest. We call this the

**[16:29]** be adopting AI fastest. We call this the

**[16:29]** be adopting AI fastest. We call this the AI leaprog effect internally. Um, these

**[16:32]** AI leaprog effect internally. Um, these

**[16:32]** AI leaprog effect internally. Um, these are three portfolio companies. Um,

**[16:33]** are three portfolio companies. Um,

**[16:33]** are three portfolio companies. Um, they're working. Sierra resolves 70% of

**[16:37]** they're working. Sierra resolves 70% of

**[16:37]** they're working. Sierra resolves 70% of uh customer service queries for their

**[16:39]** uh customer service queries for their

**[16:39]** uh customer service queries for their customers. They serve people that you

**[16:41]** customers. They serve people that you

**[16:41]** customers. They serve people that you know you guys use like SiriusXM or ADT.

**[16:44]** know you guys use like SiriusXM or ADT.

**[16:44]** know you guys use like SiriusXM or ADT. Harvey is you know two years in well

**[16:47]** Harvey is you know two years in well

**[16:47]** Harvey is you know two years in well over 70 million of ARR. It's AI is

**[16:50]** over 70 million of ARR. It's AI is

**[16:50]** over 70 million of ARR. It's AI is essential now to being competitive in

**[16:52]** essential now to being competitive in

**[16:52]** essential now to being competitive in the legal industry. Um there's a company

**[16:54]** the legal industry. Um there's a company

**[16:54]** the legal industry. Um there's a company called Open Evidence uh which helps

**[16:56]** called Open Evidence uh which helps

**[16:56]** called Open Evidence uh which helps doctors stay upto-date with medical

**[16:58]** doctors stay upto-date with medical

**[16:58]** doctors stay upto-date with medical research. You have to be a clinician to


### [17:00 - 18:00]

**[17:00]** research. You have to be a clinician to

**[17:00]** research. You have to be a clinician to use it but you know you give it your

**[17:02]** use it but you know you give it your

**[17:02]** use it but you know you give it your medical ID number and you can do

**[17:03]** medical ID number and you can do

**[17:03]** medical ID number and you can do intelligent search against um uh medical

**[17:06]** intelligent search against um uh medical

**[17:06]** intelligent search against um uh medical research uh at the point of clinical

**[17:09]** research uh at the point of clinical

**[17:09]** research uh at the point of clinical decisionmaking. Today it reaches a third

**[17:11]** decisionmaking. Today it reaches a third

**[17:11]** decisionmaking. Today it reaches a third of doctors in the US weekly and the

**[17:14]** of doctors in the US weekly and the

**[17:14]** of doctors in the US weekly and the average user uses it daily, right? And

**[17:16]** average user uses it daily, right? And

**[17:16]** average user uses it daily, right? And so I think there's just examples of, you

**[17:19]** so I think there's just examples of, you

**[17:19]** so I think there's just examples of, you know, huge value beyond chatbt. These

**[17:22]** know, huge value beyond chatbt. These

**[17:22]** know, huge value beyond chatbt. These are companies that know their customer

**[17:24]** are companies that know their customer

**[17:24]** are companies that know their customer and are solving real problems. As a as a

**[17:26]** and are solving real problems. As a as a

**[17:26]** and are solving real problems. As a as a piece of trivia that you may or may not

**[17:28]** piece of trivia that you may or may not

**[17:28]** piece of trivia that you may or may not know, um Brett at Sierra is the chairman

**[17:30]** know, um Brett at Sierra is the chairman

**[17:30]** know, um Brett at Sierra is the chairman of the board at OpenAI. Um OpenAI was

**[17:34]** of the board at OpenAI. Um OpenAI was

**[17:34]** of the board at OpenAI. Um OpenAI was Harvey's uh seed investor. And if you

**[17:38]** Harvey's uh seed investor. And if you

**[17:38]** Harvey's uh seed investor. And if you know these people are not fretting about

**[17:39]** know these people are not fretting about

**[17:40]** know these people are not fretting about thin rappers like I suggest you don't

**[17:41]** thin rappers like I suggest you don't

**[17:41]** thin rappers like I suggest you don't either. Okay. Finally, I'll make an

**[17:44]** either. Okay. Finally, I'll make an

**[17:44]** either. Okay. Finally, I'll make an observation. A lot of people are excited

**[17:46]** observation. A lot of people are excited

**[17:46]** observation. A lot of people are excited about full automation. Now I'm sweaty

**[17:48]** about full automation. Now I'm sweaty

**[17:48]** about full automation. Now I'm sweaty enough. So agents agents agents agents

**[17:50]** enough. So agents agents agents agents

**[17:50]** enough. So agents agents agents agents agents agents. Um but when we analyze

**[17:53]** agents agents. Um but when we analyze

**[17:53]** agents agents. Um but when we analyze the applications to embed I said you

**[17:55]** the applications to embed I said you

**[17:56]** the applications to embed I said you know it's gone up to 50% you know

**[17:58]** know it's gone up to 50% you know

**[17:58]** know it's gone up to 50% you know doubling a applications for agentic


### [18:00 - 19:00]

**[18:01]** doubling a applications for agentic

**[18:01]** doubling a applications for agentic startups in the last year. Um I I think

**[18:04]** startups in the last year. Um I I think

**[18:04]** startups in the last year. Um I I think some people think co-pilots are

**[18:06]** some people think co-pilots are

**[18:06]** some people think co-pilots are yesterday's news. They want to get to

**[18:07]** yesterday's news. They want to get to

**[18:08]** yesterday's news. They want to get to the endgame, right? Like you know your

**[18:09]** the endgame, right? Like you know your

**[18:09]** the endgame, right? Like you know your colleague and AGI. But in terms of what

**[18:12]** colleague and AGI. But in terms of what

**[18:12]** colleague and AGI. But in terms of what works, like the data on what's driving

**[18:14]** works, like the data on what's driving

**[18:14]** works, like the data on what's driving revenue, uh I think co-pilots are still

**[18:16]** revenue, uh I think co-pilots are still

**[18:16]** revenue, uh I think co-pilots are still really underrated. We see a whole

**[18:18]** really underrated. We see a whole

**[18:18]** really underrated. We see a whole spectrum of how much automation and I

**[18:21]** spectrum of how much automation and I

**[18:21]** spectrum of how much automation and I think the uh Iron Man analogy is still

**[18:23]** think the uh Iron Man analogy is still

**[18:23]** think the uh Iron Man analogy is still really great here. Tony Stark's Iron Man

**[18:26]** really great here. Tony Stark's Iron Man

**[18:26]** really great here. Tony Stark's Iron Man suit augments him, right? He can do all

**[18:27]** suit augments him, right? He can do all

**[18:28]** suit augments him, right? He can do all these amazing things, but could also fly

**[18:29]** these amazing things, but could also fly

**[18:30]** these amazing things, but could also fly around on command, could do some basic

**[18:32]** around on command, could do some basic

**[18:32]** around on command, could do some basic tasks without Tony. And my experience

**[18:35]** tasks without Tony. And my experience

**[18:35]** tasks without Tony. And my experience with these companies has been that human

**[18:36]** with these companies has been that human

**[18:36]** with these companies has been that human tolerance for failure or hallucinations

**[18:39]** tolerance for failure or hallucinations

**[18:39]** tolerance for failure or hallucinations or lack of reliability, it just reduces

**[18:41]** or lack of reliability, it just reduces

**[18:41]** or lack of reliability, it just reduces dramatically as latency increases,

**[18:43]** dramatically as latency increases,

**[18:43]** dramatically as latency increases, right? Um, so the path of least

**[18:45]** right? Um, so the path of least

**[18:45]** right? Um, so the path of least frustration today for many domains is to

**[18:47]** frustration today for many domains is to

**[18:47]** frustration today for many domains is to build great augmentation and then just

**[18:49]** build great augmentation and then just

**[18:49]** build great augmentation and then just ride the wave of capability because we

**[18:51]** ride the wave of capability because we

**[18:51]** ride the wave of capability because we know it's coming. And so my advice for

**[18:54]** know it's coming. And so my advice for

**[18:54]** know it's coming. And so my advice for many domains would think about like you

**[18:56]** many domains would think about like you

**[18:56]** many domains would think about like you know build the suit and you can extend

**[18:58]** know build the suit and you can extend

**[18:58]** know build the suit and you can extend out to the suit that flies on its own


### [19:00 - 20:00]

**[19:00]** out to the suit that flies on its own

**[19:00]** out to the suit that flies on its own once Tony or any of us is wearing it.

**[19:03]** once Tony or any of us is wearing it.

**[19:04]** once Tony or any of us is wearing it. Um

**[19:05]** Um

**[19:05]** Um I'm not going to go through each of

**[19:06]** I'm not going to go through each of

**[19:06]** I'm not going to go through each of these mostly because I lost time but um

**[19:09]** these mostly because I lost time but um

**[19:09]** these mostly because I lost time but um there are a ton of opportunities. We put

**[19:10]** there are a ton of opportunities. We put

**[19:10]** there are a ton of opportunities. We put requests for startups on our website.

**[19:12]** requests for startups on our website.

**[19:12]** requests for startups on our website. We're interested in a couple different

**[19:14]** We're interested in a couple different

**[19:14]** We're interested in a couple different categories of things. They go from uh um

**[19:18]** categories of things. They go from uh um

**[19:18]** categories of things. They go from uh um like just good fit for purpose like the

**[19:21]** like just good fit for purpose like the

**[19:21]** like just good fit for purpose like the law is a space of lots of text

**[19:23]** law is a space of lots of text

**[19:23]** law is a space of lots of text generation, right? Um to things that

**[19:26]** generation, right? Um to things that

**[19:26]** generation, right? Um to things that weren't possible before AI. My partner

**[19:28]** weren't possible before AI. My partner

**[19:28]** weren't possible before AI. My partner Mike will say like this is a really

**[19:30]** Mike will say like this is a really

**[19:30]** Mike will say like this is a really interesting era of machines

**[19:31]** interesting era of machines

**[19:32]** interesting era of machines interrogating humans. What can you do if

**[19:34]** interrogating humans. What can you do if

**[19:34]** interrogating humans. What can you do if you can go like collect data on demand

**[19:36]** you can go like collect data on demand

**[19:36]** you can go like collect data on demand from people? Um we could talk to every

**[19:38]** from people? Um we could talk to every

**[19:38]** from people? Um we could talk to every customer, not just the top 5% by

**[19:40]** customer, not just the top 5% by

**[19:40]** customer, not just the top 5% by contract value. Um, we could root cause

**[19:44]** contract value. Um, we could root cause

**[19:44]** contract value. Um, we could root cause every alert proactively, right? Versus

**[19:47]** every alert proactively, right? Versus

**[19:47]** every alert proactively, right? Versus like just firefight. Um, and the mental

**[19:49]** like just firefight. Um, and the mental

**[19:49]** like just firefight. Um, and the mental model is how can you build as if you had

**[19:50]** model is how can you build as if you had

**[19:50]** model is how can you build as if you had an army of compliant, infinitely patient

**[19:53]** an army of compliant, infinitely patient

**[19:53]** an army of compliant, infinitely patient knowledge workers. Um,

**[19:56]** knowledge workers. Um,

**[19:56]** knowledge workers. Um, you know, one aside here is I think

**[19:58]** you know, one aside here is I think

**[19:58]** you know, one aside here is I think there are many hard problems where like


### [20:00 - 21:00]

**[20:01]** there are many hard problems where like

**[20:01]** there are many hard problems where like the basic premise is the answer to them

**[20:03]** the basic premise is the answer to them

**[20:03]** the basic premise is the answer to them is not in common crawl, right? The

**[20:05]** is not in common crawl, right? The

**[20:05]** is not in common crawl, right? The reasoning around them is not in common

**[20:07]** reasoning around them is not in common

**[20:07]** reasoning around them is not in common crawl. So um this would be robotics,

**[20:09]** crawl. So um this would be robotics,

**[20:09]** crawl. So um this would be robotics, biology, material science, physics,

**[20:11]** biology, material science, physics,

**[20:11]** biology, material science, physics, simulation. Um they require clever data

**[20:15]** simulation. Um they require clever data

**[20:15]** simulation. Um they require clever data collection. Um probably interaction with

**[20:17]** collection. Um probably interaction with

**[20:17]** collection. Um probably interaction with atoms, not just bits. Super scary uh for

**[20:19]** atoms, not just bits. Super scary uh for

**[20:20]** atoms, not just bits. Super scary uh for a software person, but I think the juice

**[20:21]** a software person, but I think the juice

**[20:21]** a software person, but I think the juice is worth the squeeze, right? The same

**[20:23]** is worth the squeeze, right? The same

**[20:23]** is worth the squeeze, right? The same reasoning that crushes math olympiads

**[20:25]** reasoning that crushes math olympiads

**[20:25]** reasoning that crushes math olympiads can seemingly navigate molecular space.

**[20:27]** can seemingly navigate molecular space.

**[20:27]** can seemingly navigate molecular space. And I think there are some really

**[20:28]** And I think there are some really

**[20:28]** And I think there are some really fundamental questions for um human

**[20:30]** fundamental questions for um human

**[20:30]** fundamental questions for um human society that can be answered when people

**[20:32]** society that can be answered when people

**[20:32]** society that can be answered when people work on these problems. And uh it's it's

**[20:34]** work on these problems. And uh it's it's

**[20:34]** work on these problems. And uh it's it's really cool as a machine learning person

**[20:36]** really cool as a machine learning person

**[20:36]** really cool as a machine learning person to meet people in their at the top of

**[20:39]** to meet people in their at the top of

**[20:39]** to meet people in their at the top of their field at the intersection of

**[20:41]** their field at the intersection of

**[20:41]** their field at the intersection of machine learning and all of these other

**[20:42]** machine learning and all of these other

**[20:42]** machine learning and all of these other areas because like you guys would also

**[20:45]** areas because like you guys would also

**[20:45]** areas because like you guys would also the same architectures apply right and

**[20:47]** the same architectures apply right and

**[20:47]** the same architectures apply right and and that's just um that's really

**[20:49]** and that's just um that's really

**[20:49]** and that's just um that's really exciting.

**[20:51]** exciting.

**[20:51]** exciting. Um

**[20:53]** Um

**[20:53]** Um how should we think about defensibility?

**[20:55]** how should we think about defensibility?

**[20:56]** how should we think about defensibility? Did this advance?

**[20:59]** Did this advance?

**[20:59]** Did this advance? Okay. So, um, one last point and then


### [21:00 - 22:00]

**[21:01]** Okay. So, um, one last point and then

**[21:01]** Okay. So, um, one last point and then I'll conclude here. Uh, some would say

**[21:04]** I'll conclude here. Uh, some would say

**[21:04]** I'll conclude here. Uh, some would say stay out of the weight of the labs.

**[21:06]** stay out of the weight of the labs.

**[21:06]** stay out of the weight of the labs. Don't pick up pennies in front of the

**[21:07]** Don't pick up pennies in front of the

**[21:07]** Don't pick up pennies in front of the steamroller, right? But I would offer,

**[21:10]** steamroller, right? But I would offer,

**[21:10]** steamroller, right? But I would offer, um, what I think is an uncomfortable

**[21:11]** um, what I think is an uncomfortable

**[21:11]** um, what I think is an uncomfortable truth. Execution is the moat in AI. Um,

**[21:14]** truth. Execution is the moat in AI. Um,

**[21:14]** truth. Execution is the moat in AI. Um, and that's available to all of us.

**[21:16]** and that's available to all of us.

**[21:16]** and that's available to all of us. Cursor arguably did not invent code

**[21:18]** Cursor arguably did not invent code

**[21:18]** Cursor arguably did not invent code completion. They did not invent the

**[21:20]** completion. They did not invent the

**[21:20]** completion. They did not invent the model. They didn't invent their product

**[21:21]** model. They didn't invent their product

**[21:21]** model. They didn't invent their product surface area, right? They just

**[21:23]** surface area, right? They just

**[21:24]** surface area, right? They just outexecuted on every dimension of this.

**[21:26]** outexecuted on every dimension of this.

**[21:26]** outexecuted on every dimension of this. They shipped a great experience faster

**[21:28]** They shipped a great experience faster

**[21:28]** They shipped a great experience faster than their competitors could copy. and

**[21:29]** than their competitors could copy. and

**[21:30]** than their competitors could copy. and they capture the hearts and minds of

**[21:31]** they capture the hearts and minds of

**[21:31]** they capture the hearts and minds of developers at least in this term. Um I

**[21:34]** developers at least in this term. Um I

**[21:34]** developers at least in this term. Um I don't I don't mean this to be cruel but

**[21:35]** don't I don't mean this to be cruel but

**[21:35]** don't I don't mean this to be cruel but I often get asked about like counter

**[21:37]** I often get asked about like counter

**[21:37]** I often get asked about like counter cases and the importance of first mover

**[21:39]** cases and the importance of first mover

**[21:39]** cases and the importance of first mover advantage. Let's be brutally honest. In

**[21:41]** advantage. Let's be brutally honest. In

**[21:42]** advantage. Let's be brutally honest. In contrast, like Jasper had first mover

**[21:44]** contrast, like Jasper had first mover

**[21:44]** contrast, like Jasper had first mover advantage brand. They raised $125

**[21:46]** advantage brand. They raised $125

**[21:46]** advantage brand. They raised $125 million, but its first product was a

**[21:49]** million, but its first product was a

**[21:49]** million, but its first product was a series of prompts and a text box and

**[21:51]** series of prompts and a text box and

**[21:51]** series of prompts and a text box and like very good SEO. And like you have to

**[21:53]** like very good SEO. And like you have to

**[21:53]** like very good SEO. And like you have to keep running like ChatBT, you know,

**[21:55]** keep running like ChatBT, you know,

**[21:55]** keep running like ChatBT, you know, crushed the first iteration pretty

**[21:57]** crushed the first iteration pretty

**[21:57]** crushed the first iteration pretty quickly. And so, uh, I I don't think

**[21:59]** quickly. And so, uh, I I don't think

**[21:59]** quickly. And so, uh, I I don't think this is satisfying advice, but I think


### [22:00 - 23:00]

**[22:00]** this is satisfying advice, but I think

**[22:00]** this is satisfying advice, but I think it is like real from the trenches. Build

**[22:02]** it is like real from the trenches. Build

**[22:02]** it is like real from the trenches. Build something thick and stay ahead. And like

**[22:04]** something thick and stay ahead. And like

**[22:04]** something thick and stay ahead. And like no domains are out of question. Um,

**[22:06]** no domains are out of question. Um,

**[22:06]** no domains are out of question. Um, magical AI experiences, they build

**[22:08]** magical AI experiences, they build

**[22:08]** magical AI experiences, they build customer trust and drive adoption. And a

**[22:11]** customer trust and drive adoption. And a

**[22:11]** customer trust and drive adoption. And a lot of the data we need to improve these

**[22:13]** lot of the data we need to improve these

**[22:14]** lot of the data we need to improve these experiences and the context we need it

**[22:16]** experiences and the context we need it

**[22:16]** experiences and the context we need it is not easily available today. And that

**[22:18]** is not easily available today. And that

**[22:18]** is not easily available today. And that advantage is you know uh open for the

**[22:21]** advantage is you know uh open for the

**[22:22]** advantage is you know uh open for the taking and not for the labs.

**[22:25]** taking and not for the labs.

**[22:25]** taking and not for the labs. So I I guess in conclusion I think the

**[22:27]** So I I guess in conclusion I think the

**[22:27]** So I I guess in conclusion I think the opportunity is early and really massive

**[22:30]** opportunity is early and really massive

**[22:30]** opportunity is early and really massive like I've made a career bet on it. Um I

**[22:32]** like I've made a career bet on it. Um I

**[22:32]** like I've made a career bet on it. Um I I think many of you are. We're in the

**[22:33]** I think many of you are. We're in the

**[22:33]** I think many of you are. We're in the dialup era of AI and we're moving pretty

**[22:36]** dialup era of AI and we're moving pretty

**[22:36]** dialup era of AI and we're moving pretty quickly to to broadband. Um, Instagram

**[22:38]** quickly to to broadband. Um, Instagram

**[22:38]** quickly to to broadband. Um, Instagram came four years after the iPhone. Like I

**[22:40]** came four years after the iPhone. Like I

**[22:40]** came four years after the iPhone. Like I was was there when Greylock made that

**[22:42]** was was there when Greylock made that

**[22:42]** was was there when Greylock made that investment. Um, Uber five years. Uh,

**[22:44]** investment. Um, Uber five years. Uh,

**[22:44]** investment. Um, Uber five years. Uh, Door Dash six, right? So, the truly

**[22:46]** Door Dash six, right? So, the truly

**[22:46]** Door Dash six, right? So, the truly transformative companies. They weren't

**[22:49]** transformative companies. They weren't

**[22:49]** transformative companies. They weren't necessarily the first people to

**[22:50]** necessarily the first people to

**[22:50]** necessarily the first people to recognize the changes or the opportunity

**[22:53]** recognize the changes or the opportunity

**[22:53]** recognize the changes or the opportunity is those who reimagine the experiences.

**[22:55]** is those who reimagine the experiences.

**[22:55]** is those who reimagine the experiences. Um, and the game board keeps getting

**[22:57]** Um, and the game board keeps getting

**[22:57]** Um, and the game board keeps getting shaken up. That's the thing that's

**[22:58]** shaken up. That's the thing that's

**[22:58]** shaken up. That's the thing that's different this time, right? It's like


### [23:00 - 24:00]

**[23:00]** different this time, right? It's like

**[23:00]** different this time, right? It's like getting a new iPhone that's actually

**[23:02]** getting a new iPhone that's actually

**[23:02]** getting a new iPhone that's actually different every 12 months. And um so you

**[23:05]** different every 12 months. And um so you

**[23:05]** different every 12 months. And um so you have like new model release, new

**[23:07]** have like new model release, new

**[23:07]** have like new model release, new capability breakthrough, you know,

**[23:08]** capability breakthrough, you know,

**[23:08]** capability breakthrough, you know, onetenth the cost. And every time the

**[23:11]** onetenth the cost. And every time the

**[23:11]** onetenth the cost. And every time the game board turns, I think there are like

**[23:13]** game board turns, I think there are like

**[23:13]** game board turns, I think there are like there's an opportunity to to win again.

**[23:16]** there's an opportunity to to win again.

**[23:16]** there's an opportunity to to win again. Okay. Um so I I'll give you one last

**[23:18]** Okay. Um so I I'll give you one last

**[23:18]** Okay. Um so I I'll give you one last sentence and be chased off the stage.

**[23:20]** sentence and be chased off the stage.

**[23:20]** sentence and be chased off the stage. This was not my fault. Um here's what I

**[23:22]** This was not my fault. Um here's what I

**[23:22]** This was not my fault. Um here's what I really want you to remember. Uh you as

**[23:24]** really want you to remember. Uh you as

**[23:24]** really want you to remember. Uh you as the engineers got the magic first. Um

**[23:26]** the engineers got the magic first. Um

**[23:26]** the engineers got the magic first. Um the anthropic like economic index said

**[23:28]** the anthropic like economic index said

**[23:28]** the anthropic like economic index said that 40% of use was still coding. That's

**[23:31]** that 40% of use was still coding. That's

**[23:31]** that 40% of use was still coding. That's not like 40% of the economic opportunity

**[23:33]** not like 40% of the economic opportunity

**[23:34]** not like 40% of the economic opportunity in the world, right? And so it is the

**[23:35]** in the world, right? And so it is the

**[23:35]** in the world, right? And so it is the job of everyone in this room and you

**[23:38]** job of everyone in this room and you

**[23:38]** job of everyone in this room and you know globally online to be the

**[23:39]** know globally online to be the

**[23:39]** know globally online to be the translators for the rest of the world.

**[23:41]** translators for the rest of the world.

**[23:41]** translators for the rest of the world. So I encourage you to build something

**[23:42]** So I encourage you to build something

**[23:42]** So I encourage you to build something revolutionary. Thanks.

**[23:45]** revolutionary. Thanks.

**[23:45]** revolutionary. Thanks. [Music]


