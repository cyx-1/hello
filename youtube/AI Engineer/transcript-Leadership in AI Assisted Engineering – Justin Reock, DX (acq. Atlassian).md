# Leadership in AI Assisted Engineering â€“ Justin Reock, DX (acq. Atlassian)

**Video URL:** https://youtu.be/PmZDupFP3UM?si=P9hErZrZKKBRN85e

---

## Full Transcript

### [00:00 - 01:00]

**[00:22]** Thanks for joining me in one of the

**[00:22]** Thanks for joining me in one of the later day sessions. Looks like we we we

**[00:23]** later day sessions. Looks like we we we

**[00:23]** later day sessions. Looks like we we we kept a lot of people here. This is a

**[00:24]** kept a lot of people here. This is a

**[00:24]** kept a lot of people here. This is a nice full room. I'm great to see it.

**[00:26]** nice full room. I'm great to see it.

**[00:26]** nice full room. I'm great to see it. We're going to go through a lot of

**[00:27]** We're going to go through a lot of

**[00:27]** We're going to go through a lot of content in a short amount of time. So,

**[00:29]** content in a short amount of time. So,

**[00:29]** content in a short amount of time. So, I'm going to get right into it. If you

**[00:30]** I'm going to get right into it. If you

**[00:30]** I'm going to get right into it. If you want to get deeper into any of this

**[00:31]** want to get deeper into any of this

**[00:32]** want to get deeper into any of this stuff, we have published this uh AI

**[00:34]** stuff, we have published this uh AI

**[00:34]** stuff, we have published this uh AI strategy playbook for senior executives.

**[00:37]** strategy playbook for senior executives.

**[00:37]** strategy playbook for senior executives. And uh a lot of the content that I'm

**[00:38]** And uh a lot of the content that I'm

**[00:38]** And uh a lot of the content that I'm going to go through, I'm not going to

**[00:39]** going to go through, I'm not going to

**[00:39]** going to go through, I'm not going to have time to get quite as deep, but this

**[00:41]** have time to get quite as deep, but this

**[00:41]** have time to get quite as deep, but this is just a nice PDF copy that you can

**[00:42]** is just a nice PDF copy that you can

**[00:42]** is just a nice PDF copy that you can come and refer to later. If you missed

**[00:44]** come and refer to later. If you missed

**[00:44]** come and refer to later. If you missed this QR code, don't worry. I'll show it

**[00:46]** this QR code, don't worry. I'll show it

**[00:46]** this QR code, don't worry. I'll show it again uh at the end. So, what is the

**[00:48]** again uh at the end. So, what is the

**[00:48]** again uh at the end. So, what is the current impact of Genai?

**[00:51]** current impact of Genai?

**[00:51]** current impact of Genai? Nobody knows, right? We've got Google on

**[00:53]** Nobody knows, right? We've got Google on

**[00:53]** Nobody knows, right? We've got Google on the one hand telling us that everyone's

**[00:55]** the one hand telling us that everyone's

**[00:55]** the one hand telling us that everyone's 10% more productive. That's interesting.

**[00:56]** 10% more productive. That's interesting.

**[00:56]** 10% more productive. That's interesting. Now, they're Google. they were already

**[00:58]** Now, they're Google. they were already

**[00:58]** Now, they're Google. they were already pretty productive to begin with, but we

**[00:59]** pretty productive to begin with, but we

**[00:59]** pretty productive to begin with, but we have this sort of now infamous meter MER


### [01:00 - 02:00]

**[01:03]** have this sort of now infamous meter MER

**[01:03]** have this sort of now infamous meter MER study which has some flaws in the way

**[01:04]** study which has some flaws in the way

**[01:04]** study which has some flaws in the way that study was put together that showed

**[01:06]** that study was put together that showed

**[01:06]** that study was put together that showed actually a 19% decrease in productivity

**[01:09]** actually a 19% decrease in productivity

**[01:09]** actually a 19% decrease in productivity using codec assistance. So there's a lot

**[01:11]** using codec assistance. So there's a lot

**[01:11]** using codec assistance. So there's a lot of volatility, a lot of variability. Uh

**[01:13]** of volatility, a lot of variability. Uh

**[01:13]** of volatility, a lot of variability. Uh what was really interesting about this

**[01:14]** what was really interesting about this

**[01:14]** what was really interesting about this study, even though I I mentioned there

**[01:15]** study, even though I I mentioned there

**[01:16]** study, even though I I mentioned there were some flaws, um but every engineer

**[01:18]** were some flaws, um but every engineer

**[01:18]** were some flaws, um but every engineer that took part in this study felt more

**[01:21]** that took part in this study felt more

**[01:22]** that took part in this study felt more productive, but then the data actually

**[01:23]** productive, but then the data actually

**[01:23]** productive, but then the data actually bore out that they were less productive.

**[01:26]** bore out that they were less productive.

**[01:26]** bore out that they were less productive. kind of interesting, right? We've got

**[01:27]** kind of interesting, right? We've got

**[01:27]** kind of interesting, right? We've got this induced flow uh that makes us feel

**[01:29]** this induced flow uh that makes us feel

**[01:29]** this induced flow uh that makes us feel really good about what we're doing. So,

**[01:31]** really good about what we're doing. So,

**[01:31]** really good about what we're doing. So, we need to address this. Dora has put

**[01:33]** we need to address this. Dora has put

**[01:33]** we need to address this. Dora has put out some really good research on this,

**[01:35]** out some really good research on this,

**[01:35]** out some really good research on this, too. But this is based on industry

**[01:37]** too. But this is based on industry

**[01:37]** too. But this is based on industry averages. This is impact based on what

**[01:39]** averages. This is impact based on what

**[01:39]** averages. This is impact based on what do we look at when we see a large sample

**[01:41]** do we look at when we see a large sample

**[01:41]** do we look at when we see a large sample and an average of how certain factors

**[01:43]** and an average of how certain factors

**[01:43]** and an average of how certain factors are being impacted by in this case 25%

**[01:46]** are being impacted by in this case 25%

**[01:46]** are being impacted by in this case 25% increase in AI adoption. We see these

**[01:49]** increase in AI adoption. We see these

**[01:49]** increase in AI adoption. We see these modest but positive leaning indicators.

**[01:52]** modest but positive leaning indicators.

**[01:52]** modest but positive leaning indicators. 7.5% increase in documentation quality

**[01:56]** 7.5% increase in documentation quality

**[01:56]** 7.5% increase in documentation quality and uh increase in code quality by about

**[01:58]** and uh increase in code quality by about

**[01:58]** and uh increase in code quality by about 3.4%. At least that's not leaning in the


### [02:00 - 03:00]

**[02:00]** 3.4%. At least that's not leaning in the

**[02:00]** 3.4%. At least that's not leaning in the other direction, right? And when we

**[02:02]** other direction, right? And when we

**[02:02]** other direction, right? And when we started digging through some of DX's

**[02:04]** started digging through some of DX's

**[02:04]** started digging through some of DX's data, we have, you know, we're the

**[02:05]** data, we have, you know, we're the

**[02:05]** data, we have, you know, we're the developer productivity measurement

**[02:06]** developer productivity measurement

**[02:06]** developer productivity measurement company. We have lots of aggregate data

**[02:08]** company. We have lots of aggregate data

**[02:08]** company. We have lots of aggregate data that we can look at with this. We found

**[02:10]** that we can look at with this. We found

**[02:10]** that we can look at with this. We found the same thing. When we looked at

**[02:11]** the same thing. When we looked at

**[02:12]** the same thing. When we looked at averages, we see about a 2.6% 6%

**[02:14]** averages, we see about a 2.6% 6%

**[02:14]** averages, we see about a 2.6% 6% increase in overall uh change

**[02:16]** increase in overall uh change

**[02:16]** increase in overall uh change confidence, which is a a percentage of

**[02:19]** confidence, which is a a percentage of

**[02:19]** confidence, which is a a percentage of people who answered positively that they

**[02:21]** people who answered positively that they

**[02:21]** people who answered positively that they feel confident in the changes that

**[02:22]** feel confident in the changes that

**[02:22]** feel confident in the changes that they're putting into production. Uh

**[02:24]** they're putting into production. Uh

**[02:24]** they're putting into production. Uh similar positive leaning average when we

**[02:27]** similar positive leaning average when we

**[02:27]** similar positive leaning average when we looked at code maintainability, another

**[02:28]** looked at code maintainability, another

**[02:28]** looked at code maintainability, another qualitative metric, a1% reduction in

**[02:32]** qualitative metric, a1% reduction in

**[02:32]** qualitative metric, a1% reduction in change failure rate. uh which when you

**[02:34]** change failure rate. uh which when you

**[02:34]** change failure rate. uh which when you think about the industry benchmark being

**[02:36]** think about the industry benchmark being

**[02:36]** think about the industry benchmark being 4% it's not insignificant

**[02:38]** 4% it's not insignificant

**[02:38]** 4% it's not insignificant but this is not the full story because

**[02:40]** but this is not the full story because

**[02:40]** but this is not the full story because this is what we saw when we broke the

**[02:42]** this is what we saw when we broke the

**[02:42]** this is what we saw when we broke the same studies down per company. Every

**[02:45]** same studies down per company. Every

**[02:45]** same studies down per company. Every company here is a every every bar

**[02:47]** company here is a every every bar

**[02:47]** company here is a every every bar represents a company right we have some

**[02:50]** represents a company right we have some

**[02:50]** represents a company right we have some that are seeing 20% increases in change

**[02:52]** that are seeing 20% increases in change

**[02:52]** that are seeing 20% increases in change confidence while others are seeing 20%

**[02:55]** confidence while others are seeing 20%

**[02:55]** confidence while others are seeing 20% decreases. We're seeing extreme

**[02:56]** decreases. We're seeing extreme

**[02:56]** decreases. We're seeing extreme volatility which is why these averages

**[02:59]** volatility which is why these averages

**[02:59]** volatility which is why these averages look so innocuous but they're belying


### [03:00 - 04:00]

**[03:01]** look so innocuous but they're belying

**[03:01]** look so innocuous but they're belying the greater story of variability. See

**[03:04]** the greater story of variability. See

**[03:04]** the greater story of variability. See the same thing with code

**[03:05]** the same thing with code

**[03:05]** the same thing with code maintainability.

**[03:06]** maintainability.

**[03:06]** maintainability. The same thing with change failure rate.

**[03:08]** The same thing with change failure rate.

**[03:08]** The same thing with change failure rate. So this is a 2% increase in change

**[03:11]** So this is a 2% increase in change

**[03:11]** So this is a 2% increase in change failure rate up here at the top. Again

**[03:13]** failure rate up here at the top. Again

**[03:13]** failure rate up here at the top. Again with an industry benchmark of 4%. That

**[03:16]** with an industry benchmark of 4%. That

**[03:16]** with an industry benchmark of 4%. That means shipping as much as 50% more

**[03:18]** means shipping as much as 50% more

**[03:18]** means shipping as much as 50% more defects than we were shipping before.

**[03:20]** defects than we were shipping before.

**[03:20]** defects than we were shipping before. Right? We want to make sure we're on the

**[03:21]** Right? We want to make sure we're on the

**[03:22]** Right? We want to make sure we're on the lower end of this. But how? Like what

**[03:23]** lower end of this. But how? Like what

**[03:23]** lower end of this. But how? Like what should we be doing? Well, we found some

**[03:27]** should we be doing? Well, we found some

**[03:27]** should we be doing? Well, we found some patterns here. We see that some

**[03:28]** patterns here. We see that some

**[03:28]** patterns here. We see that some organizations are seeing positive

**[03:30]** organizations are seeing positive

**[03:30]** organizations are seeing positive impacts to KPIs, but others are

**[03:32]** impacts to KPIs, but others are

**[03:32]** impacts to KPIs, but others are struggling with adoption and even seeing

**[03:33]** struggling with adoption and even seeing

**[03:33]** struggling with adoption and even seeing some of these negative impacts. Top down

**[03:36]** some of these negative impacts. Top down

**[03:36]** some of these negative impacts. Top down mandates are not working, right? Driving

**[03:38]** mandates are not working, right? Driving

**[03:38]** mandates are not working, right? Driving towards, oh, we must have 100% adoption

**[03:40]** towards, oh, we must have 100% adoption

**[03:40]** towards, oh, we must have 100% adoption of AI. Great, I will update my read my

**[03:42]** of AI. Great, I will update my read my

**[03:42]** of AI. Great, I will update my read my file every morning and I will be

**[03:43]** file every morning and I will be

**[03:44]** file every morning and I will be compliant, right? We're not actually

**[03:45]** compliant, right? We're not actually

**[03:45]** compliant, right? We're not actually moving the needle anywhere when we do

**[03:46]** moving the needle anywhere when we do

**[03:46]** moving the needle anywhere when we do that. We also find that lack of

**[03:49]** that. We also find that lack of

**[03:49]** that. We also find that lack of education and enablement uh has a big

**[03:52]** education and enablement uh has a big

**[03:52]** education and enablement uh has a big impact on sort of negatively impacting

**[03:54]** impact on sort of negatively impacting

**[03:54]** impact on sort of negatively impacting this. Some organizations just turn on

**[03:56]** this. Some organizations just turn on

**[03:56]** this. Some organizations just turn on the tech and expect it to just start

**[03:58]** the tech and expect it to just start

**[03:58]** the tech and expect it to just start working and everybody to know the best

**[03:59]** working and everybody to know the best

**[03:59]** working and everybody to know the best ways to use it. Uh and a difficulty


### [04:00 - 05:00]

**[04:02]** ways to use it. Uh and a difficulty

**[04:02]** ways to use it. Uh and a difficulty measuring the impact or even knowing

**[04:03]** measuring the impact or even knowing

**[04:04]** measuring the impact or even knowing what we should be measuring like what

**[04:05]** what we should be measuring like what

**[04:05]** what we should be measuring like what metrics would should we be looking at

**[04:07]** metrics would should we be looking at

**[04:07]** metrics would should we be looking at you know does utilization really tell us

**[04:09]** you know does utilization really tell us

**[04:09]** you know does utilization really tell us much about the full story of Genai

**[04:10]** much about the full story of Genai

**[04:10]** much about the full story of Genai impact. This is another graph from Dora.

**[04:14]** impact. This is another graph from Dora.

**[04:14]** impact. This is another graph from Dora. uh this is a basian uh posterior

**[04:17]** uh this is a basian uh posterior

**[04:17]** uh this is a basian uh posterior distribution which is an interesting way

**[04:18]** distribution which is an interesting way

**[04:18]** distribution which is an interesting way of representing data. Basically you want

**[04:21]** of representing data. Basically you want

**[04:21]** of representing data. Basically you want your mass to be on the yellow side of

**[04:23]** your mass to be on the yellow side of

**[04:23]** your mass to be on the yellow side of this line uh the the uh the right side

**[04:26]** this line uh the the uh the right side

**[04:26]** this line uh the the uh the right side of this line for the audience. Yeah. And

**[04:28]** of this line for the audience. Yeah. And

**[04:28]** of this line for the audience. Yeah. And you want a sharp peak which is telling

**[04:30]** you want a sharp peak which is telling

**[04:30]** you want a sharp peak which is telling you that we're pretty confident that

**[04:31]** you that we're pretty confident that

**[04:31]** you that we're pretty confident that this initiative will have this impact.

**[04:33]** this initiative will have this impact.

**[04:33]** this initiative will have this impact. And if we look at some of the topline

**[04:35]** And if we look at some of the topline

**[04:35]** And if we look at some of the topline initiatives here, these are things like

**[04:36]** initiatives here, these are things like

**[04:36]** initiatives here, these are things like clear AI policies. All right, we want to

**[04:39]** clear AI policies. All right, we want to

**[04:39]** clear AI policies. All right, we want to make sure we have that. We want time to

**[04:41]** make sure we have that. We want time to

**[04:41]** make sure we have that. We want time to learn. Not just giving people materials,

**[04:43]** learn. Not just giving people materials,

**[04:43]** learn. Not just giving people materials, but actually giving them space to

**[04:44]** but actually giving them space to

**[04:44]** but actually giving them space to experiment, right? Um, and so these

**[04:47]** experiment, right? Um, and so these

**[04:47]** experiment, right? Um, and so these types of factors are the ones that seem

**[04:49]** types of factors are the ones that seem

**[04:49]** types of factors are the ones that seem to be moving the needle the most. So,

**[04:51]** to be moving the needle the most. So,

**[04:51]** to be moving the needle the most. So, we're going to go over some quick tips

**[04:53]** we're going to go over some quick tips

**[04:53]** we're going to go over some quick tips on how we can do all of these things.

**[04:54]** on how we can do all of these things.

**[04:54]** on how we can do all of these things. And again, the guide will go deeper into

**[04:56]** And again, the guide will go deeper into

**[04:56]** And again, the guide will go deeper into this. We want to integrate across the

**[04:58]** this. We want to integrate across the

**[04:58]** this. We want to integrate across the SDLC. All right. For most organizations,


### [05:00 - 06:00]

**[05:01]** SDLC. All right. For most organizations,

**[05:01]** SDLC. All right. For most organizations, writing code has never been the

**[05:02]** writing code has never been the

**[05:02]** writing code has never been the bottleneck, right? We can in uh we can

**[05:05]** bottleneck, right? We can in uh we can

**[05:06]** bottleneck, right? We can in uh we can increase productivity a bit by helping

**[05:07]** increase productivity a bit by helping

**[05:07]** increase productivity a bit by helping with code completion, but our our

**[05:09]** with code completion, but our our

**[05:09]** with code completion, but our our biggest bottlenecks are elsewhere within

**[05:11]** biggest bottlenecks are elsewhere within

**[05:11]** biggest bottlenecks are elsewhere within the SDLC. There's a lot more to creating

**[05:13]** the SDLC. There's a lot more to creating

**[05:13]** the SDLC. There's a lot more to creating software than just writing code. We want

**[05:15]** software than just writing code. We want

**[05:15]** software than just writing code. We want to unblock usage. We can't just say,

**[05:16]** to unblock usage. We can't just say,

**[05:16]** to unblock usage. We can't just say, well, we're worried about data

**[05:17]** well, we're worried about data

**[05:18]** well, we're worried about data xfiltration, so we can't try this thing.

**[05:19]** xfiltration, so we can't try this thing.

**[05:19]** xfiltration, so we can't try this thing. Like, no, get creative about it. We've

**[05:21]** Like, no, get creative about it. We've

**[05:21]** Like, no, get creative about it. We've got really good infrastructure out there

**[05:22]** got really good infrastructure out there

**[05:22]** got really good infrastructure out there now like bedrock and fireworks AI that

**[05:25]** now like bedrock and fireworks AI that

**[05:25]** now like bedrock and fireworks AI that can let us run powerful models in safe

**[05:27]** can let us run powerful models in safe

**[05:28]** can let us run powerful models in safe spaces. We have to have open discussions

**[05:31]** spaces. We have to have open discussions

**[05:31]** spaces. We have to have open discussions about these metrics. We need to

**[05:32]** about these metrics. We need to

**[05:32]** about these metrics. We need to evangelize the wins and we need to let

**[05:34]** evangelize the wins and we need to let

**[05:34]** evangelize the wins and we need to let our engineers know why we're gathering

**[05:37]** our engineers know why we're gathering

**[05:37]** our engineers know why we're gathering metrics and data. What is it that we're

**[05:39]** metrics and data. What is it that we're

**[05:39]** metrics and data. What is it that we're trying to improve? We have to reduce the

**[05:41]** trying to improve? We have to reduce the

**[05:41]** trying to improve? We have to reduce the fear of AI, right? We have to make sure

**[05:43]** fear of AI, right? We have to make sure

**[05:43]** fear of AI, right? We have to make sure that people understand that this is not

**[05:45]** that people understand that this is not

**[05:45]** that people understand that this is not a technology that is ready to replace

**[05:47]** a technology that is ready to replace

**[05:47]** a technology that is ready to replace engineers. This is a a technology that's

**[05:50]** engineers. This is a a technology that's

**[05:50]** engineers. This is a a technology that's really good at augmenting engineers and

**[05:52]** really good at augmenting engineers and

**[05:52]** really good at augmenting engineers and increasing the throughput of our

**[05:54]** increasing the throughput of our

**[05:54]** increasing the throughput of our business. We have to establish better

**[05:56]** business. We have to establish better

**[05:56]** business. We have to establish better compliance and trust. And we need to tie

**[05:58]** compliance and trust. And we need to tie

**[05:58]** compliance and trust. And we need to tie this stuff to employee success. These


### [06:00 - 07:00]

**[06:00]** this stuff to employee success. These

**[06:00]** this stuff to employee success. These are new skill sets. AI is not coming for

**[06:02]** are new skill sets. AI is not coming for

**[06:02]** are new skill sets. AI is not coming for your job, but somebody really good at AI

**[06:05]** your job, but somebody really good at AI

**[06:05]** your job, but somebody really good at AI might take your job. And so, as leaders,

**[06:07]** might take your job. And so, as leaders,

**[06:08]** might take your job. And so, as leaders, we have the opportunity to help our

**[06:09]** we have the opportunity to help our

**[06:09]** we have the opportunity to help our employees become more successful with

**[06:11]** employees become more successful with

**[06:11]** employees become more successful with this technology. So, how do we reduce

**[06:13]** this technology. So, how do we reduce

**[06:13]** this technology. So, how do we reduce the fear? Well, first of all, why do we

**[06:15]** the fear? Well, first of all, why do we

**[06:15]** the fear? Well, first of all, why do we need to do this? Well, there's a lot of

**[06:16]** need to do this? Well, there's a lot of

**[06:16]** need to do this? Well, there's a lot of good reasons, but I love to point to

**[06:18]** good reasons, but I love to point to

**[06:18]** good reasons, but I love to point to Google's project Aristotle. This was a

**[06:20]** Google's project Aristotle. This was a

**[06:20]** Google's project Aristotle. This was a 2012 study where Google wanted to figure

**[06:23]** 2012 study where Google wanted to figure

**[06:23]** 2012 study where Google wanted to figure out what are the characteristics of

**[06:25]** out what are the characteristics of

**[06:25]** out what are the characteristics of highly performant teams. uh they thought

**[06:27]** highly performant teams. uh they thought

**[06:27]** highly performant teams. uh they thought that the recipe was just going to be

**[06:28]** that the recipe was just going to be

**[06:28]** that the recipe was just going to be what Google had this combination of high

**[06:30]** what Google had this combination of high

**[06:30]** what Google had this combination of high performers, experienced managers and

**[06:32]** performers, experienced managers and

**[06:32]** performers, experienced managers and basically unlimited resources and they

**[06:34]** basically unlimited resources and they

**[06:34]** basically unlimited resources and they were dead wrong. Overwhelmingly the

**[06:37]** were dead wrong. Overwhelmingly the

**[06:37]** were dead wrong. Overwhelmingly the biggest indicator of productivity was

**[06:39]** biggest indicator of productivity was

**[06:39]** biggest indicator of productivity was psychological safety. Okay. And so that

**[06:41]** psychological safety. Okay. And so that

**[06:41]** psychological safety. Okay. And so that very much applies now. We also have data

**[06:44]** very much applies now. We also have data

**[06:44]** very much applies now. We also have data like this is SweetBench. I'm sure a lot

**[06:45]** like this is SweetBench. I'm sure a lot

**[06:46]** like this is SweetBench. I'm sure a lot of you have seen this and there are some

**[06:47]** of you have seen this and there are some

**[06:47]** of you have seen this and there are some impressive benchmarks that the agents

**[06:50]** impressive benchmarks that the agents

**[06:50]** impressive benchmarks that the agents can do like a third of the things

**[06:52]** can do like a third of the things

**[06:52]** can do like a third of the things they're asked to do without any human

**[06:54]** they're asked to do without any human

**[06:54]** they're asked to do without any human intervention. That means that they're

**[06:56]** intervention. That means that they're

**[06:56]** intervention. That means that they're not able to do twothirds of them. Right?

**[06:58]** not able to do twothirds of them. Right?

**[06:58]** not able to do twothirds of them. Right? Again, we are augmenting. We're not


### [07:00 - 08:00]

**[07:00]** Again, we are augmenting. We're not

**[07:00]** Again, we are augmenting. We're not replacing. We're not ready. We may never

**[07:01]** replacing. We're not ready. We may never

**[07:02]** replacing. We're not ready. We may never be ready. So, we need to be very

**[07:03]** be ready. So, we need to be very

**[07:03]** be ready. So, we need to be very transparent with what we're doing. We

**[07:05]** transparent with what we're doing. We

**[07:05]** transparent with what we're doing. We need to set very clear intents. Why, you

**[07:07]** need to set very clear intents. Why, you

**[07:07]** need to set very clear intents. Why, you know, are we uh using this to to

**[07:10]** know, are we uh using this to to

**[07:10]** know, are we uh using this to to augment, not to replace. We need to be

**[07:12]** augment, not to replace. We need to be

**[07:12]** augment, not to replace. We need to be proactive in the way that we communicate

**[07:14]** proactive in the way that we communicate

**[07:14]** proactive in the way that we communicate that and not just wait for people to get

**[07:16]** that and not just wait for people to get

**[07:16]** that and not just wait for people to get upset and possibly scared. We need to

**[07:18]** upset and possibly scared. We need to

**[07:18]** upset and possibly scared. We need to say, "No, we are here to help you to

**[07:20]** say, "No, we are here to help you to

**[07:20]** say, "No, we are here to help you to give you a better developer experience

**[07:22]** give you a better developer experience

**[07:22]** give you a better developer experience and to increase the throughput of the

**[07:23]** and to increase the throughput of the

**[07:24]** and to increase the throughput of the business." And again we have to have

**[07:25]** business." And again we have to have

**[07:25]** business." And again we have to have these discussions about metrics. Now

**[07:28]** these discussions about metrics. Now

**[07:28]** these discussions about metrics. Now what metrics? What should we be looking

**[07:29]** what metrics? What should we be looking

**[07:29]** what metrics? What should we be looking at? Well DX again developer experience

**[07:32]** at? Well DX again developer experience

**[07:32]** at? Well DX again developer experience and productivity measurement company. Um

**[07:35]** and productivity measurement company. Um

**[07:35]** and productivity measurement company. Um there are two sort of classes of metrics

**[07:37]** there are two sort of classes of metrics

**[07:37]** there are two sort of classes of metrics that we can be looking at really two

**[07:39]** that we can be looking at really two

**[07:39]** that we can be looking at really two levers that matter here and that's speed

**[07:41]** levers that matter here and that's speed

**[07:41]** levers that matter here and that's speed and quality. Right? We want to increase

**[07:43]** and quality. Right? We want to increase

**[07:43]** and quality. Right? We want to increase PR throughput. We want to increase our

**[07:45]** PR throughput. We want to increase our

**[07:45]** PR throughput. We want to increase our velocity but not by just creating a

**[07:47]** velocity but not by just creating a

**[07:47]** velocity but not by just creating a bunch of slop that's going to give us a

**[07:49]** bunch of slop that's going to give us a

**[07:49]** bunch of slop that's going to give us a bunch of tech debt later that we're

**[07:50]** bunch of tech debt later that we're

**[07:50]** bunch of tech debt later that we're going to have to deal with and we just

**[07:51]** going to have to deal with and we just

**[07:51]** going to have to deal with and we just kick the bottleneck down the road if we

**[07:53]** kick the bottleneck down the road if we

**[07:53]** kick the bottleneck down the road if we do that. Right? So we want to be looking

**[07:54]** do that. Right? So we want to be looking

**[07:54]** do that. Right? So we want to be looking at things like change failure rate, our

**[07:57]** at things like change failure rate, our

**[07:57]** at things like change failure rate, our overall perception of quality, change

**[07:58]** overall perception of quality, change

**[07:58]** overall perception of quality, change confidence, maintainability.


### [08:00 - 09:00]

**[08:01]** confidence, maintainability.

**[08:01]** confidence, maintainability. And we have three types of metrics that

**[08:03]** And we have three types of metrics that

**[08:03]** And we have three types of metrics that we can be looking at here. We have our

**[08:05]** we can be looking at here. We have our

**[08:05]** we can be looking at here. We have our telemetry metrics. These are the things

**[08:07]** telemetry metrics. These are the things

**[08:07]** telemetry metrics. These are the things coming out of the API. And they're good

**[08:09]** coming out of the API. And they're good

**[08:09]** coming out of the API. And they're good for some stuff, but they're not always

**[08:11]** for some stuff, but they're not always

**[08:11]** for some stuff, but they're not always accurate, right? We know like accept

**[08:13]** accurate, right? We know like accept

**[08:13]** accurate, right? We know like accept versus suggest was kind of like all the

**[08:16]** versus suggest was kind of like all the

**[08:16]** versus suggest was kind of like all the rage until we realize that engineers

**[08:17]** rage until we realize that engineers

**[08:17]** rage until we realize that engineers need to click accept in the IDE in order

**[08:20]** need to click accept in the IDE in order

**[08:20]** need to click accept in the IDE in order for the API to know about it. even if

**[08:22]** for the API to know about it. even if

**[08:22]** for the API to know about it. even if they do click accept, who's to say they

**[08:23]** they do click accept, who's to say they

**[08:23]** they do click accept, who's to say they didn't just go back and rewrite every

**[08:25]** didn't just go back and rewrite every

**[08:25]** didn't just go back and rewrite every line that was suggested, right? So

**[08:27]** line that was suggested, right? So

**[08:27]** line that was suggested, right? So that's providing us some context, but we

**[08:29]** that's providing us some context, but we

**[08:29]** that's providing us some context, but we also need to do some experience

**[08:30]** also need to do some experience

**[08:30]** also need to do some experience sampling. We need to like for instance

**[08:32]** sampling. We need to like for instance

**[08:32]** sampling. We need to like for instance add a new field to a PR form that says I

**[08:35]** add a new field to a PR form that says I

**[08:35]** add a new field to a PR form that says I used AI to generate this PR or I enjoyed

**[08:38]** used AI to generate this PR or I enjoyed

**[08:38]** used AI to generate this PR or I enjoyed using AI to generate this PR and get

**[08:40]** using AI to generate this PR and get

**[08:40]** using AI to generate this PR and get some data that way. And then

**[08:42]** some data that way. And then

**[08:42]** some data that way. And then self-reported data or survey data. We

**[08:44]** self-reported data or survey data. We

**[08:44]** self-reported data or survey data. We are big on surveys, but let me

**[08:46]** are big on surveys, but let me

**[08:46]** are big on surveys, but let me underscore we're big on effective

**[08:47]** underscore we're big on effective

**[08:47]** underscore we're big on effective surveys. 90% plus participation rates

**[08:51]** surveys. 90% plus participation rates

**[08:51]** surveys. 90% plus participation rates engineered against questions that treat

**[08:53]** engineered against questions that treat

**[08:53]** engineered against questions that treat developer experience as a systems

**[08:56]** developer experience as a systems

**[08:56]** developer experience as a systems problem not a people problem because

**[08:58]** problem not a people problem because

**[08:58]** problem not a people problem because that's what it is W. Edwards Deming 90


### [09:00 - 10:00]

**[09:00]** that's what it is W. Edwards Deming 90

**[09:00]** that's what it is W. Edwards Deming 90 to 95% of the productivity output of an

**[09:03]** to 95% of the productivity output of an

**[09:03]** to 95% of the productivity output of an organization is determined by the system

**[09:05]** organization is determined by the system

**[09:05]** organization is determined by the system and not the worker. Okay, so

**[09:08]** and not the worker. Okay, so

**[09:08]** and not the worker. Okay, so foundational developer experience and

**[09:10]** foundational developer experience and

**[09:10]** foundational developer experience and developer productivity metrics still

**[09:12]** developer productivity metrics still

**[09:12]** developer productivity metrics still matter the most. Right? Our AI metrics

**[09:14]** matter the most. Right? Our AI metrics

**[09:14]** matter the most. Right? Our AI metrics like utilization and things are telling

**[09:16]** like utilization and things are telling

**[09:16]** like utilization and things are telling us what's happening with the tech, but

**[09:18]** us what's happening with the tech, but

**[09:18]** us what's happening with the tech, but these core metrics that we've been able

**[09:20]** these core metrics that we've been able

**[09:20]** these core metrics that we've been able to trust are telling us whether these

**[09:21]** to trust are telling us whether these

**[09:21]** to trust are telling us whether these initiatives are actually working, right?

**[09:23]** initiatives are actually working, right?

**[09:24]** initiatives are actually working, right? Are we actually moving the needle and

**[09:25]** Are we actually moving the needle and

**[09:25]** Are we actually moving the needle and having the outcomes that we want to see?

**[09:28]** having the outcomes that we want to see?

**[09:28]** having the outcomes that we want to see? So top companies are looking at

**[09:29]** So top companies are looking at

**[09:29]** So top companies are looking at different things, right? We are seeing

**[09:31]** different things, right? We are seeing

**[09:31]** different things, right? We are seeing like adoption metrics coming out of

**[09:33]** like adoption metrics coming out of

**[09:33]** like adoption metrics coming out of Microsoft. They've also got this great

**[09:34]** Microsoft. They've also got this great

**[09:34]** Microsoft. They've also got this great metric called a bad developer day. I'm

**[09:37]** metric called a bad developer day. I'm

**[09:37]** metric called a bad developer day. I'm not going to go into it, but there's a

**[09:38]** not going to go into it, but there's a

**[09:38]** not going to go into it, but there's a really good white paper that shows like

**[09:40]** really good white paper that shows like

**[09:40]** really good white paper that shows like all the different telemetry that they

**[09:41]** all the different telemetry that they

**[09:41]** all the different telemetry that they can look at to determine what makes a

**[09:43]** can look at to determine what makes a

**[09:43]** can look at to determine what makes a bad developer day. Dropbox is looking at

**[09:45]** bad developer day. Dropbox is looking at

**[09:45]** bad developer day. Dropbox is looking at similar stuff. Adoption like weekly

**[09:47]** similar stuff. Adoption like weekly

**[09:48]** similar stuff. Adoption like weekly active users, daily active users, that

**[09:49]** active users, daily active users, that

**[09:49]** active users, daily active users, that sort of thing, but also looking at

**[09:51]** sort of thing, but also looking at

**[09:51]** sort of thing, but also looking at quality metrics like change failure

**[09:52]** quality metrics like change failure

**[09:52]** quality metrics like change failure rate. And booking is looking at similar

**[09:54]** rate. And booking is looking at similar

**[09:54]** rate. And booking is looking at similar stuff as well. And so we built a

**[09:56]** stuff as well. And so we built a

**[09:56]** stuff as well. And so we built a framework around this. We were first to

**[09:58]** framework around this. We were first to

**[09:58]** framework around this. We were first to market with what we call our DXAI


### [10:00 - 11:00]

**[10:00]** market with what we call our DXAI

**[10:00]** market with what we call our DXAI measurement framework. And this is very

**[10:02]** measurement framework. And this is very

**[10:02]** measurement framework. And this is very much inspired by things like Dora space

**[10:04]** much inspired by things like Dora space

**[10:04]** much inspired by things like Dora space framework, DevX just like our core four

**[10:06]** framework, DevX just like our core four

**[10:06]** framework, DevX just like our core four metric set which you can ask me about

**[10:08]** metric set which you can ask me about

**[10:08]** metric set which you can ask me about later. Uh and we take these metrics and

**[10:11]** later. Uh and we take these metrics and

**[10:11]** later. Uh and we take these metrics and we uh normalize them into these three

**[10:14]** we uh normalize them into these three

**[10:14]** we uh normalize them into these three dimensions of utilization, impact and

**[10:17]** dimensions of utilization, impact and

**[10:17]** dimensions of utilization, impact and cost. And you can kind of think about

**[10:19]** cost. And you can kind of think about

**[10:19]** cost. And you can kind of think about this as a maturity curve too. A lot of

**[10:21]** this as a maturity curve too. A lot of

**[10:21]** this as a maturity curve too. A lot of people start just figuring out okay

**[10:23]** people start just figuring out okay

**[10:23]** people start just figuring out okay what's happening? who's using the tech,

**[10:25]** what's happening? who's using the tech,

**[10:25]** what's happening? who's using the tech, what's the percentage of pull requests

**[10:27]** what's the percentage of pull requests

**[10:27]** what's the percentage of pull requests that we're getting that are AI assisted

**[10:28]** that we're getting that are AI assisted

**[10:28]** that we're getting that are AI assisted maybe through experience sampling? How

**[10:30]** maybe through experience sampling? How

**[10:30]** maybe through experience sampling? How many tasks are being assigned to agents?

**[10:32]** many tasks are being assigned to agents?

**[10:32]** many tasks are being assigned to agents? But then we can mature that perspective

**[10:34]** But then we can mature that perspective

**[10:34]** But then we can mature that perspective a little bit and we can correlate that

**[10:36]** a little bit and we can correlate that

**[10:36]** a little bit and we can correlate that utilization to impact. What is this

**[10:38]** utilization to impact. What is this

**[10:38]** utilization to impact. What is this actually doing to velocity? What is this

**[10:41]** actually doing to velocity? What is this

**[10:41]** actually doing to velocity? What is this actually doing to quality? And this is

**[10:43]** actually doing to quality? And this is

**[10:43]** actually doing to quality? And this is when we start getting more mature in our

**[10:44]** when we start getting more mature in our

**[10:44]** when we start getting more mature in our picture of our impact. And then finally,

**[10:47]** picture of our impact. And then finally,

**[10:47]** picture of our impact. And then finally, cost. Although I like to joke that we're

**[10:48]** cost. Although I like to joke that we're

**[10:48]** cost. Although I like to joke that we're 15 years past the last hype cycle, which

**[10:50]** 15 years past the last hype cycle, which

**[10:50]** 15 years past the last hype cycle, which was cloud, and we still have new

**[10:52]** was cloud, and we still have new

**[10:52]** was cloud, and we still have new companies spinning up that are teaching

**[10:53]** companies spinning up that are teaching

**[10:53]** companies spinning up that are teaching us how to understand and optimize our

**[10:55]** us how to understand and optimize our

**[10:55]** us how to understand and optimize our cloud costs. So, we will see if we get

**[10:57]** cloud costs. So, we will see if we get

**[10:57]** cloud costs. So, we will see if we get there. Although, I also hear horror

**[10:58]** there. Although, I also hear horror

**[10:58]** there. Although, I also hear horror stories about people burning through

**[10:59]** stories about people burning through

**[10:59]** stories about people burning through 2,000 tokens at $2,000 worth of tokens a


### [11:00 - 12:00]

**[11:02]** 2,000 tokens at $2,000 worth of tokens a

**[11:02]** 2,000 tokens at $2,000 worth of tokens a day. So, we probably do need to hit that

**[11:04]** day. So, we probably do need to hit that

**[11:04]** day. So, we probably do need to hit that as well. What about compliance and

**[11:06]** as well. What about compliance and

**[11:06]** as well. What about compliance and trust? What can we do to ensure that the

**[11:08]** trust? What can we do to ensure that the

**[11:08]** trust? What can we do to ensure that the output uh that that's being generated is

**[11:10]** output uh that that's being generated is

**[11:10]** output uh that that's being generated is something that can be trusted by our

**[11:12]** something that can be trusted by our

**[11:12]** something that can be trusted by our engineers? We have a lot of levers to

**[11:14]** engineers? We have a lot of levers to

**[11:14]** engineers? We have a lot of levers to pull here, but one of the ones that I'd

**[11:16]** pull here, but one of the ones that I'd

**[11:16]** pull here, but one of the ones that I'd like to talk about is setting up a

**[11:18]** like to talk about is setting up a

**[11:18]** like to talk about is setting up a feedback loop for our system prompts. So

**[11:21]** feedback loop for our system prompts. So

**[11:21]** feedback loop for our system prompts. So these could be called system prompts,

**[11:23]** these could be called system prompts,

**[11:23]** these could be called system prompts, cursor rules, agent markdown. Pretty

**[11:25]** cursor rules, agent markdown. Pretty

**[11:25]** cursor rules, agent markdown. Pretty much all of the mainstream solutions

**[11:27]** much all of the mainstream solutions

**[11:27]** much all of the mainstream solutions have something like this where you can

**[11:28]** have something like this where you can

**[11:28]** have something like this where you can go and provide a set of rules uh to

**[11:31]** go and provide a set of rules uh to

**[11:31]** go and provide a set of rules uh to control how these models behave. Uh and

**[11:34]** control how these models behave. Uh and

**[11:34]** control how these models behave. Uh and I won't get too much into the technical

**[11:36]** I won't get too much into the technical

**[11:36]** I won't get too much into the technical details here. We have an example where

**[11:37]** details here. We have an example where

**[11:37]** details here. We have an example where like the uh models have been providing

**[11:39]** like the uh models have been providing

**[11:39]** like the uh models have been providing outdated Spring Boot uh stuff. We want

**[11:42]** outdated Spring Boot uh stuff. We want

**[11:42]** outdated Spring Boot uh stuff. We want Spring Boot 3. It's It's been sending us

**[11:44]** Spring Boot 3. It's It's been sending us

**[11:44]** Spring Boot 3. It's It's been sending us Spring Boot 2 stuff. The big takeaway

**[11:46]** Spring Boot 2 stuff. The big takeaway

**[11:46]** Spring Boot 2 stuff. The big takeaway here is to have the feedback loop. Have

**[11:48]** here is to have the feedback loop. Have

**[11:48]** here is to have the feedback loop. Have a gatekeeper, right? Have somebody or a

**[11:50]** a gatekeeper, right? Have somebody or a

**[11:50]** a gatekeeper, right? Have somebody or a group in the organization that can

**[11:52]** group in the organization that can

**[11:52]** group in the organization that can receive this feedback that understand

**[11:54]** receive this feedback that understand

**[11:54]** receive this feedback that understand how to maintain and continuously improve

**[11:56]** how to maintain and continuously improve

**[11:56]** how to maintain and continuously improve these system prompts, right? And that

**[11:58]** these system prompts, right? And that

**[11:58]** these system prompts, right? And that way we're always maintaining the way


### [12:00 - 13:00]

**[12:00]** way we're always maintaining the way

**[12:00]** way we're always maintaining the way that these assistants or models or

**[12:02]** that these assistants or models or

**[12:02]** that these assistants or models or agents affect the whole business. It

**[12:04]** agents affect the whole business. It

**[12:04]** agents affect the whole business. It also pays to understand the way that uh

**[12:07]** also pays to understand the way that uh

**[12:07]** also pays to understand the way that uh temperature works, especially when we're

**[12:09]** temperature works, especially when we're

**[12:09]** temperature works, especially when we're building agents, right? we do have some

**[12:11]** building agents, right? we do have some

**[12:11]** building agents, right? we do have some control over the determinism and

**[12:13]** control over the determinism and

**[12:13]** control over the determinism and nondeterminism of these models. Uh again

**[12:15]** nondeterminism of these models. Uh again

**[12:15]** nondeterminism of these models. Uh again like when a model is predicting a next

**[12:17]** like when a model is predicting a next

**[12:17]** like when a model is predicting a next token, it doesn't just have like one

**[12:18]** token, it doesn't just have like one

**[12:18]** token, it doesn't just have like one token. It has a matrix of tokens and

**[12:21]** token. It has a matrix of tokens and

**[12:21]** token. It has a matrix of tokens and those are associated with a certain

**[12:22]** those are associated with a certain

**[12:22]** those are associated with a certain probability of that being like the right

**[12:24]** probability of that being like the right

**[12:24]** probability of that being like the right token. And so we have this setting

**[12:26]** token. And so we have this setting

**[12:26]** token. And so we have this setting called temperature which is heat which

**[12:28]** called temperature which is heat which

**[12:28]** called temperature which is heat which is entropy which is randomness that can

**[12:30]** is entropy which is randomness that can

**[12:30]** is entropy which is randomness that can control the amount of randomness

**[12:31]** control the amount of randomness

**[12:31]** control the amount of randomness involved in actually picking that token.

**[12:33]** involved in actually picking that token.

**[12:33]** involved in actually picking that token. This is sometimes called increasing the

**[12:34]** This is sometimes called increasing the

**[12:34]** This is sometimes called increasing the creativity of the model. And it's a

**[12:37]** creativity of the model. And it's a

**[12:37]** creativity of the model. And it's a number between 0 and one. For those

**[12:38]** number between 0 and one. For those

**[12:38]** number between 0 and one. For those reasons I just mentioned, don't use zero

**[12:40]** reasons I just mentioned, don't use zero

**[12:40]** reasons I just mentioned, don't use zero or don't use one. Weird things will

**[12:41]** or don't use one. Weird things will

**[12:42]** or don't use one. Weird things will happen. But you want some decimal in

**[12:44]** happen. But you want some decimal in

**[12:44]** happen. But you want some decimal in between zero and one. When we have a

**[12:46]** between zero and one. When we have a

**[12:46]** between zero and one. When we have a lower temperature, like we're seeing

**[12:47]** lower temperature, like we're seeing

**[12:47]** lower temperature, like we're seeing here, 0.001,

**[12:49]** here, 0.001,

**[12:49]** here, 0.001, we give it the same task twice, and it

**[12:52]** we give it the same task twice, and it

**[12:52]** we give it the same task twice, and it gives us the exact same output character

**[12:54]** gives us the exact same output character

**[12:54]** gives us the exact same output character for character. When we set that

**[12:55]** for character. When we set that

**[12:55]** for character. When we set that temperature higher, this is an example

**[12:57]** temperature higher, this is an example

**[12:57]** temperature higher, this is an example of 0.9. I'm asking the agent to create a


### [13:00 - 14:00]

**[13:00]** of 0.9. I'm asking the agent to create a

**[13:00]** of 0.9. I'm asking the agent to create a gradient for me. Uh, simple task. It's

**[13:03]** gradient for me. Uh, simple task. It's

**[13:03]** gradient for me. Uh, simple task. It's giving me two relatively valid

**[13:05]** giving me two relatively valid

**[13:05]** giving me two relatively valid solutions. I did ask it for a JavaScript

**[13:07]** solutions. I did ask it for a JavaScript

**[13:07]** solutions. I did ask it for a JavaScript method and this is the only one that's

**[13:09]** method and this is the only one that's

**[13:09]** method and this is the only one that's giving me a JavaScript method. But the

**[13:11]** giving me a JavaScript method. But the

**[13:11]** giving me a JavaScript method. But the point is they are wildly different

**[13:12]** point is they are wildly different

**[13:12]** point is they are wildly different approaches to the same problem when I've

**[13:14]** approaches to the same problem when I've

**[13:14]** approaches to the same problem when I've increased the creativity of that model.

**[13:16]** increased the creativity of that model.

**[13:16]** increased the creativity of that model. So we need to think about like use case

**[13:18]** So we need to think about like use case

**[13:18]** So we need to think about like use case wise where should we have more

**[13:20]** wise where should we have more

**[13:20]** wise where should we have more creativity and where should we have more

**[13:22]** creativity and where should we have more

**[13:22]** creativity and where should we have more determinism and temperature is another

**[13:24]** determinism and temperature is another

**[13:24]** determinism and temperature is another setting that we have that can help

**[13:25]** setting that we have that can help

**[13:25]** setting that we have that can help control this. You can experiment with

**[13:27]** control this. You can experiment with

**[13:27]** control this. You can experiment with all this using like docker model runner

**[13:29]** all this using like docker model runner

**[13:29]** all this using like docker model runner lama lm studio that sort of thing. How

**[13:32]** lama lm studio that sort of thing. How

**[13:32]** lama lm studio that sort of thing. How can we tie this to better employee

**[13:33]** can we tie this to better employee

**[13:33]** can we tie this to better employee success? We had to provide both

**[13:35]** success? We had to provide both

**[13:35]** success? We had to provide both education and adequate time to learn. So

**[13:38]** education and adequate time to learn. So

**[13:38]** education and adequate time to learn. So we put together a study where we sampled

**[13:41]** we put together a study where we sampled

**[13:41]** we put together a study where we sampled a bunch of uh developers that were

**[13:43]** a bunch of uh developers that were

**[13:43]** a bunch of uh developers that were saving at least an hour a day uh uh

**[13:46]** saving at least an hour a day uh uh

**[13:46]** saving at least an hour a day uh uh excuse me an hour a week and we asked

**[13:47]** excuse me an hour a week and we asked

**[13:47]** excuse me an hour a week and we asked them to stack rank their top five most

**[13:50]** them to stack rank their top five most

**[13:50]** them to stack rank their top five most valuable use cases. And we built a guide

**[13:52]** valuable use cases. And we built a guide

**[13:52]** valuable use cases. And we built a guide around that. a guide that effectively

**[13:54]** around that. a guide that effectively

**[13:54]** around that. a guide that effectively goes through code examples, prompting

**[13:56]** goes through code examples, prompting

**[13:56]** goes through code examples, prompting examples uh of what we determined using

**[13:59]** examples uh of what we determined using

**[13:59]** examples uh of what we determined using the sort of data approach where we


### [14:00 - 15:00]

**[14:02]** the sort of data approach where we

**[14:02]** the sort of data approach where we should get more reflexive about our best

**[14:03]** should get more reflexive about our best

**[14:03]** should get more reflexive about our best practice and about uh the use cases that

**[14:06]** practice and about uh the use cases that

**[14:06]** practice and about uh the use cases that we're becoming reflexive in in our use

**[14:08]** we're becoming reflexive in in our use

**[14:08]** we're becoming reflexive in in our use of AI. And so that's what this guide was

**[14:10]** of AI. And so that's what this guide was

**[14:10]** of AI. And so that's what this guide was about. And uh we've had this become

**[14:12]** about. And uh we've had this become

**[14:12]** about. And uh we've had this become required reading in certain engineering

**[14:13]** required reading in certain engineering

**[14:13]** required reading in certain engineering groups and uh proud of that. And this is

**[14:15]** groups and uh proud of that. And this is

**[14:15]** groups and uh proud of that. And this is another way that we can help educate.

**[14:17]** another way that we can help educate.

**[14:17]** another way that we can help educate. But we need to give time. Uh we don't

**[14:18]** But we need to give time. Uh we don't

**[14:18]** But we need to give time. Uh we don't have time to go through all of this. I

**[14:20]** have time to go through all of this. I

**[14:20]** have time to go through all of this. I do think it's interesting that the

**[14:22]** do think it's interesting that the

**[14:22]** do think it's interesting that the number one use case for this was stack

**[14:23]** number one use case for this was stack

**[14:23]** number one use case for this was stack trace analysis, right? So, not a

**[14:25]** trace analysis, right? So, not a

**[14:25]** trace analysis, right? So, not a generative use case, actually more of an

**[14:27]** generative use case, actually more of an

**[14:27]** generative use case, actually more of an interpretive use case. And we see some

**[14:29]** interpretive use case. And we see some

**[14:29]** interpretive use case. And we see some other ones here that are not too

**[14:30]** other ones here that are not too

**[14:30]** other ones here that are not too surprising. And there's examples of each

**[14:32]** surprising. And there's examples of each

**[14:32]** surprising. And there's examples of each of these. What about unblocking usage?

**[14:35]** of these. What about unblocking usage?

**[14:35]** of these. What about unblocking usage? How can we make sure that we can

**[14:36]** How can we make sure that we can

**[14:36]** How can we make sure that we can creatively ensure that engineers can

**[14:38]** creatively ensure that engineers can

**[14:38]** creatively ensure that engineers can take the most advantage of this? Well,

**[14:40]** take the most advantage of this? Well,

**[14:40]** take the most advantage of this? Well, leverage self-hosted and private models.

**[14:42]** leverage self-hosted and private models.

**[14:42]** leverage self-hosted and private models. That's getting easier and easier to do.

**[14:44]** That's getting easier and easier to do.

**[14:44]** That's getting easier and easier to do. Partner with compliance on day one,

**[14:46]** Partner with compliance on day one,

**[14:46]** Partner with compliance on day one, right? Make sure that what you're doing

**[14:48]** right? Make sure that what you're doing

**[14:48]** right? Make sure that what you're doing is in line with your organization's

**[14:50]** is in line with your organization's

**[14:50]** is in line with your organization's compliance. You may find that you're

**[14:51]** compliance. You may find that you're

**[14:52]** compliance. You may find that you're making a lot of assumptions about things

**[14:53]** making a lot of assumptions about things

**[14:53]** making a lot of assumptions about things that you don't think you can do that you

**[14:55]** that you don't think you can do that you

**[14:55]** that you don't think you can do that you can actually do, right? And then think

**[14:57]** can actually do, right? And then think

**[14:57]** can actually do, right? And then think creatively around various barriers.

**[14:59]** creatively around various barriers.

**[14:59]** creatively around various barriers. Finally, how can we integrate across the


### [15:00 - 16:00]

**[15:01]** Finally, how can we integrate across the

**[15:02]** Finally, how can we integrate across the SDLC? What should we think about doing

**[15:04]** SDLC? What should we think about doing

**[15:04]** SDLC? What should we think about doing there? You know, and I'm a big Ellie

**[15:06]** there? You know, and I'm a big Ellie

**[15:06]** there? You know, and I'm a big Ellie Gold theory of constraints fan. Probably

**[15:08]** Gold theory of constraints fan. Probably

**[15:08]** Gold theory of constraints fan. Probably have some others in the audience. An

**[15:09]** have some others in the audience. An

**[15:09]** have some others in the audience. An hour saved on something that isn't the

**[15:11]** hour saved on something that isn't the

**[15:11]** hour saved on something that isn't the bottleneck is worthless. And when we

**[15:14]** bottleneck is worthless. And when we

**[15:14]** bottleneck is worthless. And when we look at data across in this case almost

**[15:16]** look at data across in this case almost

**[15:16]** look at data across in this case almost 140,000 engineers, we find that there

**[15:18]** 140,000 engineers, we find that there

**[15:18]** 140,000 engineers, we find that there are definitely good like annualized time

**[15:21]** are definitely good like annualized time

**[15:21]** are definitely good like annualized time savings with AI that are being eclipsed

**[15:24]** savings with AI that are being eclipsed

**[15:24]** savings with AI that are being eclipsed by sources of context switching and

**[15:26]** by sources of context switching and

**[15:26]** by sources of context switching and interruption, meeting heavy days, these

**[15:29]** interruption, meeting heavy days, these

**[15:29]** interruption, meeting heavy days, these other things that it's like, yeah, we

**[15:30]** other things that it's like, yeah, we

**[15:30]** other things that it's like, yeah, we can save time here, but we're losing so

**[15:32]** can save time here, but we're losing so

**[15:32]** can save time here, but we're losing so much more time over there. So find the

**[15:34]** much more time over there. So find the

**[15:34]** much more time over there. So find the bottleneck, fix the bottleneck, right?

**[15:36]** bottleneck, fix the bottleneck, right?

**[15:36]** bottleneck, fix the bottleneck, right? Morgan Stanley's been very public about

**[15:38]** Morgan Stanley's been very public about

**[15:38]** Morgan Stanley's been very public about the uh building this thing called Dev

**[15:40]** the uh building this thing called Dev

**[15:40]** the uh building this thing called Dev Gen AI that looks at a bunch of legacy

**[15:43]** Gen AI that looks at a bunch of legacy

**[15:43]** Gen AI that looks at a bunch of legacy code, Cobalt, mainframe natural. I hate

**[15:45]** code, Cobalt, mainframe natural. I hate

**[15:45]** code, Cobalt, mainframe natural. I hate to admit Pearl because I'm an old school

**[15:47]** to admit Pearl because I'm an old school

**[15:47]** to admit Pearl because I'm an old school Pearl developer. Uh but apparently

**[15:49]** Pearl developer. Uh but apparently

**[15:49]** Pearl developer. Uh but apparently that's legacy now, too. And basically

**[15:51]** that's legacy now, too. And basically

**[15:51]** that's legacy now, too. And basically creating specs uh for developers that

**[15:54]** creating specs uh for developers that

**[15:54]** creating specs uh for developers that can just be handed to developers to

**[15:55]** can just be handed to developers to

**[15:55]** can just be handed to developers to start modernizing the code without

**[15:57]** start modernizing the code without

**[15:57]** start modernizing the code without having to do all that reverse

**[15:59]** having to do all that reverse

**[15:59]** having to do all that reverse engineering, right? And they're saving


### [16:00 - 17:00]

**[16:00]** engineering, right? And they're saving

**[16:00]** engineering, right? And they're saving about 300,000 hours annually right now

**[16:02]** about 300,000 hours annually right now

**[16:02]** about 300,000 hours annually right now doing this. There's a Wall Street

**[16:03]** doing this. There's a Wall Street

**[16:03]** doing this. There's a Wall Street Journal journal article about this,

**[16:05]** Journal journal article about this,

**[16:05]** Journal journal article about this, Business Insider article about it. Uh

**[16:07]** Business Insider article about it. Uh

**[16:07]** Business Insider article about it. Uh they're very public about that. Zapier,

**[16:10]** they're very public about that. Zapier,

**[16:10]** they're very public about that. Zapier, Zapier should be the example for

**[16:12]** Zapier should be the example for

**[16:12]** Zapier should be the example for everyone. They have a whole series of

**[16:14]** everyone. They have a whole series of

**[16:14]** everyone. They have a whole series of bots and agents that are doing things

**[16:16]** bots and agents that are doing things

**[16:16]** bots and agents that are doing things like assisting with onboarding. They can

**[16:18]** like assisting with onboarding. They can

**[16:18]** like assisting with onboarding. They can now make engineers effective in 2 weeks.

**[16:21]** now make engineers effective in 2 weeks.

**[16:21]** now make engineers effective in 2 weeks. Industry benchmark on the good side is

**[16:23]** Industry benchmark on the good side is

**[16:23]** Industry benchmark on the good side is like a month. On the medium side is like

**[16:26]** like a month. On the medium side is like

**[16:26]** like a month. On the medium side is like 90 days. And uh because they're able to

**[16:30]** 90 days. And uh because they're able to

**[16:30]** 90 days. And uh because they're able to increase the effectiveness of the

**[16:31]** increase the effectiveness of the

**[16:31]** increase the effectiveness of the engineers that they're h that they've

**[16:33]** engineers that they're h that they've

**[16:33]** engineers that they're h that they've bringing into the organization, they

**[16:34]** bringing into the organization, they

**[16:34]** bringing into the organization, they realized that they should be hiring

**[16:36]** realized that they should be hiring

**[16:36]** realized that they should be hiring more, right? As opposed to trying to

**[16:39]** more, right? As opposed to trying to

**[16:39]** more, right? As opposed to trying to maintain status quo by like cutting

**[16:41]** maintain status quo by like cutting

**[16:41]** maintain status quo by like cutting headcount and trying to make individual

**[16:43]** headcount and trying to make individual

**[16:43]** headcount and trying to make individual engineers more productive. They said,

**[16:44]** engineers more productive. They said,

**[16:44]** engineers more productive. They said, "No, we could get more value out of a

**[16:46]** "No, we could get more value out of a

**[16:46]** "No, we could get more value out of a single engineer. We should be hiring

**[16:48]** single engineer. We should be hiring

**[16:48]** single engineer. We should be hiring faster than ever." And they are. And

**[16:51]** faster than ever." And they are. And

**[16:51]** faster than ever." And they are. And it's really increasing their competitive

**[16:52]** it's really increasing their competitive

**[16:52]** it's really increasing their competitive edge. I think that's the right attitude.

**[16:55]** edge. I think that's the right attitude.

**[16:55]** edge. I think that's the right attitude. Spotify has been helping out their SRRES

**[16:57]** Spotify has been helping out their SRRES

**[16:57]** Spotify has been helping out their SRRES by pulling together context when

**[16:59]** by pulling together context when

**[16:59]** by pulling together context when incidents uh are detected and then


### [17:00 - 18:00]

**[17:02]** incidents uh are detected and then

**[17:02]** incidents uh are detected and then taking things like run but steps and and

**[17:04]** taking things like run but steps and and

**[17:04]** taking things like run but steps and and other areas of context and documentation

**[17:06]** other areas of context and documentation

**[17:06]** other areas of context and documentation and pushing them directly into S sur

**[17:08]** and pushing them directly into S sur

**[17:08]** and pushing them directly into S sur channels so that those critical minutes

**[17:10]** channels so that those critical minutes

**[17:10]** channels so that those critical minutes of trying to get to the bottom of what's

**[17:12]** of trying to get to the bottom of what's

**[17:12]** of trying to get to the bottom of what's actually happening and what we should do

**[17:14]** actually happening and what we should do

**[17:14]** actually happening and what we should do do to resolve the incident uh they just

**[17:17]** do to resolve the incident uh they just

**[17:17]** do to resolve the incident uh they just eliminated that time right it's

**[17:18]** eliminated that time right it's

**[17:18]** eliminated that time right it's significantly increased their MTTR so

**[17:20]** significantly increased their MTTR so

**[17:20]** significantly increased their MTTR so let's get creative about areas in the

**[17:21]** let's get creative about areas in the

**[17:22]** let's get creative about areas in the STLC that are our actual bottlenecks

**[17:25]** STLC that are our actual bottlenecks

**[17:25]** STLC that are our actual bottlenecks All right, next steps. Uh, distribute

**[17:27]** All right, next steps. Uh, distribute

**[17:27]** All right, next steps. Uh, distribute this guide as a reference for

**[17:29]** this guide as a reference for

**[17:29]** this guide as a reference for integrating AI into the development

**[17:31]** integrating AI into the development

**[17:31]** integrating AI into the development workflows that you have. Uh, determine a

**[17:33]** workflows that you have. Uh, determine a

**[17:33]** workflows that you have. Uh, determine a method for measuring and evaluating

**[17:35]** method for measuring and evaluating

**[17:35]** method for measuring and evaluating Genai impact. It's really important to

**[17:37]** Genai impact. It's really important to

**[17:37]** Genai impact. It's really important to make sure that we're not on the bad

**[17:39]** make sure that we're not on the bad

**[17:39]** make sure that we're not on the bad sides of those graphs that I showed you

**[17:41]** sides of those graphs that I showed you

**[17:41]** sides of those graphs that I showed you earlier and then track and measure AI

**[17:43]** earlier and then track and measure AI

**[17:43]** earlier and then track and measure AI adoption and and see how that correlates

**[17:45]** adoption and and see how that correlates

**[17:45]** adoption and and see how that correlates to overall impact metrics and iterate on

**[17:48]** to overall impact metrics and iterate on

**[17:48]** to overall impact metrics and iterate on best practices and use cases. And here's

**[17:50]** best practices and use cases. And here's

**[17:50]** best practices and use cases. And here's a guide again. Thank you so much.

**[17:53]** a guide again. Thank you so much.

**[17:53]** a guide again. Thank you so much. [applause]

**[17:54]** [applause]

**[17:54]** [applause] [music]


