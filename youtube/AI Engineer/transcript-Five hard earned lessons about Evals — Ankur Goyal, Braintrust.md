# Five hard earned lessons about Evals — Ankur Goyal, Braintrust

**Video URL:** https://www.youtube.com/watch?v=a4BV0gGmXgA

---

## Full Transcript

### [00:00 - 01:00]

**[00:16]** Uh let's talk about some of the

**[00:16]** Uh let's talk about some of the interesting things we've learned uh over

**[00:17]** interesting things we've learned uh over

**[00:18]** interesting things we've learned uh over time.

**[00:19]** time.

**[00:19]** time. Um so the first thing is I think it's

**[00:21]** Um so the first thing is I think it's

**[00:21]** Um so the first thing is I think it's super important for you to uh understand

**[00:24]** super important for you to uh understand

**[00:24]** super important for you to uh understand and define um whether evals are actually

**[00:27]** and define um whether evals are actually

**[00:27]** and define um whether evals are actually providing value uh for your organization

**[00:29]** providing value uh for your organization

**[00:29]** providing value uh for your organization or not. Um and I tried to come up with

**[00:31]** or not. Um and I tried to come up with

**[00:31]** or not. Um and I tried to come up with three signs that you should look for um

**[00:34]** three signs that you should look for um

**[00:34]** three signs that you should look for um that that are good. Uh so the first is

**[00:37]** that that are good. Uh so the first is

**[00:37]** that that are good. Uh so the first is um if a new model comes out uh you

**[00:40]** um if a new model comes out uh you

**[00:40]** um if a new model comes out uh you should be prepared um uh via your evals

**[00:43]** should be prepared um uh via your evals

**[00:43]** should be prepared um uh via your evals to be able to launch an update to your

**[00:45]** to be able to launch an update to your

**[00:45]** to be able to launch an update to your product within 24 hours that

**[00:47]** product within 24 hours that

**[00:47]** product within 24 hours that incorporates the new model. Uh Sarah

**[00:49]** incorporates the new model. Uh Sarah

**[00:49]** incorporates the new model. Uh Sarah from Notion um she talked yesterday she

**[00:52]** from Notion um she talked yesterday she

**[00:52]** from Notion um she talked yesterday she talked about this um specifically but um

**[00:55]** talked about this um specifically but um

**[00:55]** talked about this um specifically but um for the past several model releases

**[00:57]** for the past several model releases

**[00:57]** for the past several model releases every time something comes out Notion's

**[00:59]** every time something comes out Notion's

**[00:59]** every time something comes out Notion's able to incorporate um the new model


### [01:00 - 02:00]

**[01:01]** able to incorporate um the new model

**[01:01]** able to incorporate um the new model within 24 hours. And I think that's a

**[01:03]** within 24 hours. And I think that's a

**[01:03]** within 24 hours. And I think that's a really good sign of success. If you

**[01:04]** really good sign of success. If you

**[01:04]** really good sign of success. If you can't do that, um, then it means that,

**[01:06]** can't do that, um, then it means that,

**[01:06]** can't do that, um, then it means that, uh, you have some work to do on your

**[01:08]** uh, you have some work to do on your

**[01:08]** uh, you have some work to do on your emails.

**[01:10]** emails.

**[01:10]** emails. Um, another sign of success is if a user

**[01:13]** Um, another sign of success is if a user

**[01:13]** Um, another sign of success is if a user complains about something, do you have a

**[01:15]** complains about something, do you have a

**[01:15]** complains about something, do you have a very clear and straightforward path to

**[01:18]** very clear and straightforward path to

**[01:18]** very clear and straightforward path to take their complaint and add it into

**[01:19]** take their complaint and add it into

**[01:20]** take their complaint and add it into your evals? Um, if you do, then you have

**[01:22]** your evals? Um, if you do, then you have

**[01:22]** your evals? Um, if you do, then you have a shot at actually um, incorporating

**[01:25]** a shot at actually um, incorporating

**[01:25]** a shot at actually um, incorporating user feedback, pulling it into your

**[01:27]** user feedback, pulling it into your

**[01:27]** user feedback, pulling it into your emails, and ultimately doing it better.

**[01:29]** emails, and ultimately doing it better.

**[01:29]** emails, and ultimately doing it better. If you don't, then you're going to lose

**[01:30]** If you don't, then you're going to lose

**[01:30]** If you don't, then you're going to lose a lot of valuable information into the

**[01:31]** a lot of valuable information into the

**[01:32]** a lot of valuable information into the ether. Uh so again I think this is a

**[01:33]** ether. Uh so again I think this is a

**[01:33]** ether. Uh so again I think this is a really important kind of threshold or

**[01:35]** really important kind of threshold or

**[01:35]** really important kind of threshold or milestone to hit.

**[01:38]** milestone to hit.

**[01:38]** milestone to hit. Um and the last one which I'm actually

**[01:40]** Um and the last one which I'm actually

**[01:40]** Um and the last one which I'm actually going to talk about a little bit more

**[01:41]** going to talk about a little bit more

**[01:41]** going to talk about a little bit more throughout the presentation is um you

**[01:44]** throughout the presentation is um you

**[01:44]** throughout the presentation is um you should really start using evals to play

**[01:46]** should really start using evals to play

**[01:46]** should really start using evals to play offense and understand which use cases

**[01:48]** offense and understand which use cases

**[01:48]** offense and understand which use cases you can solve um and how well you can

**[01:50]** you can solve um and how well you can

**[01:50]** you can solve um and how well you can solve them before you actually ship

**[01:52]** solve them before you actually ship

**[01:52]** solve them before you actually ship things not like unit tests which allow

**[01:55]** things not like unit tests which allow

**[01:55]** things not like unit tests which allow you to just test for regressions. Um,

**[01:57]** you to just test for regressions. Um,

**[01:57]** you to just test for regressions. Um, and so if you if you really adopt EVELs,

**[01:59]** and so if you if you really adopt EVELs,

**[01:59]** and so if you if you really adopt EVELs, then I think uh before you launch a new


### [02:00 - 03:00]

**[02:01]** then I think uh before you launch a new

**[02:01]** then I think uh before you launch a new product, you have a really good idea of

**[02:03]** product, you have a really good idea of

**[02:03]** product, you have a really good idea of how well the product might work given

**[02:06]** how well the product might work given

**[02:06]** how well the product might work given what your EVLs say.

**[02:13]** Um, the second lesson is that great eval

**[02:13]** Um, the second lesson is that great eval uh they have to be engineered. They

**[02:15]** uh they have to be engineered. They

**[02:16]** uh they have to be engineered. They don't just come for free with uh

**[02:18]** don't just come for free with uh

**[02:18]** don't just come for free with uh synthetic data sets and random LLM as a

**[02:21]** synthetic data sets and random LLM as a

**[02:21]** synthetic data sets and random LLM as a judge scores that you read about online.

**[02:24]** judge scores that you read about online.

**[02:24]** judge scores that you read about online. Um, and I think there's maybe two ways

**[02:26]** Um, and I think there's maybe two ways

**[02:26]** Um, and I think there's maybe two ways of thinking about this. Um, there's no

**[02:28]** of thinking about this. Um, there's no

**[02:28]** of thinking about this. Um, there's no data set that is perfectly aligned with

**[02:30]** data set that is perfectly aligned with

**[02:30]** data set that is perfectly aligned with reality. I think in the cases that there

**[02:33]** reality. I think in the cases that there

**[02:33]** reality. I think in the cases that there are, there's like basically nothing to

**[02:35]** are, there's like basically nothing to

**[02:35]** are, there's like basically nothing to do and the use cases already work, which

**[02:37]** do and the use cases already work, which

**[02:37]** do and the use cases already work, which there are a few that that are kind of

**[02:39]** there are a few that that are kind of

**[02:39]** there are a few that that are kind of like that, like solving competition math

**[02:41]** like that, like solving competition math

**[02:41]** like that, like solving competition math problems for example. But for most real

**[02:43]** problems for example. But for most real

**[02:43]** problems for example. But for most real world use cases, any data set that you

**[02:46]** world use cases, any data set that you

**[02:46]** world use cases, any data set that you can come up with ahead of time is not

**[02:47]** can come up with ahead of time is not

**[02:47]** can come up with ahead of time is not going to represent what users are

**[02:49]** going to represent what users are

**[02:49]** going to represent what users are actually experiencing. And I think um

**[02:51]** actually experiencing. And I think um

**[02:51]** actually experiencing. And I think um the best data sets are those that you

**[02:53]** the best data sets are those that you

**[02:53]** the best data sets are those that you can continuously reconcile um as you

**[02:56]** can continuously reconcile um as you

**[02:56]** can continuously reconcile um as you actually experience what happens in

**[02:57]** actually experience what happens in

**[02:57]** actually experience what happens in reality. And doing that well requires

**[02:59]** reality. And doing that well requires


### [03:00 - 04:00]

**[03:00]** reality. And doing that well requires quite a bit of engineering. Um of course

**[03:02]** quite a bit of engineering. Um of course

**[03:02]** quite a bit of engineering. Um of course brain trust can help you with that. But

**[03:03]** brain trust can help you with that. But

**[03:03]** brain trust can help you with that. But I think the the point is you have to

**[03:05]** I think the the point is you have to

**[03:05]** I think the the point is you have to think about uh a data set as an

**[03:07]** think about uh a data set as an

**[03:07]** think about uh a data set as an engineering problem not just something

**[03:08]** engineering problem not just something

**[03:08]** engineering problem not just something that's given to you.

**[03:11]** that's given to you.

**[03:11]** that's given to you. And the same is true with scorers. I

**[03:13]** And the same is true with scorers. I

**[03:13]** And the same is true with scorers. I think um a lot of people we talk to ask

**[03:16]** think um a lot of people we talk to ask

**[03:16]** think um a lot of people we talk to ask hey what scorers does brain trust come

**[03:18]** hey what scorers does brain trust come

**[03:18]** hey what scorers does brain trust come with and and how can we use those uh so

**[03:20]** with and and how can we use those uh so

**[03:20]** with and and how can we use those uh so that we don't need to think about

**[03:21]** that we don't need to think about

**[03:21]** that we don't need to think about scoring and we actually have a really uh

**[03:23]** scoring and we actually have a really uh

**[03:23]** scoring and we actually have a really uh powerful um open source library called

**[03:25]** powerful um open source library called

**[03:25]** powerful um open source library called auto evals but it's very open- source

**[03:28]** auto evals but it's very open- source

**[03:28]** auto evals but it's very open- source and uh uh flexible for a reason which is

**[03:30]** and uh uh flexible for a reason which is

**[03:30]** and uh uh flexible for a reason which is that um every company that we work with

**[03:32]** that um every company that we work with

**[03:32]** that um every company that we work with that's sufficiently advanced is writing

**[03:34]** that's sufficiently advanced is writing

**[03:34]** that's sufficiently advanced is writing their own scoring functions um and

**[03:37]** their own scoring functions um and

**[03:37]** their own scoring functions um and modifying them uh constantly and I think

**[03:39]** modifying them uh constantly and I think

**[03:39]** modifying them uh constantly and I think uh one way to think about scores is

**[03:41]** uh one way to think about scores is

**[03:41]** uh one way to think about scores is they're like a spec or like a PRD for

**[03:44]** they're like a spec or like a PRD for

**[03:44]** they're like a spec or like a PRD for your AI application. And if you think

**[03:46]** your AI application. And if you think

**[03:46]** your AI application. And if you think about them that way, um, one, it it

**[03:48]** about them that way, um, one, it it

**[03:48]** about them that way, um, one, it it actually justifies making an investment

**[03:50]** actually justifies making an investment

**[03:50]** actually justifies making an investment in scoring beyond just using something

**[03:52]** in scoring beyond just using something

**[03:52]** in scoring beyond just using something off the shelf. And two, hopefully it's

**[03:54]** off the shelf. And two, hopefully it's

**[03:54]** off the shelf. And two, hopefully it's fairly obvious that if you just use, you

**[03:56]** fairly obvious that if you just use, you

**[03:56]** fairly obvious that if you just use, you know, an open- source or generic scorer,

**[03:58]** know, an open- source or generic scorer,

**[03:58]** know, an open- source or generic scorer, that's a spec for someone else's


### [04:00 - 05:00]

**[04:00]** that's a spec for someone else's

**[04:00]** that's a spec for someone else's project, not yours.

**[04:08]** Um, there's been a real shift towards

**[04:08]** Um, there's been a real shift towards context in prompts that's not just the

**[04:10]** context in prompts that's not just the

**[04:10]** context in prompts that's not just the system prompt that you write. And I

**[04:12]** system prompt that you write. And I

**[04:12]** system prompt that you write. And I actually think that um just traditional

**[04:14]** actually think that um just traditional

**[04:14]** actually think that um just traditional prompt engineering pe people say this in

**[04:16]** prompt engineering pe people say this in

**[04:16]** prompt engineering pe people say this in different ways, but I think traditional

**[04:18]** different ways, but I think traditional

**[04:18]** different ways, but I think traditional prompt engineering is evolving quite a

**[04:19]** prompt engineering is evolving quite a

**[04:19]** prompt engineering is evolving quite a bit and it's very important to think

**[04:21]** bit and it's very important to think

**[04:21]** bit and it's very important to think about context, not just a prompt. Um so

**[04:24]** about context, not just a prompt. Um so

**[04:24]** about context, not just a prompt. Um so this um is an example of what kind of a

**[04:27]** this um is an example of what kind of a

**[04:27]** this um is an example of what kind of a modern prompt looks like for an agent.

**[04:29]** modern prompt looks like for an agent.

**[04:29]** modern prompt looks like for an agent. Usually you have a system prompt and

**[04:31]** Usually you have a system prompt and

**[04:31]** Usually you have a system prompt and then a for loop which you know uh runs

**[04:33]** then a for loop which you know uh runs

**[04:33]** then a for loop which you know uh runs LLM calls uh issues tool calls,

**[04:36]** LLM calls uh issues tool calls,

**[04:36]** LLM calls uh issues tool calls, incorporates the tool calls into the

**[04:38]** incorporates the tool calls into the

**[04:38]** incorporates the tool calls into the prompt and then iterates and iterates.

**[04:40]** prompt and then iterates and iterates.

**[04:40]** prompt and then iterates and iterates. Um, and I I actually took a few uh um uh

**[04:44]** Um, and I I actually took a few uh um uh

**[04:44]** Um, and I I actually took a few uh um uh uh trajectories from agents that that we

**[04:47]** uh trajectories from agents that that we

**[04:47]** uh trajectories from agents that that we see in the wild and summarized these

**[04:49]** see in the wild and summarized these

**[04:49]** see in the wild and summarized these numbers. And as you can see, a vast

**[04:51]** numbers. And as you can see, a vast

**[04:51]** numbers. And as you can see, a vast majority of the tokens in the average

**[04:53]** majority of the tokens in the average

**[04:53]** majority of the tokens in the average prompt um are not from the system

**[04:55]** prompt um are not from the system

**[04:55]** prompt um are not from the system prompt. And so, yes, it's very important

**[04:57]** prompt. And so, yes, it's very important

**[04:57]** prompt. And so, yes, it's very important to write a good system prompt and

**[04:59]** to write a good system prompt and

**[04:59]** to write a good system prompt and continue to improve it. But if you're


### [05:00 - 06:00]

**[05:01]** continue to improve it. But if you're

**[05:01]** continue to improve it. But if you're not very precise about uh how you define

**[05:04]** not very precise about uh how you define

**[05:04]** not very precise about uh how you define tools and how you define their outputs,

**[05:06]** tools and how you define their outputs,

**[05:06]** tools and how you define their outputs, uh then you're leaving a lot on the

**[05:08]** uh then you're leaving a lot on the

**[05:08]** uh then you're leaving a lot on the table. And I think one of the most

**[05:09]** table. And I think one of the most

**[05:09]** table. And I think one of the most important things we've learned uh

**[05:11]** important things we've learned uh

**[05:11]** important things we've learned uh together with some customers is that um

**[05:15]** together with some customers is that um

**[05:15]** together with some customers is that um you can't just take tools as a

**[05:17]** you can't just take tools as a

**[05:17]** you can't just take tools as a reflection of your APIs or your product

**[05:21]** reflection of your APIs or your product

**[05:21]** reflection of your APIs or your product as it exists today. You have to think

**[05:23]** as it exists today. You have to think

**[05:23]** as it exists today. You have to think about tools in terms of what the LLM

**[05:25]** about tools in terms of what the LLM

**[05:25]** about tools in terms of what the LLM wants to see um and how you can use you

**[05:28]** wants to see um and how you can use you

**[05:28]** wants to see um and how you can use you know exactly what you uh present to the

**[05:30]** know exactly what you uh present to the

**[05:30]** know exactly what you uh present to the LLM to make it work really well. And I

**[05:33]** LLM to make it work really well. And I

**[05:33]** LLM to make it work really well. And I think that in most projects um it's

**[05:35]** think that in most projects um it's

**[05:35]** think that in most projects um it's actually very disruptive when you write

**[05:38]** actually very disruptive when you write

**[05:38]** actually very disruptive when you write good tools. Um it's not something that's

**[05:40]** good tools. Um it's not something that's

**[05:40]** good tools. Um it's not something that's just like an API layer on top of the

**[05:42]** just like an API layer on top of the

**[05:42]** just like an API layer on top of the stuff that you already have. And the

**[05:43]** stuff that you already have. And the

**[05:44]** stuff that you already have. And the same is true with their outputs. Um

**[05:45]** same is true with their outputs. Um

**[05:45]** same is true with their outputs. Um there's one example that we uh worked on

**[05:48]** there's one example that we uh worked on

**[05:48]** there's one example that we uh worked on recently for an internal project where

**[05:51]** recently for an internal project where

**[05:51]** recently for an internal project where um shifting the output of a tool from

**[05:54]** um shifting the output of a tool from

**[05:54]** um shifting the output of a tool from JSON to YAML actually made a significant

**[05:56]** JSON to YAML actually made a significant

**[05:56]** JSON to YAML actually made a significant difference. And I know that's a little

**[05:57]** difference. And I know that's a little

**[05:57]** difference. And I know that's a little bit of a meme in the AI universe, but


### [06:00 - 07:00]

**[06:00]** bit of a meme in the AI universe, but

**[06:00]** bit of a meme in the AI universe, but it's just so much more token efficient

**[06:02]** it's just so much more token efficient

**[06:02]** it's just so much more token efficient and easy for an LLM to look at um a YAML

**[06:07]** and easy for an LLM to look at um a YAML

**[06:07]** and easy for an LLM to look at um a YAML shaped data while doing analysis than

**[06:09]** shaped data while doing analysis than

**[06:09]** shaped data while doing analysis than extremely verbose JSON. Um now, if

**[06:12]** extremely verbose JSON. Um now, if

**[06:12]** extremely verbose JSON. Um now, if you're writing code and you're plugging

**[06:14]** you're writing code and you're plugging

**[06:14]** you're writing code and you're plugging something into, you know, a charting

**[06:16]** something into, you know, a charting

**[06:16]** something into, you know, a charting library, it makes no difference because

**[06:18]** library, it makes no difference because

**[06:18]** library, it makes no difference because to JavaScript, YAML and JSON are both

**[06:20]** to JavaScript, YAML and JSON are both

**[06:20]** to JavaScript, YAML and JSON are both structured data. Um but to an LLM,

**[06:22]** structured data. Um but to an LLM,

**[06:22]** structured data. Um but to an LLM, they're very different. And so I think

**[06:24]** they're very different. And so I think

**[06:24]** they're very different. And so I think you have to be very very thoughtful

**[06:26]** you have to be very very thoughtful

**[06:26]** you have to be very very thoughtful about um you know how you actually

**[06:28]** about um you know how you actually

**[06:28]** about um you know how you actually construct the definition of a tool and

**[06:29]** construct the definition of a tool and

**[06:30]** construct the definition of a tool and how you construct its output for the LLM

**[06:32]** how you construct its output for the LLM

**[06:32]** how you construct its output for the LLM to maximally benefit from it.

**[06:40]** So I think one of the most important

**[06:40]** So I think one of the most important things we've learned um and actually I I

**[06:43]** things we've learned um and actually I I

**[06:43]** things we've learned um and actually I I would credit some of the folks at Replet

**[06:45]** would credit some of the folks at Replet

**[06:45]** would credit some of the folks at Replet uh for really uh pioneering this

**[06:46]** uh for really uh pioneering this

**[06:46]** uh for really uh pioneering this pattern. Um but you know every time a

**[06:49]** pattern. Um but you know every time a

**[06:49]** pattern. Um but you know every time a new model comes out uh everything might

**[06:51]** new model comes out uh everything might

**[06:51]** new model comes out uh everything might change. Um and I think you need to

**[06:53]** change. Um and I think you need to

**[06:54]** change. Um and I think you need to engineer your product, engineer your

**[06:56]** engineer your product, engineer your

**[06:56]** engineer your product, engineer your team, um engineer your you know mindset

**[06:59]** team, um engineer your you know mindset

**[06:59]** team, um engineer your you know mindset so that when a new model comes out if it


### [07:00 - 08:00]

**[07:01]** so that when a new model comes out if it

**[07:01]** so that when a new model comes out if it changes everything for you, you can jump

**[07:03]** changes everything for you, you can jump

**[07:03]** changes everything for you, you can jump on that opportunity and and ship

**[07:05]** on that opportunity and and ship

**[07:05]** on that opportunity and and ship something that maybe wasn't possible

**[07:07]** something that maybe wasn't possible

**[07:07]** something that maybe wasn't possible before. Um and I'm going to show you

**[07:09]** before. Um and I'm going to show you

**[07:09]** before. Um and I'm going to show you some numbers uh for a product uh feature

**[07:12]** some numbers uh for a product uh feature

**[07:12]** some numbers uh for a product uh feature that we're actually launching and I'm

**[07:14]** that we're actually launching and I'm

**[07:14]** that we're actually launching and I'm going to show you a little bit of it

**[07:15]** going to show you a little bit of it

**[07:15]** going to show you a little bit of it today. Um, but uh we we've had an eval

**[07:18]** today. Um, but uh we we've had an eval

**[07:18]** today. Um, but uh we we've had an eval for a while that tells us how well this

**[07:21]** for a while that tells us how well this

**[07:21]** for a while that tells us how well this feature might work and we run it every

**[07:23]** feature might work and we run it every

**[07:23]** feature might work and we run it every few months and you can see you know it

**[07:25]** few months and you can see you know it

**[07:25]** few months and you can see you know it wasn't uh that long ago that GPT40

**[07:28]** wasn't uh that long ago that GPT40

**[07:28]** wasn't uh that long ago that GPT40 was the best model out there. Um but but

**[07:31]** was the best model out there. Um but but

**[07:31]** was the best model out there. Um but but things have changed uh and you know

**[07:33]** things have changed uh and you know

**[07:33]** things have changed uh and you know progressively uh GPT41 did a little bit

**[07:36]** progressively uh GPT41 did a little bit

**[07:36]** progressively uh GPT41 did a little bit better. Uh 37 sonnet is much better and

**[07:39]** better. Uh 37 sonnet is much better and

**[07:39]** better. Uh 37 sonnet is much better and and for sonnet is actually even more

**[07:41]** and for sonnet is actually even more

**[07:41]** and for sonnet is actually even more remarkably better. Um and uh what what

**[07:45]** remarkably better. Um and uh what what

**[07:45]** remarkably better. Um and uh what what that's meant for us is that this feature

**[07:47]** that's meant for us is that this feature

**[07:47]** that's meant for us is that this feature that um you know at 10% would would

**[07:50]** that um you know at 10% would would

**[07:50]** that um you know at 10% would would really not be viable for our users to

**[07:52]** really not be viable for our users to

**[07:52]** really not be viable for our users to use suddenly becomes viable. Um and so

**[07:55]** use suddenly becomes viable. Um and so

**[07:55]** use suddenly becomes viable. Um and so you know Cloud 4 sonnet actually came

**[07:57]** you know Cloud 4 sonnet actually came

**[07:57]** you know Cloud 4 sonnet actually came out two weeks ago. Um and we're shipping

**[07:59]** out two weeks ago. Um and we're shipping

**[07:59]** out two weeks ago. Um and we're shipping the first version of this feature today


### [08:00 - 09:00]

**[08:01]** the first version of this feature today

**[08:01]** the first version of this feature today which is just two weeks later. But we

**[08:03]** which is just two weeks later. But we

**[08:03]** which is just two weeks later. But we were able to jump on that opportunity

**[08:05]** were able to jump on that opportunity

**[08:05]** were able to jump on that opportunity because we ran this eval. Um we were

**[08:07]** because we ran this eval. Um we were

**[08:08]** because we ran this eval. Um we were ready to do it and we we saw that okay

**[08:10]** ready to do it and we we saw that okay

**[08:10]** ready to do it and we we saw that okay great we've actually finally crossed uh

**[08:12]** great we've actually finally crossed uh

**[08:12]** great we've actually finally crossed uh this threshold. Um so everyone that I

**[08:14]** this threshold. Um so everyone that I

**[08:14]** this threshold. Um so everyone that I personally work with or talk to I

**[08:16]** personally work with or talk to I

**[08:16]** personally work with or talk to I encourage to create evals that are very

**[08:19]** encourage to create evals that are very

**[08:19]** encourage to create evals that are very very ambitious and um likely not uh uh

**[08:24]** very ambitious and um likely not uh uh

**[08:24]** very ambitious and um likely not uh uh viable with today's models and construct

**[08:26]** viable with today's models and construct

**[08:26]** viable with today's models and construct them in a way that when a new model

**[08:28]** them in a way that when a new model

**[08:28]** them in a way that when a new model comes out you can just plug the new

**[08:30]** comes out you can just plug the new

**[08:30]** comes out you can just plug the new model in and try it. Um, in Brain Trust,

**[08:32]** model in and try it. Um, in Brain Trust,

**[08:32]** model in and try it. Um, in Brain Trust, we have this tool called the Brain Trust

**[08:34]** we have this tool called the Brain Trust

**[08:34]** we have this tool called the Brain Trust proxy. Um, there's a lot of of similar

**[08:37]** proxy. Um, there's a lot of of similar

**[08:37]** proxy. Um, there's a lot of of similar tools. You could use ours or you could

**[08:38]** tools. You could use ours or you could

**[08:38]** tools. You could use ours or you could use something else. Uh, but really the

**[08:40]** use something else. Uh, but really the

**[08:40]** use something else. Uh, but really the point is that you don't need to change

**[08:42]** point is that you don't need to change

**[08:42]** point is that you don't need to change any code to work across model providers.

**[08:45]** any code to work across model providers.

**[08:45]** any code to work across model providers. And so, um, you know, Google just

**[08:47]** And so, um, you know, Google just

**[08:47]** And so, um, you know, Google just launched the newest version of of uh,

**[08:49]** launched the newest version of of uh,

**[08:49]** launched the newest version of of uh, Gemini. Um, actually Gemini 2.5 Pro0520

**[08:54]** Gemini. Um, actually Gemini 2.5 Pro0520

**[08:54]** Gemini. Um, actually Gemini 2.5 Pro0520 scores 1% on this benchmark. Uh, so we

**[08:57]** scores 1% on this benchmark. Uh, so we

**[08:57]** scores 1% on this benchmark. Uh, so we didn't even put it on here. Um, but

**[08:58]** didn't even put it on here. Um, but

**[08:58]** didn't even put it on here. Um, but maybe the thing they launched today


### [09:00 - 10:00]

**[09:00]** maybe the thing they launched today

**[09:00]** maybe the thing they launched today actually uh does a lot better. We can

**[09:02]** actually uh does a lot better. We can

**[09:02]** actually uh does a lot better. We can find out, you know, with with just a few

**[09:03]** find out, you know, with with just a few

**[09:04]** find out, you know, with with just a few keystrokes maybe right after this talk.

**[09:12]** Um, and the last thing is it's super

**[09:12]** Um, and the last thing is it's super important if you uh think about um

**[09:15]** important if you uh think about um

**[09:15]** important if you uh think about um optimizing your prompts to optimize the

**[09:17]** optimizing your prompts to optimize the

**[09:17]** optimizing your prompts to optimize the entire system. Um so that means uh

**[09:20]** entire system. Um so that means uh

**[09:20]** entire system. Um so that means uh thinking holistically about your um AI

**[09:24]** thinking holistically about your um AI

**[09:24]** thinking holistically about your um AI system as the data that you use for your

**[09:26]** system as the data that you use for your

**[09:26]** system as the data that you use for your evals, the task which is you know the

**[09:29]** evals, the task which is you know the

**[09:29]** evals, the task which is you know the prompt, the agentic system, tools etc

**[09:32]** prompt, the agentic system, tools etc

**[09:32]** prompt, the agentic system, tools etc and the scoring functions and and every

**[09:34]** and the scoring functions and and every

**[09:34]** and the scoring functions and and every time you think about making um you know

**[09:37]** time you think about making um you know

**[09:37]** time you think about making um you know your your app better you need to think

**[09:39]** your your app better you need to think

**[09:39]** your your app better you need to think about improving this overall system. Um

**[09:42]** about improving this overall system. Um

**[09:42]** about improving this overall system. Um we actually ran a benchmark uh which is

**[09:45]** we actually ran a benchmark uh which is

**[09:45]** we actually ran a benchmark uh which is uh the same benchmark that I showed

**[09:47]** uh the same benchmark that I showed

**[09:47]** uh the same benchmark that I showed previously. um it autooptimizes prompts

**[09:50]** previously. um it autooptimizes prompts

**[09:50]** previously. um it autooptimizes prompts uh using um an LLM and uh we ran it once

**[09:56]** uh using um an LLM and uh we ran it once

**[09:56]** uh using um an LLM and uh we ran it once by just giving it the prompt and saying

**[09:58]** by just giving it the prompt and saying

**[09:58]** by just giving it the prompt and saying like hey please optimize the prompt and


### [10:00 - 11:00]

**[10:00]** like hey please optimize the prompt and

**[10:00]** like hey please optimize the prompt and a second time giving it the prompt the

**[10:02]** a second time giving it the prompt the

**[10:02]** a second time giving it the prompt the data set and the scores and said please

**[10:04]** data set and the scores and said please

**[10:04]** data set and the scores and said please optimize this whole system. Um and you

**[10:06]** optimize this whole system. Um and you

**[10:06]** optimize this whole system. Um and you can see there's a very dramatic

**[10:07]** can see there's a very dramatic

**[10:07]** can see there's a very dramatic difference. So again um something goes

**[10:10]** difference. So again um something goes

**[10:10]** difference. So again um something goes from unviable to viable. Um, but it's

**[10:13]** from unviable to viable. Um, but it's

**[10:13]** from unviable to viable. Um, but it's just super important to optimize the

**[10:15]** just super important to optimize the

**[10:15]** just super important to optimize the entire system, not not just the prompt.

**[10:23]** And actually, uh, this is, uh, a new

**[10:23]** And actually, uh, this is, uh, a new product feature that we are starting to

**[10:25]** product feature that we are starting to

**[10:25]** product feature that we are starting to launch today. Um, if you're a Brain

**[10:27]** launch today. Um, if you're a Brain

**[10:27]** launch today. Um, if you're a Brain Trust user, uh, you can go to the

**[10:29]** Trust user, uh, you can go to the

**[10:29]** Trust user, uh, you can go to the feature flag section of Brain and turn

**[10:32]** feature flag section of Brain and turn

**[10:32]** feature flag section of Brain and turn on a new feature flag called loop. Um

**[10:34]** on a new feature flag called loop. Um

**[10:34]** on a new feature flag called loop. Um and uh the loop is this amazing cool new

**[10:38]** and uh the loop is this amazing cool new

**[10:38]** and uh the loop is this amazing cool new feature that actually autooptimizes

**[10:41]** feature that actually autooptimizes

**[10:41]** feature that actually autooptimizes uh your eval um directly within brain

**[10:43]** uh your eval um directly within brain

**[10:43]** uh your eval um directly within brain trust. Uh so uh you can work in our

**[10:46]** trust. Uh so uh you can work in our

**[10:46]** trust. Uh so uh you can work in our playground and um give it you know a

**[10:49]** playground and um give it you know a

**[10:49]** playground and um give it you know a prompt uh a data set um and some scores

**[10:52]** prompt uh a data set um and some scores

**[10:52]** prompt uh a data set um and some scores and it can actually create prompts, data

**[10:54]** and it can actually create prompts, data

**[10:54]** and it can actually create prompts, data sets and scores too um and just you know

**[10:57]** sets and scores too um and just you know

**[10:57]** sets and scores too um and just you know work with it. Uh the kinds of things

**[10:59]** work with it. Uh the kinds of things

**[10:59]** work with it. Uh the kinds of things that we've seen work really well are


### [11:00 - 12:00]

**[11:01]** that we've seen work really well are

**[11:01]** that we've seen work really well are optimize this prompt or uh what am I

**[11:04]** optimize this prompt or uh what am I

**[11:04]** optimize this prompt or uh what am I missing from this data set that would be

**[11:07]** missing from this data set that would be

**[11:07]** missing from this data set that would be really good to test for this use case?

**[11:09]** really good to test for this use case?

**[11:09]** really good to test for this use case? Um why is my score so low? Um or why is

**[11:13]** Um why is my score so low? Um or why is

**[11:13]** Um why is my score so low? Um or why is my score so high? Can you please help me

**[11:15]** my score so high? Can you please help me

**[11:15]** my score so high? Can you please help me write a score that is uh you know

**[11:17]** write a score that is uh you know

**[11:17]** write a score that is uh you know harsher than the one that I have right

**[11:19]** harsher than the one that I have right

**[11:19]** harsher than the one that I have right now? Um you can also try it out with

**[11:21]** now? Um you can also try it out with

**[11:21]** now? Um you can also try it out with different models. So, uh, as you could

**[11:23]** different models. So, uh, as you could

**[11:23]** different models. So, uh, as you could see from this, uh, we've definitely seen

**[11:26]** see from this, uh, we've definitely seen

**[11:26]** see from this, uh, we've definitely seen the best performance with Cloud 4 Sonnet

**[11:28]** the best performance with Cloud 4 Sonnet

**[11:28]** the best performance with Cloud 4 Sonnet and Cloud 4 Opus performs a couple of

**[11:30]** and Cloud 4 Opus performs a couple of

**[11:30]** and Cloud 4 Opus performs a couple of percentage points better. Um, but we

**[11:33]** percentage points better. Um, but we

**[11:33]** percentage points better. Um, but we encourage you to try it out with

**[11:34]** encourage you to try it out with

**[11:34]** encourage you to try it out with different models. You can use 03, you

**[11:36]** different models. You can use 03, you

**[11:36]** different models. You can use 03, you can use 04 mini, you can use Gemini,

**[11:39]** can use 04 mini, you can use Gemini,

**[11:39]** can use 04 mini, you can use Gemini, maybe you're building your own uh, LLM

**[11:41]** maybe you're building your own uh, LLM

**[11:41]** maybe you're building your own uh, LLM or fine-tune model. You can try that as

**[11:43]** or fine-tune model. You can try that as

**[11:43]** or fine-tune model. You can try that as well. Um, and yeah, we're very excited

**[11:46]** well. Um, and yeah, we're very excited

**[11:46]** well. Um, and yeah, we're very excited uh, for this. I think uh, I'm going to

**[11:48]** uh, for this. I think uh, I'm going to

**[11:48]** uh, for this. I think uh, I'm going to talk about this a little bit later. Um,

**[11:50]** talk about this a little bit later. Um,

**[11:50]** talk about this a little bit later. Um, and I'm happy to do it with some Q&A as

**[11:52]** and I'm happy to do it with some Q&A as

**[11:52]** and I'm happy to do it with some Q&A as well, but um, I actually I really think

**[11:54]** well, but um, I actually I really think

**[11:54]** well, but um, I actually I really think that the workflow around evals is going

**[11:56]** that the workflow around evals is going

**[11:56]** that the workflow around evals is going to dramatically change now that LLMs are

**[11:59]** to dramatically change now that LLMs are

**[11:59]** to dramatically change now that LLMs are capable of looking at prompts and


### [12:00 - 13:00]

**[12:03]** capable of looking at prompts and

**[12:03]** capable of looking at prompts and looking at data and actually making um,

**[12:06]** looking at data and actually making um,

**[12:06]** looking at data and actually making um, you know, constructive improvements

**[12:07]** you know, constructive improvements

**[12:07]** you know, constructive improvements automatically. A lot of the manual labor

**[12:09]** automatically. A lot of the manual labor

**[12:10]** automatically. A lot of the manual labor that went into iterating with EVELs um,

**[12:12]** that went into iterating with EVELs um,

**[12:12]** that went into iterating with EVELs um, doesn't need to be there anymore. So,

**[12:13]** doesn't need to be there anymore. So,

**[12:14]** doesn't need to be there anymore. So, it's it's really exciting. Uh, we're

**[12:15]** it's it's really exciting. Uh, we're

**[12:15]** it's it's really exciting. Uh, we're excited uh, to ship this and and to

**[12:17]** excited uh, to ship this and and to

**[12:17]** excited uh, to ship this and and to start to get some feedback.

**[12:24]** Uh so just to recap um five lessons that

**[12:24]** Uh so just to recap um five lessons that I think are really important. Um

**[12:26]** I think are really important. Um

**[12:26]** I think are really important. Um effective eval speak for themselves.

**[12:28]** effective eval speak for themselves.

**[12:28]** effective eval speak for themselves. It's it's important to understand

**[12:29]** It's it's important to understand

**[12:30]** It's it's important to understand whether you've kind of reached a point

**[12:31]** whether you've kind of reached a point

**[12:31]** whether you've kind of reached a point of eval competence in your organization

**[12:33]** of eval competence in your organization

**[12:33]** of eval competence in your organization or not. It's okay if you haven't. Um

**[12:35]** or not. It's okay if you haven't. Um

**[12:36]** or not. It's okay if you haven't. Um it's not easy, but it's important to be

**[12:38]** it's not easy, but it's important to be

**[12:38]** it's not easy, but it's important to be honest about that and work towards it.

**[12:41]** honest about that and work towards it.

**[12:41]** honest about that and work towards it. Um when you're working on evals, it's

**[12:43]** Um when you're working on evals, it's

**[12:43]** Um when you're working on evals, it's very important to engineer the entire

**[12:45]** very important to engineer the entire

**[12:45]** very important to engineer the entire system. So don't just think about the

**[12:47]** system. So don't just think about the

**[12:47]** system. So don't just think about the prompt. Don't just think about improving

**[12:49]** prompt. Don't just think about improving

**[12:49]** prompt. Don't just think about improving the prompt. Please don't just use

**[12:51]** the prompt. Please don't just use

**[12:51]** the prompt. Please don't just use synthetic data or hugging face data

**[12:53]** synthetic data or hugging face data

**[12:53]** synthetic data or hugging face data sets. I know they're awesome, but please

**[12:55]** sets. I know they're awesome, but please

**[12:55]** sets. I know they're awesome, but please use more than just that. Please don't

**[12:57]** use more than just that. Please don't

**[12:57]** use more than just that. Please don't use off-the-shelf scores only. Write


### [13:00 - 14:00]

**[13:00]** use off-the-shelf scores only. Write

**[13:00]** use off-the-shelf scores only. Write your own. Think very deliberately about

**[13:02]** your own. Think very deliberately about

**[13:02]** your own. Think very deliberately about um how you can craft the spec of what

**[13:05]** um how you can craft the spec of what

**[13:05]** um how you can craft the spec of what you're working on into your scoring

**[13:07]** you're working on into your scoring

**[13:07]** you're working on into your scoring functions.

**[13:08]** functions.

**[13:08]** functions. Um think very carefully about context.

**[13:10]** Um think very carefully about context.

**[13:10]** Um think very carefully about context. And I think in particular um what helps

**[13:13]** And I think in particular um what helps

**[13:13]** And I think in particular um what helps me personally is to think about writing

**[13:15]** me personally is to think about writing

**[13:15]** me personally is to think about writing tools uh like I would think about

**[13:17]** tools uh like I would think about

**[13:17]** tools uh like I would think about writing a prompt. It's my opportunity to

**[13:19]** writing a prompt. It's my opportunity to

**[13:19]** writing a prompt. It's my opportunity to communicate with an LLM and set it up

**[13:22]** communicate with an LLM and set it up

**[13:22]** communicate with an LLM and set it up for success. And how I define the API

**[13:25]** for success. And how I define the API

**[13:25]** for success. And how I define the API interface of the tool and I define its

**[13:26]** interface of the tool and I define its

**[13:26]** interface of the tool and I define its output has a very dramatic impact on

**[13:29]** output has a very dramatic impact on

**[13:29]** output has a very dramatic impact on that.

**[13:31]** that.

**[13:31]** that. Make sure that you're ready for new

**[13:33]** Make sure that you're ready for new

**[13:33]** Make sure that you're ready for new models to come out and to just change

**[13:35]** models to come out and to just change

**[13:35]** models to come out and to just change everything. Um, so if a if a new model

**[13:38]** everything. Um, so if a if a new model

**[13:38]** everything. Um, so if a if a new model comes out, you want to be prepared to

**[13:39]** comes out, you want to be prepared to

**[13:40]** comes out, you want to be prepared to know that immediately, ideally the day

**[13:42]** know that immediately, ideally the day

**[13:42]** know that immediately, ideally the day that it comes out. Um, and also be

**[13:44]** that it comes out. Um, and also be

**[13:44]** that it comes out. Um, and also be prepared to like rip out everything and

**[13:46]** prepared to like rip out everything and

**[13:46]** prepared to like rip out everything and replace it with a fundamentally new

**[13:49]** replace it with a fundamentally new

**[13:49]** replace it with a fundamentally new architecture that takes advantage of

**[13:50]** architecture that takes advantage of

**[13:50]** architecture that takes advantage of that new model. And I think part of that

**[13:52]** that new model. And I think part of that

**[13:52]** that new model. And I think part of that is obviously having the right eval. Part

**[13:55]** is obviously having the right eval. Part

**[13:55]** is obviously having the right eval. Part of it is engineering your product in a

**[13:57]** of it is engineering your product in a

**[13:57]** of it is engineering your product in a way that actually allows you to do that.


### [14:00 - 15:00]

**[14:00]** way that actually allows you to do that.

**[14:00]** way that actually allows you to do that. And then finally when you think about

**[14:02]** And then finally when you think about

**[14:02]** And then finally when you think about optimizing or improving uh your eval

**[14:04]** optimizing or improving uh your eval

**[14:04]** optimizing or improving uh your eval performance um you have to think about

**[14:06]** performance um you have to think about

**[14:06]** performance um you have to think about optimizing the whole system the data and

**[14:09]** optimizing the whole system the data and

**[14:09]** optimizing the whole system the data and how you get that data the task itself um

**[14:12]** how you get that data the task itself um

**[14:12]** how you get that data the task itself um which you know the prompt tools etc and

**[14:15]** which you know the prompt tools etc and

**[14:15]** which you know the prompt tools etc and the scoring functions

**[14:21]** and with that uh we have some time for

**[14:21]** and with that uh we have some time for Q&A.

**[14:22]** Q&A.

**[14:22]** Q&A. >> Yeah there's uh two microphones up here

**[14:25]** >> Yeah there's uh two microphones up here

**[14:25]** >> Yeah there's uh two microphones up here one on the left side one on the right

**[14:27]** one on the left side one on the right

**[14:27]** one on the left side one on the right side. uh feel free to stand up and ask

**[14:30]** side. uh feel free to stand up and ask

**[14:30]** side. uh feel free to stand up and ask your questions.

**[14:39]** >> Hi, this is Joti. Um, one of your slides

**[14:39]** >> Hi, this is Joti. Um, one of your slides said take feedback and turn it into an

**[14:42]** said take feedback and turn it into an

**[14:42]** said take feedback and turn it into an eval.

**[14:44]** eval.

**[14:44]** eval. Are you concerned about overfitting

**[14:46]** Are you concerned about overfitting

**[14:46]** Are you concerned about overfitting evals at that point where every feedback

**[14:48]** evals at that point where every feedback

**[14:48]** evals at that point where every feedback then turns into an eval?

**[14:50]** then turns into an eval?

**[14:50]** then turns into an eval? >> Oh, that's a great question. Um, also

**[14:52]** >> Oh, that's a great question. Um, also

**[14:52]** >> Oh, that's a great question. Um, also nice to see you. Um so uh the question

**[14:55]** nice to see you. Um so uh the question

**[14:55]** nice to see you. Um so uh the question was um one of the slides was about

**[14:58]** was um one of the slides was about

**[14:58]** was um one of the slides was about taking feedback uh from you know real


### [15:00 - 16:00]

**[15:01]** taking feedback uh from you know real

**[15:01]** taking feedback uh from you know real data and adding it to a data set and

**[15:03]** data and adding it to a data set and

**[15:03]** data and adding it to a data set and incorporating it in an email. Are you

**[15:04]** incorporating it in an email. Are you

**[15:04]** incorporating it in an email. Are you worried about overfitting? Um and I

**[15:06]** worried about overfitting? Um and I

**[15:06]** worried about overfitting? Um and I think the answer is I'm actually way

**[15:08]** think the answer is I'm actually way

**[15:08]** think the answer is I'm actually way more worried about overfitting to the

**[15:10]** more worried about overfitting to the

**[15:10]** more worried about overfitting to the data set without the user's feedback

**[15:12]** data set without the user's feedback

**[15:12]** data set without the user's feedback than I am to um adjusting the fit to

**[15:16]** than I am to um adjusting the fit to

**[15:16]** than I am to um adjusting the fit to incorporate the user's feedback. Like

**[15:17]** incorporate the user's feedback. Like

**[15:17]** incorporate the user's feedback. Like the most important thing about a data

**[15:20]** the most important thing about a data

**[15:20]** the most important thing about a data set is not the state of the data set at

**[15:23]** set is not the state of the data set at

**[15:23]** set is not the state of the data set at any point in time. It is how well you

**[15:25]** any point in time. It is how well you

**[15:25]** any point in time. It is how well you are equipped to reconcile the data set

**[15:27]** are equipped to reconcile the data set

**[15:28]** are equipped to reconcile the data set with the reality that you want. Um and I

**[15:30]** with the reality that you want. Um and I

**[15:30]** with the reality that you want. Um and I actually think one of the things that we

**[15:31]** actually think one of the things that we

**[15:31]** actually think one of the things that we discourage uh in the product and some

**[15:34]** discourage uh in the product and some

**[15:34]** discourage uh in the product and some people complain to us about this. I get

**[15:35]** people complain to us about this. I get

**[15:35]** people complain to us about this. I get it uh if you're one of those people. Um

**[15:37]** it uh if you're one of those people. Um

**[15:37]** it uh if you're one of those people. Um but we don't automatically take user

**[15:40]** but we don't automatically take user

**[15:40]** but we don't automatically take user feedback and add it to data sets right

**[15:41]** feedback and add it to data sets right

**[15:41]** feedback and add it to data sets right now. We actually want a human who has

**[15:44]** now. We actually want a human who has

**[15:44]** now. We actually want a human who has some taste and maybe uh can build some

**[15:47]** some taste and maybe uh can build some

**[15:47]** some taste and maybe uh can build some intuition about the problem to find the

**[15:50]** intuition about the problem to find the

**[15:50]** intuition about the problem to find the uh data points from users that are

**[15:52]** uh data points from users that are

**[15:52]** uh data points from users that are interesting and add them to the data

**[15:54]** interesting and add them to the data

**[15:54]** interesting and add them to the data set. And I think that is your

**[15:55]** set. And I think that is your

**[15:55]** set. And I think that is your opportunity as a user to apply some

**[15:57]** opportunity as a user to apply some

**[15:57]** opportunity as a user to apply some judgment about like oh okay this user is


### [16:00 - 17:00]

**[16:00]** judgment about like oh okay this user is

**[16:00]** judgment about like oh okay this user is trying to do something that should

**[16:01]** trying to do something that should

**[16:01]** trying to do something that should obviously work. It's really sad that it

**[16:03]** obviously work. It's really sad that it

**[16:03]** obviously work. It's really sad that it doesn't work in my product. Let me add

**[16:05]** doesn't work in my product. Let me add

**[16:05]** doesn't work in my product. Let me add it to the data set so I can make sure it

**[16:07]** it to the data set so I can make sure it

**[16:07]** it to the data set so I can make sure it does. Excuse me.

**[16:10]** does. Excuse me.

**[16:10]** does. Excuse me. You had a slide I think in the tool

**[16:11]** You had a slide I think in the tool

**[16:11]** You had a slide I think in the tool descriptions about like with some

**[16:14]** descriptions about like with some

**[16:14]** descriptions about like with some percentages on it. Yeah, this one.

**[16:16]** percentages on it. Yeah, this one.

**[16:16]** percentages on it. Yeah, this one. >> What what is that?

**[16:17]** >> What what is that?

**[16:17]** >> What what is that? >> Yeah. So, um we took a few agents um

**[16:21]** >> Yeah. So, um we took a few agents um

**[16:21]** >> Yeah. So, um we took a few agents um like we you know have a lot of traces uh

**[16:24]** like we you know have a lot of traces uh

**[16:24]** like we you know have a lot of traces uh and we analyzed the relative um number

**[16:27]** and we analyzed the relative um number

**[16:27]** and we analyzed the relative um number of tokens for different message types.

**[16:30]** of tokens for different message types.

**[16:30]** of tokens for different message types. So the system prompt is one message

**[16:32]** So the system prompt is one message

**[16:32]** So the system prompt is one message type. Tool definitions um are you know

**[16:34]** type. Tool definitions um are you know

**[16:34]** type. Tool definitions um are you know the spec of what uh tools the model can

**[16:37]** the spec of what uh tools the model can

**[16:37]** the spec of what uh tools the model can call. user and assistant um uh are um

**[16:42]** call. user and assistant um uh are um

**[16:42]** call. user and assistant um uh are um tokens from user and assistant just text

**[16:45]** tokens from user and assistant just text

**[16:45]** tokens from user and assistant just text interactions and then tool responses are

**[16:48]** interactions and then tool responses are

**[16:48]** interactions and then tool responses are um tokens from the that you know that

**[16:50]** um tokens from the that you know that

**[16:50]** um tokens from the that you know that the tool generates itself.

**[16:52]** the tool generates itself.

**[16:52]** the tool generates itself. >> Oh this is the percentage of tokens

**[16:54]** >> Oh this is the percentage of tokens

**[16:54]** >> Oh this is the percentage of tokens >> correct and this is the relative

**[16:55]** >> correct and this is the relative

**[16:55]** >> correct and this is the relative percentage of those tokens. Yeah. Yeah.

**[16:59]** percentage of those tokens. Yeah. Yeah.

**[16:59]** percentage of those tokens. Yeah. Yeah. Yeah. Yeah. So the the the the point


### [17:00 - 18:00]

**[17:00]** Yeah. Yeah. So the the the the point

**[17:00]** Yeah. Yeah. So the the the the point that we're trying to make here is that

**[17:03]** that we're trying to make here is that

**[17:03]** that we're trying to make here is that um I think in modern agentic systems uh

**[17:06]** um I think in modern agentic systems uh

**[17:06]** um I think in modern agentic systems uh tools actually like very very

**[17:09]** tools actually like very very

**[17:09]** tools actually like very very significantly dominate the token budget

**[17:12]** significantly dominate the token budget

**[17:12]** significantly dominate the token budget of the LLM. And I think that it's very

**[17:14]** of the LLM. And I think that it's very

**[17:14]** of the LLM. And I think that it's very important to um think about how you

**[17:17]** important to um think about how you

**[17:17]** important to um think about how you define the definition of tools and how

**[17:19]** define the definition of tools and how

**[17:19]** define the definition of tools and how you define their outputs so that you u

**[17:21]** you define their outputs so that you u

**[17:21]** you define their outputs so that you u you know engineer the LLM for success.

**[17:24]** you know engineer the LLM for success.

**[17:24]** you know engineer the LLM for success. uh not just sort of take you know your

**[17:26]** uh not just sort of take you know your

**[17:26]** uh not just sort of take you know your GraphQL API and give it as a bunch of uh

**[17:29]** GraphQL API and give it as a bunch of uh

**[17:29]** GraphQL API and give it as a bunch of uh you know uh tool calls to to the LLM.

**[17:33]** you know uh tool calls to to the LLM.

**[17:34]** you know uh tool calls to to the LLM. >> Um first off that point about the thumbs

**[17:36]** >> Um first off that point about the thumbs

**[17:36]** >> Um first off that point about the thumbs down is such a good point. I'm working

**[17:39]** down is such a good point. I'm working

**[17:39]** down is such a good point. I'm working with the government and people don't

**[17:41]** with the government and people don't

**[17:41]** with the government and people don't like the answer they got for example

**[17:44]** like the answer they got for example

**[17:44]** like the answer they got for example about taxes and they give it a thumbs

**[17:45]** about taxes and they give it a thumbs

**[17:45]** about taxes and they give it a thumbs down.

**[17:46]** down.

**[17:46]** down. >> Yeah.

**[17:46]** >> Yeah.

**[17:46]** >> Yeah. >> Right. So like adding that human aspect

**[17:49]** >> Right. So like adding that human aspect

**[17:49]** >> Right. So like adding that human aspect is a really good idea. We actually even

**[17:51]** is a really good idea. We actually even

**[17:51]** is a really good idea. We actually even added a little thing that said the

**[17:53]** added a little thing that said the

**[17:53]** added a little thing that said the answer is right, but I just don't like

**[17:55]** answer is right, but I just don't like

**[17:55]** answer is right, but I just don't like it.

**[17:56]** it.

**[17:56]** it. >> That's awesome.

**[17:57]** >> That's awesome.

**[17:57]** >> That's awesome. >> Um, but my question is about your point

**[17:59]** >> Um, but my question is about your point

**[17:59]** >> Um, but my question is about your point that the new model changes everything.


### [18:00 - 19:00]

**[18:02]** that the new model changes everything.

**[18:02]** that the new model changes everything. We've updated our models several times

**[18:05]** We've updated our models several times

**[18:05]** We've updated our models several times and and use code and open AAI and we

**[18:09]** and and use code and open AAI and we

**[18:09]** and and use code and open AAI and we haven't found huge differences other

**[18:12]** haven't found huge differences other

**[18:12]** haven't found huge differences other than recently someone really cheap

**[18:14]** than recently someone really cheap

**[18:14]** than recently someone really cheap wanted to use 4.1 mini and like it

**[18:18]** wanted to use 4.1 mini and like it

**[18:18]** wanted to use 4.1 mini and like it seemed to ignore every it. I swear it

**[18:21]** seemed to ignore every it. I swear it

**[18:21]** seemed to ignore every it. I swear it ignored the system prop completely.

**[18:23]** ignored the system prop completely.

**[18:23]** ignored the system prop completely. >> Yeah.

**[18:23]** >> Yeah.

**[18:23]** >> Yeah. >> But what kind of things when you say it

**[18:25]** >> But what kind of things when you say it

**[18:25]** >> But what kind of things when you say it changes everything, can you tell me a

**[18:27]** changes everything, can you tell me a

**[18:27]** changes everything, can you tell me a little more about what kind of changes

**[18:28]** little more about what kind of changes

**[18:28]** little more about what kind of changes you're seeing?

**[18:29]** you're seeing?

**[18:29]** you're seeing? >> For sure. I think um the use case that

**[18:31]** >> For sure. I think um the use case that

**[18:32]** >> For sure. I think um the use case that we just shipped with loop is a really

**[18:33]** we just shipped with loop is a really

**[18:33]** we just shipped with loop is a really good example of that. So this is a very

**[18:35]** good example of that. So this is a very

**[18:35]** good example of that. So this is a very ambitious uh agent. It's looking at

**[18:38]** ambitious uh agent. It's looking at

**[18:38]** ambitious uh agent. It's looking at prompts and uh data sets and scores and

**[18:42]** prompts and uh data sets and scores and

**[18:42]** prompts and uh data sets and scores and automatically optimizing the prompts

**[18:44]** automatically optimizing the prompts

**[18:44]** automatically optimizing the prompts based on the data sets and scores. And

**[18:46]** based on the data sets and scores. And

**[18:46]** based on the data sets and scores. And this is something that um you know we

**[18:49]** this is something that um you know we

**[18:49]** this is something that um you know we wrote a benchmark for a while ago and we

**[18:51]** wrote a benchmark for a while ago and we

**[18:51]** wrote a benchmark for a while ago and we ran with every consecutive model launch

**[18:54]** ran with every consecutive model launch

**[18:54]** ran with every consecutive model launch and the numbers looked more like what

**[18:56]** and the numbers looked more like what

**[18:56]** and the numbers looked more like what you see for GPT40 for a very long time.

**[18:59]** you see for GPT40 for a very long time.

**[18:59]** you see for GPT40 for a very long time. This isn't true for every benchmark. So


### [19:00 - 20:00]

**[19:02]** This isn't true for every benchmark. So

**[19:02]** This isn't true for every benchmark. So um as part of this exercise we actually

**[19:03]** um as part of this exercise we actually

**[19:03]** um as part of this exercise we actually have a bunch of uh evals that loop

**[19:07]** have a bunch of uh evals that loop

**[19:07]** have a bunch of uh evals that loop optimizes. That's our eval set. And

**[19:09]** optimizes. That's our eval set. And

**[19:09]** optimizes. That's our eval set. And there's some evals like uh classifying

**[19:12]** there's some evals like uh classifying

**[19:12]** there's some evals like uh classifying taking movie quotes and figuring out

**[19:14]** taking movie quotes and figuring out

**[19:14]** taking movie quotes and figuring out what movie they're coming from that have

**[19:16]** what movie they're coming from that have

**[19:16]** what movie they're coming from that have worked really well since GPT 3.5. Um and

**[19:19]** worked really well since GPT 3.5. Um and

**[19:19]** worked really well since GPT 3.5. Um and so there are certain use cases where it

**[19:21]** so there are certain use cases where it

**[19:21]** so there are certain use cases where it just doesn't matter. There are other use

**[19:23]** just doesn't matter. There are other use

**[19:23]** just doesn't matter. There are other use cases where um they're so ambitious that

**[19:25]** cases where um they're so ambitious that

**[19:25]** cases where um they're so ambitious that they just don't work today. And I think

**[19:27]** they just don't work today. And I think

**[19:27]** they just don't work today. And I think you want to create evals uh so that if

**[19:30]** you want to create evals uh so that if

**[19:30]** you want to create evals uh so that if there's something ambitious that you

**[19:31]** there's something ambitious that you

**[19:31]** there's something ambitious that you want to do in the future, you are very

**[19:33]** want to do in the future, you are very

**[19:33]** want to do in the future, you are very well prepared when a new model comes out

**[19:35]** well prepared when a new model comes out

**[19:35]** well prepared when a new model comes out to just push a button and find that out.

**[19:38]** to just push a button and find that out.

**[19:38]** to just push a button and find that out. >> Okay. Thank you.


