# 2025 is the Year of Evals! Just like 2024, and 2023, and … — John Dickerson, CEO Mozilla AI

**Video URL:** https://www.youtube.com/watch?v=CQGuvf6gSrM

---

## Executive Summary

John Dickerson, CEO of Mozilla AI and former co-founder/chief scientist at Arthur AI, argues that 2025 is finally the year AI evaluation becomes a top priority for enterprises. The convergence of three key factors makes this possible: (1) AI became accessible to C-suite executives with ChatGPT's launch in November 2022, (2) a budget freeze in late 2022 created focused spending on Gen AI pet projects, and (3) agentic systems are now taking autonomous actions that require rigorous evaluation. While evaluation companies have existed since the 2010s, they previously struggled to sell beyond CTOs/CIOs. Now, the entire C-suite (CEO, CFO, CISO, CTO, CIO) understands the need for quantitative AI evaluation, driving hockey stick growth for evaluation platforms as Gen AI applications move from science projects (2023) to production (2024) to scaling (2025).

---

## Main Topics

### [Introduction & Background](https://www.youtube.com/watch?v=CQGuvf6gSrM&t=17s)
**Timestamp:** 00:17 - 01:16

- John Dickerson introduces himself as Mozilla AI CEO, previously co-founder/chief scientist at Arthur AI for 6 years
- Mozilla AI's mission: provide open source AI tooling to enable the open source community to have a seat at the table with leaders like Sam Altman
- Arthur AI specializes in observability, evaluation, and security across traditional ML, deep learning, Gen AI, and agentic revolutions
- Core thesis: AI/ML monitoring and evaluation are two sides of the same sword—you can't monitor without measurement, which is evaluation's core functionality

**Key Points:**
- Mozilla AI operates in open source AI stack
- Arthur AI has been in eval space through multiple AI revolutions
- Monitoring requires measurement; measurement is evaluation

---

### [The Perfect Storm: Why Evaluation Wasn't a Priority Before](https://www.youtube.com/watch?v=CQGuvf6gSrM&t=135s)
**Timestamp:** 02:15 - 04:00

- Pre-ChatGPT: AI wasn't understood beyond CIO/CTO level, preventing C-suite buy-in
- November 2022: Budget freeze across US enterprises due to recession fears, except for specific pet projects
- November 30, 2022: ChatGPT launched, making AI understandable to CEOs, CFOs, and CISOs
- Budget unlocked specifically for Gen AI while everything else remained frozen
- Three converging factors: (1) C-suite AI awareness, (2) budget constraints forcing focus, (3) agentic systems requiring evaluation

**Key Points:**
- ChatGPT democratized AI understanding to non-technical executives
- Budget freeze created focused "pet project" spending on Gen AI
- ML models previously hidden in opaque boxes, not top-of-mind for decision makers
- Evaluation wasn't critical when ML was just input to larger systems

---

### [Pre-ChatGPT Era: Limited ML Investment](https://www.youtube.com/watch?v=CQGuvf6gSrM&t=419s)
**Timestamp:** 06:59 - 07:58

- Example: Jamie Dimon's JPMC annual report (April 2022, covering fiscal 2021) showed only $100M total AI/ML spend from 2017-2021
- This was "comically small" for JPMC's scale
- ML monitoring existed since ~2012 (H2O, Algorithmia, Seldon), but with tenuous connection to business KPIs
- Evaluation companies faced "lip service" from C-suite about ROI but couldn't sell beyond CIO
- Every pitch deck claimed "this is the year a CEO gets fired for ML screw-up"—never happened

**Key Points:**
- Even major enterprises had minimal pre-ChatGPT AI investment
- ML monitoring companies existed but struggled with C-suite relevance
- Security and latency concerns overshadowed ML model quality

---

### [The Macroeconomic Context (2022-2023)](https://www.youtube.com/watch?v=CQGuvf6gSrM&t=486s)
**Timestamp:** 08:06 - 09:41

- Mid-late 2022: Deep recession fears led enterprises to freeze/shrink IT budgets for 2023
- Enterprise budgets typically set in October-November; ChatGPT launched November 30
- "Eye of Sauron" focused on small pet projects with discretionary budget
- CEOs/CFOs could now interact with AI directly via ChatGPT's simple UI
- Holiday period: Executives experimented with ChatGPT (Eminem rapping like Taylor Swift, etc.)
- 2023: Austerity continued, but all available budget went specifically to Gen AI projects

**Key Points:**
- Perfect timing: budget freeze + ChatGPT launch = focused Gen AI spending
- Simple UI accessibility wowed non-technical executives
- Science projects started floating around enterprises in 2023

---

### [2024: Production Deployment](https://www.youtube.com/watch?v=CQGuvf6gSrM&t=581s)
**Timestamp:** 09:41 - 10:57

- 2023 science projects went into production in 2024
- Common applications: internal chat apps, hiring tools
- C-suite started asking about ROI, governance, risk, compliance, brand optics
- Evaluation becoming relevant beyond ML/data science/CIO teams
- Need for quantitative risk estimates drives evaluation adoption
- 2024 IT budgets (set in 2023) earmarked specifically for AI applications

**Key Points:**
- "People in business suits" now asking about AI metrics
- Evaluation needed for quantitative risk assessment
- Budget allocation shifted from general IT to AI-specific

---

### [2025: Scale & The Year of the Agent](https://www.youtube.com/watch?v=CQGuvf6gSrM&t=657s)
**Timestamp:** 10:57 - 12:08

- Revenue scaleups across frontier model providers and evaluation companies
- C-suite comfortable allocating "large real budget" to AI
- Science projects (2023) → Production (2024) → Shipping/Scaling (2025)
- Technology improvements: models continuously improving, community support, massive VC/big tech investment
- Third vertex: ML systems moving toward full autonomy
- Definition of agents: perceive environment, learn, abstract/generalize, reason and act
- Complexity and risk in agentic systems = opportunity for evaluation companies

**Key Points:**
- 2025 confirmed as "year of the agent" (no question mark needed)
- Agents introduce complexity and risk that require evaluation
- Technology maturity + budget availability + autonomous systems = perfect conditions

---

### [Connecting to Business KPIs](https://www.youtube.com/watch?v=CQGuvf6gSrM&t=728s)
**Timestamp:** 12:08 - 14:45

- Enterprise sales require connection to downstream business KPIs: risk mitigation, revenue gains, cost savings
- Evaluations enable quantification, making them first-class discussion points
- **CEO:** Knows capabilities of generative models and agentic systems, comfortable allocating budget and discussing with board/shareholders
- **CFO:** Cares about bottom-line impact, budget planning requires quantitative evaluation numbers
- **CISO:** Sees AI as security risk/opportunity; willing to write smaller checks quickly for startups; focused on hallucination detection, prompt injection, guardrails
- **CTO/CIO:** Need standards and quantitative decision-making data

**Key Points:**
- All C-suite roles now aligned on need for evaluation
- CISOs particularly active buyers (faster procurement, lower overhead)
- Guardrail products successfully selling into CISO offices
- Evaluation connects AI performance to dollars saved/earned

---

### [The Evaluation Industry Landscape](https://www.youtube.com/watch?v=CQGuvf6gSrM&t=885s)
**Timestamp:** 14:45 - 15:46

- All evaluation/observability/monitoring companies shifting to multi-agent systems monitoring
- Industry consensus: monitor the whole system, not just one model per agent
- April 2025 article in The Information leaked revenue numbers (6-8 months old) for evaluation startups
- Dickerson claims current revenues significantly higher than leaked numbers
- Prediction: Early 2026 articles will show "revenue no longer lags at AI evaluation startups"
- Mozilla AI's contribution: "AnyAgent" - open source, non-monetized light LLM for multi-agent systems with unified interface

**Key Points:**
- Whole-system monitoring becoming standard
- Revenue growth accelerating for evaluation companies
- Open source tools available (Mozilla's AnyAgent)

---

### [Q&A: Domain Expertise in Evaluation](https://www.youtube.com/watch?v=CQGuvf6gSrM&t=978s)
**Timestamp:** 16:18 - 18:12

**Question:** How do you evaluate domain-specific tasks (e.g., financial DCF analysis) with unstructured data?

**Answer:**
- Expert validation becoming critical: companies hiring domain experts ($50-200/hour) via platforms like MTurk
- Large banks hiring experts to sit alongside multi-agent systems for validation
- For high-stakes work (DCF analysis), expensive human validation is worthwhile
- Future question: What happens in 5 years when this expert data is incorporated into systems?
- Dataset and environment creation more important than evaluation methodology
- Competitive moat comes from investing capex in high-quality domain-specific evaluation environments

**Key Points:**
- Human-in-the-loop validation using domain experts
- Data quality and environment creation = competitive advantage
- Worth the cost for high-stakes applications

---

### [Q&A: LLM-as-Judge Timeline](https://www.youtube.com/watch?v=CQGuvf6gSrM&t=1095s)
**Timestamp:** 18:13 - 19:06

**Question:** Timeline for LLM-driven evaluation becoming primary method?

**Answer:**
- LLM-as-judge paradigm already used in practice despite known issues
- Mozilla AI paper at ICLR last month documented biases in LLM judges vs. humans (conciseness, helpfulness)
- Solves dataset creation problem: personas given to LLMs act as "poor man's human judges"
- Many people using it as a product now
- Critical caveat: Must validate to avoid biased directions
- Not perfect, but pragmatic solution to human validation costs

**Key Points:**
- Already in widespread use
- Known biases require validation
- Practical compromise between cost and quality
- Validation remains essential

---

## Closing

Dickerson concludes with optimism about 2025 being the breakthrough year for AI evaluation, driven by the alignment of technical capabilities, enterprise budget allocation, and C-suite understanding of autonomous AI systems' risks and opportunities.
