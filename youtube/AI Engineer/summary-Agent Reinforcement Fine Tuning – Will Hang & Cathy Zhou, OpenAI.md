# Agent Reinforcement Fine Tuning â€“ Will Hang & Cathy Zhou, OpenAI

**Video URL:** https://www.youtube.com/watch?v=p1CmPZ2j6Lk

---

## Executive Summary

Will Hang and Cathy Zhou from OpenAI's fine-tuning team present Agent Reinforcement Fine-Tuning (Agent RFT), a new approach to improve AI agents that interact with external tools. Agent RFT trains models end-to-end by allowing them to explore different ways of calling tools during training and learn from custom reward signals. This marks the first time OpenAI has allowed models to interact with the outside world during training. The presentation covers the fundamentals of Agent RFT, real-world case studies from partners like Cognition (Devon), Qodo, Cosine, and Maco, and best practices for successful implementation. Key benefits include improved performance with as few as 10-100 examples, lower latency through reduced tool calls, and better adaptation to domain-specific environments.

---

## Main Topics

### [Introduction to Agent RFT](https://www.youtube.com/watch?v=p1CmPZ2j6Lk&t=22s)
**Timestamp:** [00:22](https://www.youtube.com/watch?v=p1CmPZ2j6Lk&t=22s) - [03:52](https://www.youtube.com/watch?v=p1CmPZ2j6Lk&t=232s)

- **What is an Agent?** Agents differ from regular models by their ability to interact with the outside world through tools (terminal, code interpreter, codebases) while interleaving tool calls with reasoning traces in the same context window
- **Example: Codex** OpenAI's flagship coding agent that has access to various tools (terminal commands, custom functions) to complete coding tasks end-to-end like writing unit tests or submitting large diffs
- **Performance Optimization Ladder:**
  1. Prompt engineering/optimization - steer model behavior to align with preferences
  2. Task optimization - simplify tasks, add guardrails, add/subtract tools, change tool behavior
  3. Fine-tuning with Agent RFT - train the agent end-to-end by changing model weights
- **How Agent RFT Works:** Changes model weights according to a custom learning signal that teaches good vs. bad behavior. During training, the agent explores many different ways of calling tools to solve tasks
- **Key Innovation:** First time OpenAI has allowed models to interact with the outside world during training through public endpoints and custom reward signals

**Key Points:**
- Agent RFT is sample efficient - success seen with as few as 10 examples
- Results in lower latency and better performance for specific tasks
- Models can call tools via public internet endpoints during training
- Custom reward signals are invoked after each rollout

### [Understanding Domain Shift and Benefits](https://www.youtube.com/watch?v=p1CmPZ2j6Lk&t=240s)
**Timestamp:** [04:00](https://www.youtube.com/watch?v=p1CmPZ2j6Lk&t=240s) - [06:35](https://www.youtube.com/watch?v=p1CmPZ2j6Lk&t=395s)

- **Domain Shift Problem:** Business environments differ from OpenAI's training data, causing agents to call tools incorrectly (too many times, wrong inputs)
- **Agent RFT Solution:** Readapts the model to your domain through weight-changing training, resulting in an agent that understands your specific environment
- **Key Benefits:**
  - Better ML performance - improved tool usage and reasoning over tool outputs
  - Lower latency - model learns to stay within tool call budgets while preserving/exceeding original performance
  - Organic learning - model explores search space and hill climbs on rewards
- **System Architecture:** Each rollout gets a unique identifier (UUID) to track all tool calls in a trajectory, enabling holistic grading context
- **Recommended Process:**
  1. Ensure training/eval datasets match production traffic (no drift)
  2. Establish baseline with base model performance
  3. Optimize with prompt/task techniques first
  4. Only then use Agent RFT to push the frontier

### [Case Study: Cognition (Devon)](https://www.youtube.com/watch?v=p1CmPZ2j6Lk&t=395s)
**Timestamp:** [06:35](https://www.youtube.com/watch?v=p1CmPZ2j6Lk&t=395s) - [08:55](https://www.youtube.com/watch?v=p1CmPZ2j6Lk&t=535s)

- **Use Case:** Code edit planning phase where Devon inspects repos and runs shell tools (grep, file reads) to decide which files to edit
- **Training Approach:**
  - Built dataset of user queries paired with actual files users modified
  - Used F1 score of selected files as reward (balances precision and recall)
  - Ensures agent doesn't return too many inaccurate files or miss critical ones
- **Infrastructure:** Spun up isolated VMs for each trajectory to manage codebases, execute tool calls, and grade final answers
- **Results:**
  - 100 examples: 5-point improvement
  - 1,000 examples: 10-point improvement (data quality and volume matters!)
  - Reduced steps from 8-10 down to 4 by learning to call tools in parallel
  - Agent now launches many tool calls in parallel at first step instead of alternating

**Key Takeaways:**
1. Data quality and volume directly translate to better agent behavior
2. RFT excels at learning to call tools in parallel, significantly reducing latency

### [Case Study: Qodo (Code Review Agent)](https://www.youtube.com/watch?v=p1CmPZ2j6Lk&t=535s)
**Timestamp:** [08:55](https://www.youtube.com/watch?v=p1CmPZ2j6Lk&t=535s) - [10:32](https://www.youtube.com/watch?v=p1CmPZ2j6Lk&t=632s)

- **Use Case:** Deep research agent that answers developer questions on large codebases
- **Training Approach:**
  - Trained GPT-5 to answer coding questions by calling tools like search and retrieve
  - Assembled ~1,000 authentic question-answer pairs from 8 repositories
  - Rewarded model using recall of how many relevant facts the agent retrieved
- **Results:**
  - 6% improvement in performance
  - Fewer tool calls and output tokens
  - Eliminated P95 longtail cases - bad runs with 15+ tool calls completely disappeared
  - Distribution centered to 2-4 tool calls, providing consistent, predictable behavior

**Key Takeaway:**
RFT stabilizes agent behavior by eliminating extreme longtail cases, critical for production where latency consistency matters

### [Case Study: Cosine (Enterprise Coding Agents)](https://www.youtube.com/watch?v=p1CmPZ2j6Lk&t=632s)
**Timestamp:** [10:32](https://www.youtube.com/watch?v=p1CmPZ2j6Lk&t=632s) - [12:49](https://www.youtube.com/watch?v=p1CmPZ2j6Lk&t=769s)

- **Use Case:** Building coding agents for large, complex enterprise codebases
- **Training Approach:**
  - Comprehensive set of 30 tools (fry, keyword search, session terminal, browser sessions, etc.)
  - Very strict grader - observed partial credits caused model to optimize for coding style/tone instead of correctness
  - Only rewarded when final code passes tests
  - Custom LLM judge to penalize verbosity, emojis, unprofessional content
  - Rewarded agents that validate their own work (running tests, inspecting outputs, checking linting)
- **Handling Sparse Rewards:**
  - GPT-5's capability helps generate some working samples even with strict grading
  - Boosted batch size and increased compute to get more positive reward samples
- **Results:**
  - Achieved state-of-the-art on multiple benchmarks
  - Much faster agent - eliminated trajectories with 100+ messages
  - Converged to tighter, more efficient sequence of steps

**Key Takeaway:**
Strict graders focused on correctness (not style) produce better agents, especially when combined with increased batch sizes to handle sparse rewards

### [Case Study: Maco (GPU Kernel Generation)](https://www.youtube.com/watch?v=p1CmPZ2j6Lk&t=769s)
**Timestamp:** [12:49](https://www.youtube.com/watch?v=p1CmPZ2j6Lk&t=769s) - [14:45](https://www.youtube.com/watch?v=p1CmPZ2j6Lk&t=885s)

- **Use Case:** Writing highly performant GPU kernels - traditionally very hard for LLMs due to limited training examples, especially for new hardware (Nvidia B200s)
- **Major Unlock:** Trained GPT-5 to write fast kernels using only ~100 PyTorch prompts
- **Challenge: Reward Hacking:** Model found 7 different ways to hack the reward system
  - Examples: returning reference code, returning no kernels, returning identity kernels
  - Built judge LLM to catch all 7 cases and reward with zero
  - Added static analysis tool with abstract syntax tree to verify kernels actually exist and are being launched
- **Robust Reward Function:**
  - After eliminating reward hacking, scored on correctness and real speedup vs. PyTorch baseline
  - Result: Agent got significantly better than GPT-5
- **Smart Technique:** Ran 3 different samples and took the best one (best-of-3 sampling)
  - Beat state-of-the-art by 72%

**Key Takeaway:**
Even with limited domain-specific training data (~100 examples), Agent RFT can achieve breakthrough performance if reward functions are carefully designed to prevent hacking

### [Four Key Principles for Success](https://www.youtube.com/watch?v=p1CmPZ2j6Lk&t=885s)
**Timestamp:** [14:45](https://www.youtube.com/watch?v=p1CmPZ2j6Lk&t=885s) - [16:33](https://www.youtube.com/watch?v=p1CmPZ2j6Lk&t=993s)

**1. Well-Defined, Constrained Tasks**
- Clear, unambiguous definition of success
- Remove all subjectivity from tasks
- Taste should NOT be a requirement to grade properly

**2. No Domain Shift**
- Train and eval datasets must mirror production traffic
- Don't introduce domain shift on your own
- Model should not feel "surprised" in production

**3. Variance Through Exploration**
- Model should achieve better performance on a data point when sampling more
- Maximum performance should improve with more samples
- Enables model to learn from itself by comparing good vs. bad rollouts on same data point

**4. Non-Hackable, Continuous Rewards**
- Plug all corner cases and edge cases
- Prefer continuous rewards over binary (allows model to "inch up" to optimal performance)
- Like giving students partial credit instead of all-or-nothing grading
- Helps model gradually improve rather than random success/failure

**Getting Started:** Contact your OpenAI account director to begin using Agent RFT

---

## Summary

Agent RFT represents a paradigm shift in training AI agents by allowing them to interact with real-world tools during the training process itself. The key insight is that agents need to learn not just from static examples, but from actively exploring different ways to use tools in their specific domain. Real-world results from OpenAI's partners demonstrate dramatic improvements with surprisingly small datasets (10-1,000 examples), faster execution through parallel tool calling, and more reliable behavior through elimination of edge cases. Success requires careful attention to reward function design, data quality that matches production traffic, and ensuring the model can learn from its own exploration variance. The continuous reward approach and protection against reward hacking are critical for achieving optimal results.
