# Efficient Reinforcement Learning – Rhythm Garg & Linden Li, Applied Compute

**Video URL:** https://www.youtube.com/watch?v=o15AaYl7Wu0

---

## Executive Summary

Rhythm Garg and Linden Li from Applied Compute (former OpenAI researchers) present their approach to efficient reinforcement learning for enterprise AI systems. They explain how they optimize RL training to be fast, cost-effective, and reliable by solving the GPU utilization problem through asynchronous pipeline RL. The talk covers the technical challenges of synchronous vs asynchronous RL, the staleness-throughput tradeoff, and their systems modeling approach to optimize GPU allocation between sampling and training workers.

---

## Main Topics

### [Introduction & Applied Compute's Mission](https://www.youtube.com/watch?v=o15AaYl7Wu0&t=22s)
**Timestamp:** 00:22 - 03:00

- Team background: Three co-founders (Rhythm, Linden, Yash) previously worked on RL at OpenAI
- Applied Compute helps enterprises build specialized AI intelligence for real work automation with quantifiable ROI
- Focus on data flywheels: systems that improve over time through continuous use
- RL is the key mechanism for bringing out-of-distribution data in-distribution for models

**Key Points:**
- Mission: Push AI beyond productivity into real automation with measurable business value
- Approach: Build specialized systems for specific enterprise use cases, deploy with data flywheel
- Different from labs: Specialized runs in days vs weeks-long training runs

### [How RL Works: High-Level Overview](https://www.youtube.com/watch?v=o15AaYl7Wu0&t=107s)
**Timestamp:** 01:47 - 03:00

- Example: Training on math problems
- Process: Take 4 problems, have model attempt each 100 times
- Each attempt is model's reasoning trajectory toward final answer
- Grade all answers: reinforce correct thinking traces, discourage incorrect ones
- Result: Model learns to reason and become excellent at the task

**Key Points:**
- Mechanism generalizes beyond math to any enterprise task
- Many reasoning tokens in thinking trajectory
- Iterative reinforcement of successful behaviors

### [RL Requirements for Enterprise vs Labs](https://www.youtube.com/watch?v=o15AaYl7Wu0&t=180s)
**Timestamp:** 03:00 - 04:00

- **Fast**: Train and deliver models to customers in days, not weeks
- **Cheap**: Unit costs must work for sustainable business scaling
- **Low variance**: Reliably fast with predictable timelines for customer commitments
- Core research problem: Build efficient RL stack that enables use-case-specific training at scale

**Key Points:**
- Business-critical to have predictable, reliable performance
- Different optimization goals than academic labs
- Efficiency directly impacts scalability

### [The Problem with Synchronous RL](https://www.youtube.com/watch?v=o15AaYl7Wu0&t=248s)
**Timestamp:** 04:08 - 05:30

- Synchronous RL: Sampling and training happen in lockstep
- Must wait for all samples in batch to finish before training
- Step times dictated by slowest (straggler) sample
- Massive GPU idle time waiting for stragglers

**Empirical Evidence:**
- Test: 40 arithmetic problems, 32 samples each with Qwen 30B
- Result: 99% of samples completed in ~40 seconds, last 1% took another 80 seconds
- Long tail distribution causes severe GPU underutilization
- Technical term at Applied Compute: "GPUs are slacking"

**Key Points:**
- Synchronous approach is fundamentally inefficient
- Throughput drops dramatically toward end of sampling phase
- Need to break lockstep condition between sampling and training

### [Asynchronous Pipeline RL Solution](https://www.youtube.com/watch?v=o15AaYl7Wu0&t=327s)
**Timestamp:** 05:27 - 07:00

- Key innovation: Allow training while sampling (break lockstep)
- Dedicate some GPUs to sampling, others to training
- Sampling workers constantly do inference at high batch size
- Completed samples added to queue, training workers pull batches from queue
- **In-flight weight updates**: Training workers propagate new model weights to sampling workers mid-sample

**How In-Flight Updates Work:**
- Sampling worker may be mid-generation when training step completes
- Weights get updated immediately, even during sample generation
- Result: Single sample may use multiple policy versions

**Key Points:**
- Continuous GPU utilization on both sampling and training
- Pipeline approach from P et al.
- Enables much higher throughput

### [The Staleness Problem](https://www.youtube.com/watch?v=o15AaYl7Wu0&t=409s)
**Timestamp:** 06:49 - 08:40

**What is Staleness:**
- Sample generated by policy at time t, trained on by policy at t+k
- "Staleness" = k (number of training steps behind)
- In-flight updates create samples with mixed policy versions
- Some tokens from policy 3 steps behind, others 2 steps behind, etc.

**The Tradeoff:**
- More staleness tolerance = fewer idle GPUs = faster runs
- But: High staleness increases variance in importance ratio
- High variance importance ratio → unstable learning → potential divergence
- No free lunch: Speed vs stability

**Technical Details:**
- Standard policy gradient with importance ratio for off-policy correction
- Importance ratio keeps gradient unbiased but variance grows with staleness
- Must innovate on algorithm/science to handle high staleness

**Key Points:**
- Core research challenge at Applied Compute
- Business-critical problem: speed vs learning quality
- Directly impacts customer delivery timelines

### [Systems Modeling Approach](https://www.youtube.com/watch?v=o15AaYl7Wu0&t=540s)
**Timestamp:** 09:00 - 11:00

**Problem Framing:**
- Given: Good algorithmic innovations to tolerate staleness up to threshold
- Given: Fixed compute budget
- Question: What's the fastest way to do RL in this setting?

**Cast of Characters (Model Parameters):**

1. **Compute Budget**: Number of GPUs
   - Synchronous: All GPUs for training OR sampling
   - Asynchronous: Choose allocation split between training and sampling

2. **Training Batch Size**: Proxy of workload
   - N problems, M samples per problem in parallel
   - More samples for difficult problems → more diversity

3. **Sampling Throughput**: Latency per GPU of forward pass
   - Determined by batch size used during sampling
   - Batch size must be maximized (subject to KV cache memory limits)
   - Need to fit latency curve as function of batch size

4. **Training Throughput**: Per GPU basis
   - Input: Training batch size
   - Output: Tokens per second each training GPU processes
   - Includes forward, backward, optimizer steps

**Key Points:**
- First principles systems modeling gets surprisingly far
- Need empirical measurements to fit curves
- All parameters interconnected

### [Sampling Throughput Deep Dive](https://www.youtube.com/watch?v=o15AaYl7Wu0&t=618s)
**Timestamp:** 10:18 - 12:30

**Inference Engine Mechanics:**
- GPU memory contains: model weights, activations, KV cache
- Forward pass samples next token, writes to KV cache
- Batch size in sampling must be as large as possible for efficiency
- Constraint: Don't run out of KV cache memory

**Latency Curve:**
- Fit latency as function of batch size
- Two regimes:
  - **Memory bound** (low batch): Waiting on parameter loading, incremental work adds little latency
  - **Compute bound** (high batch): Bottlenecked by processor, more batch = more latency
- Smooth transition modeled with sigmoid function
- Based on roofline model from systems theory

**Key Points:**
- Critical to understand memory vs compute bottlenecks
- Optimal batch size balances utilization and latency
- Empirical fitting required for accurate modeling

### [Synchronous Setup Modeling](https://www.youtube.com/watch?v=o15AaYl7Wu0&t=780s)
**Timestamp:** 13:00 - 15:00

**Why Consider Synchronous:**
- Meets staleness constraint (no stale data)
- Uses entire GPU fleet efficiently (all for training OR sampling)

**How to Model:**
- Need: Generation batch size
- Need: Response length distribution
- Simulation shows batch size starting high, declining over time
- As samples complete, batch size drops to zero
- Then run optimization step and repeat

**Sampling Procedure:**
- Do max_tokens forward passes (longest request determines)
- Use fitted latency estimator for forward pass duration
- Response length distribution tells how many responses to drop

**Training Time:**
- Total tokens in batch / (num_GPUs × per_GPU_training_throughput)

**Key Points:**
- Simple but inefficient due to declining batch size
- Significant GPU slack during straggler wait
- Sets baseline for comparison

### [Asynchronous Setup Modeling](https://www.youtube.com/watch?v=o15AaYl7Wu0&t=903s)
**Timestamp:** 15:03 - 17:30

**Key Difference:**
- Batch size relatively consistent in steady state (not declining)
- Samples continuously added to queue as they complete
- New samples launched immediately to maintain batch size

**Critical Allocation Problem:**
- Too many sampling GPUs, not enough training → queue builds up, staleness exceeds threshold
- Too many training GPUs, not enough sampling → training workers idle waiting for samples
- Must find optimal balance

**Two Extremes Illustrated:**
1. **Too many training GPUs**: Training faster than sampling produces work → training GPUs idle
2. **Too many sampling GPUs**: Producing faster than consuming → staleness increases

**Key Points:**
- Requires careful GPU allocation between sampling/training
- Steady state assumption valid with caveats (KV cache limits)
- Response length distribution still matters

### [Optimal Async Layout Constraints](https://www.youtube.com/watch?v=o15AaYl7Wu0&t=1037s)
**Timestamp:** 17:17 - 19:00

**Constraint 1: Production-Consumption Balance**
```
training_throughput = sampling_throughput
num_train_GPUs × per_GPU_train_throughput =
    num_sample_GPUs × (batch_size / latency_per_forward)
```

**Constraint 2: Staleness Limit**
```
max_staleness ≤ threshold
max_staleness = (max_tokens × latency_per_token) / training_step_duration
training_step_duration = (train_batch_size × mean_seq_length) / train_throughput
```

**Optimization Procedure:**
1. Sweep different values of num_training_GPUs
2. Implied num_sampling_GPUs from fixed compute budget
3. Compute minimum steady-state generation batch size (KV cache constraint + max throughput)
4. Prune simulations where sampling throughput exceeds max staleness

**Results:**
- ~60% speedup vs synchronous baseline
- Assumes optimal GPU allocation between training and sampling
- Can simulate different workloads before expensive GPU runs

**Key Points:**
- Mathematical optimization within constraints
- Simulation validates speedup before real runs
- Answers "what-if" questions about configuration

### [Practical Applications & Value](https://www.youtube.com/watch?v=o15AaYl7Wu0&t=1150s)
**Timestamp:** 19:10 - 20:00

**Scientific Questions Answered:**
- What's optimal GPU configuration for very long response lengths?
- What empirical throughputs to target during performance optimization?
- How does optimal allocation shift as models learn to think longer?

**Business Impact:**
- Simulations inform systems and research design decisions
- Avoid expensive trial-and-error on GPU clusters
- Predictable performance enables customer commitments
- Cost efficiency enables sustainable scaling

**Key Points:**
- First principles modeling has real business value
- Simulation is powerful tool for research engineering
- RL engineering challenges directly tied to product delivery

---

**Last Updated:** 2026-01-01
