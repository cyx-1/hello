# #define AI Engineer - Greg Brockman, OpenAI (ft. Jensen Huang)

**Video URL:** https://www.youtube.com/watch?v=avWhreBUYF0

---

## Full Transcript

### [00:00 - 01:00]

**[00:17]** [Applause]

**[00:17]** [Applause] [Music]

**[00:29]** Well, hello. Hello. Is uh mic working

**[00:29]** Well, hello. Hello. Is uh mic working for you? Check. Check. Check. One, two,

**[00:31]** for you? Check. Check. Check. One, two,

**[00:31]** for you? Check. Check. Check. One, two, three. All right. First hard technology

**[00:33]** three. All right. First hard technology

**[00:33]** three. All right. First hard technology problem of the day down. Yeah. Yeah.

**[00:35]** problem of the day down. Yeah. Yeah.

**[00:35]** problem of the day down. Yeah. Yeah. Well, the Wi-Fi is the other one. Um,

**[00:38]** Well, the Wi-Fi is the other one. Um,

**[00:38]** Well, the Wi-Fi is the other one. Um, everyone here knows. Um, so Greg,

**[00:40]** everyone here knows. Um, so Greg,

**[00:40]** everyone here knows. Um, so Greg, welcome to AI Engineer. Thank you so

**[00:42]** welcome to AI Engineer. Thank you so

**[00:42]** welcome to AI Engineer. Thank you so much for taking the time. Thank you for

**[00:43]** much for taking the time. Thank you for

**[00:43]** much for taking the time. Thank you for having me. Um, we're going to go a

**[00:45]** having me. Um, we're going to go a

**[00:46]** having me. Um, we're going to go a little bit chronologically and, uh, a

**[00:48]** little bit chronologically and, uh, a

**[00:48]** little bit chronologically and, uh, a lot of people send in questions and I've

**[00:50]** lot of people send in questions and I've

**[00:50]** lot of people send in questions and I've sort of grouped them up for you. So,

**[00:52]** sort of grouped them up for you. So,

**[00:52]** sort of grouped them up for you. So, we're just get right into it. U, so, you

**[00:54]** we're just get right into it. U, so, you

**[00:54]** we're just get right into it. U, so, you know, you you know, I did some deep

**[00:55]** know, you you know, I did some deep

**[00:55]** know, you you know, I did some deep research on you. uh you started with

**[00:57]** research on you. uh you started with

**[00:57]** research on you. uh you started with deep research with with deep research.

**[00:59]** deep research with with deep research.

**[00:59]** deep research with with deep research. Um I called it Peep research because you


### [01:00 - 02:00]

**[01:01]** Um I called it Peep research because you

**[01:01]** Um I called it Peep research because you were researching a person. Uh you

**[01:03]** were researching a person. Uh you

**[01:03]** were researching a person. Uh you actually did theater growing up and

**[01:05]** actually did theater growing up and

**[01:05]** actually did theater growing up and chemistry and math and you wrote a

**[01:07]** chemistry and math and you wrote a

**[01:07]** chemistry and math and you wrote a calendar scheduling app and that's what

**[01:09]** calendar scheduling app and that's what

**[01:09]** calendar scheduling app and that's what got you into coding. But like what

**[01:11]** got you into coding. But like what

**[01:11]** got you into coding. But like what really inspired your love for coding?

**[01:13]** really inspired your love for coding?

**[01:13]** really inspired your love for coding? Like why why are you the coding guy?

**[01:15]** Like why why are you the coding guy?

**[01:15]** Like why why are you the coding guy? Well the funny thing is I thought I was

**[01:17]** Well the funny thing is I thought I was

**[01:17]** Well the funny thing is I thought I was going to be a mathematician when I grew

**[01:18]** going to be a mathematician when I grew

**[01:18]** going to be a mathematician when I grew up. Yeah. You know I'd read about people

**[01:20]** up. Yeah. You know I'd read about people

**[01:20]** up. Yeah. You know I'd read about people like Gawwa and Gaus. you know, we work

**[01:22]** like Gawwa and Gaus. you know, we work

**[01:22]** like Gawwa and Gaus. you know, we work working on these like hundred, 200, 300

**[01:25]** working on these like hundred, 200, 300

**[01:25]** working on these like hundred, 200, 300 year time horizons. And I was like,

**[01:27]** year time horizons. And I was like,

**[01:27]** year time horizons. And I was like, that's what I want to do. If anything

**[01:28]** that's what I want to do. If anything

**[01:28]** that's what I want to do. If anything that I come up with is ever used while

**[01:30]** that I come up with is ever used while

**[01:30]** that I come up with is ever used while I'm still alive, it wasn't long-term

**[01:32]** I'm still alive, it wasn't long-term

**[01:32]** I'm still alive, it wasn't long-term enough. It wasn't abstract enough. Um,

**[01:35]** enough. It wasn't abstract enough. Um,

**[01:35]** enough. It wasn't abstract enough. Um, and I was writing this chemistry

**[01:36]** and I was writing this chemistry

**[01:36]** and I was writing this chemistry textbook after high school, sent it to

**[01:38]** textbook after high school, sent it to

**[01:38]** textbook after high school, sent it to one of my friends who' done something

**[01:39]** one of my friends who' done something

**[01:39]** one of my friends who' done something similar in math, and he said, "No one is

**[01:41]** similar in math, and he said, "No one is

**[01:41]** similar in math, and he said, "No one is going to publish this. You can either

**[01:42]** going to publish this. You can either

**[01:42]** going to publish this. You can either self-publish." I was like, "Ah, sounds

**[01:44]** self-publish." I was like, "Ah, sounds

**[01:44]** self-publish." I was like, "Ah, sounds like a lot of work, a lot of capital, or

**[01:46]** like a lot of work, a lot of capital, or

**[01:46]** like a lot of work, a lot of capital, or you could make a website." Mhm. And I

**[01:49]** you could make a website." Mhm. And I

**[01:49]** you could make a website." Mhm. And I was like, "Guess I'm going to learn how

**[01:50]** was like, "Guess I'm going to learn how

**[01:50]** was like, "Guess I'm going to learn how to make a website." And so I literally

**[01:52]** to make a website." And so I literally

**[01:52]** to make a website." And so I literally went on W3 Schools and did their PHP

**[01:55]** went on W3 Schools and did their PHP

**[01:55]** went on W3 Schools and did their PHP tutorial. How many people here remember

**[01:56]** tutorial. How many people here remember

**[01:56]** tutorial. How many people here remember W3 Schools? Yeah. Decent number of

**[01:59]** W3 Schools? Yeah. Decent number of

**[01:59]** W3 Schools? Yeah. Decent number of hands. Um, and I remember the very first


### [02:00 - 03:00]

**[02:02]** hands. Um, and I remember the very first

**[02:02]** hands. Um, and I remember the very first thing I built was a table sorting

**[02:03]** thing I built was a table sorting

**[02:04]** thing I built was a table sorting widget, right? I had this picture in my

**[02:05]** widget, right? I had this picture in my

**[02:05]** widget, right? I had this picture in my head of what it would be. And I remember

**[02:07]** head of what it would be. And I remember

**[02:07]** head of what it would be. And I remember the moment that I clicked the column and

**[02:10]** the moment that I clicked the column and

**[02:10]** the moment that I clicked the column and it sorted according to that column,

**[02:11]** it sorted according to that column,

**[02:11]** it sorted according to that column, which was exactly the thing that I

**[02:12]** which was exactly the thing that I

**[02:12]** which was exactly the thing that I wanted. And I was like, that was magic,

**[02:14]** wanted. And I was like, that was magic,

**[02:14]** wanted. And I was like, that was magic, right? And I was like, this is so cool.

**[02:16]** right? And I was like, this is so cool.

**[02:16]** right? And I was like, this is so cool. Because the thing about math is that you

**[02:18]** Because the thing about math is that you

**[02:18]** Because the thing about math is that you think hard about a problem, you

**[02:20]** think hard about a problem, you

**[02:20]** think hard about a problem, you understand it, you write it down in an

**[02:21]** understand it, you write it down in an

**[02:21]** understand it, you write it down in an obscure way, you call it proof. And then

**[02:23]** obscure way, you call it proof. And then

**[02:23]** obscure way, you call it proof. And then like three people will ever care, right?

**[02:27]** like three people will ever care, right?

**[02:27]** like three people will ever care, right? But

**[02:29]** But

**[02:29]** But in programming, you write it down in an

**[02:30]** in programming, you write it down in an

**[02:30]** in programming, you write it down in an obscure way, we call a program. And then

**[02:33]** obscure way, we call a program. And then

**[02:33]** obscure way, we call a program. And then maybe only three people ever read that

**[02:34]** maybe only three people ever read that

**[02:34]** maybe only three people ever read that program and care about the code. But

**[02:36]** program and care about the code. But

**[02:36]** program and care about the code. But everyone gets the benefit. No one has to

**[02:38]** everyone gets the benefit. No one has to

**[02:38]** everyone gets the benefit. No one has to understand the details. That thing that

**[02:39]** understand the details. That thing that

**[02:39]** understand the details. That thing that was in your head, it's real. It's in the

**[02:40]** was in your head, it's real. It's in the

**[02:40]** was in your head, it's real. It's in the world. And I was like, that that's the

**[02:42]** world. And I was like, that that's the

**[02:42]** world. And I was like, that that's the thing I want to do. Forget about that

**[02:43]** thing I want to do. Forget about that

**[02:43]** thing I want to do. Forget about that hundred-year time horizon. I just want

**[02:45]** hundred-year time horizon. I just want

**[02:45]** hundred-year time horizon. I just want to build

**[02:47]** to build

**[02:47]** to build Uh you do just want to build. Uh it's so

**[02:50]** Uh you do just want to build. Uh it's so

**[02:50]** Uh you do just want to build. Uh it's so you were so good at it that somehow

**[02:52]** you were so good at it that somehow

**[02:52]** you were so good at it that somehow somewhere you got cold emailed by Stripe

**[02:55]** somewhere you got cold emailed by Stripe

**[02:55]** somewhere you got cold emailed by Stripe while you're still in college. That's

**[02:56]** while you're still in college. That's

**[02:56]** while you're still in college. That's right. Uh what's the story? How first of

**[02:58]** right. Uh what's the story? How first of

**[02:58]** right. Uh what's the story? How first of all, how did they find you and what was

**[02:59]** all, how did they find you and what was

**[02:59]** all, how did they find you and what was it that convinced you to drop out to


### [03:00 - 04:00]

**[03:01]** it that convinced you to drop out to

**[03:01]** it that convinced you to drop out to join them? Well, so I had mutual friends

**[03:03]** join them? Well, so I had mutual friends

**[03:03]** join them? Well, so I had mutual friends with all the people at at Stripe, the

**[03:04]** with all the people at at Stripe, the

**[03:04]** with all the people at at Stripe, the you know giant company of like three

**[03:06]** you know giant company of like three

**[03:06]** you know giant company of like three people at the time. uh and uh uh they

**[03:10]** people at the time. uh and uh uh they

**[03:10]** people at the time. uh and uh uh they they had asked you know the usual thing

**[03:12]** they had asked you know the usual thing

**[03:12]** they had asked you know the usual thing where they'd asked someone at Harvard

**[03:13]** where they'd asked someone at Harvard

**[03:13]** where they'd asked someone at Harvard who the you know people around campus to

**[03:15]** who the you know people around campus to

**[03:15]** who the you know people around campus to talk to uh who they might recruit where

**[03:17]** talk to uh who they might recruit where

**[03:17]** talk to uh who they might recruit where my name came up they asked the same for

**[03:19]** my name came up they asked the same for

**[03:19]** my name came up they asked the same for the people at MIT because I actually had

**[03:21]** the people at MIT because I actually had

**[03:21]** the people at MIT because I actually had dropped I' I'd been at Harvard and

**[03:22]** dropped I' I'd been at Harvard and

**[03:22]** dropped I' I'd been at Harvard and actually dropped out to go to MIT so I I

**[03:25]** actually dropped out to go to MIT so I I

**[03:25]** actually dropped out to go to MIT so I I had the advantage of uh I guess you know

**[03:27]** had the advantage of uh I guess you know

**[03:27]** had the advantage of uh I guess you know uh get getting up votes on both sides.

**[03:29]** uh get getting up votes on both sides.

**[03:29]** uh get getting up votes on both sides. Um, but I remember when I met the

**[03:34]** Um, but I remember when I met the

**[03:34]** Um, but I remember when I met the Patrick and it was you I just flown in.

**[03:36]** Patrick and it was you I just flown in.

**[03:36]** Patrick and it was you I just flown in. It was like late at night that you know

**[03:38]** It was like late at night that you know

**[03:38]** It was like late at night that you know it was storming and uh I I showed up and

**[03:40]** it was storming and uh I I showed up and

**[03:40]** it was storming and uh I I showed up and we just started talking about code,

**[03:41]** we just started talking about code,

**[03:41]** we just started talking about code, right? And it was just like one of those

**[03:43]** right? And it was just like one of those

**[03:43]** right? And it was just like one of those moments where you're like this this is

**[03:44]** moments where you're like this this is

**[03:44]** moments where you're like this this is the kind of person that that I've wanted

**[03:46]** the kind of person that that I've wanted

**[03:46]** the kind of person that that I've wanted to work with and been looking for. Uh

**[03:47]** to work with and been looking for. Uh

**[03:47]** to work with and been looking for. Uh and so I ended up dropping out of MIT uh

**[03:50]** and so I ended up dropping out of MIT uh

**[03:50]** and so I ended up dropping out of MIT uh and uh you know flew out and been out

**[03:52]** and uh you know flew out and been out

**[03:52]** and uh you know flew out and been out here ever since. Yeah. Yeah. Uh we have

**[03:54]** here ever since. Yeah. Yeah. Uh we have

**[03:54]** here ever since. Yeah. Yeah. Uh we have a spe we have some guest questions

**[03:55]** a spe we have some guest questions

**[03:55]** a spe we have some guest questions sprinkled along the way as you know. Uh,

**[03:57]** sprinkled along the way as you know. Uh,

**[03:57]** sprinkled along the way as you know. Uh, so question from someone named Matthew

**[03:59]** so question from someone named Matthew


### [04:00 - 05:00]

**[04:00]** so question from someone named Matthew Brockman. I've heard of him. CTO of

**[04:02]** Brockman. I've heard of him. CTO of

**[04:02]** Brockman. I've heard of him. CTO of Julius AI. When do you think our parents

**[04:04]** Julius AI. When do you think our parents

**[04:04]** Julius AI. When do you think our parents will give up on the dream of you

**[04:05]** will give up on the dream of you

**[04:05]** will give up on the dream of you finishing your degree? Maybe

**[04:08]** finishing your degree? Maybe

**[04:08]** finishing your degree? Maybe maybe Harvard or UND will take you back.

**[04:10]** maybe Harvard or UND will take you back.

**[04:10]** maybe Harvard or UND will take you back. Yes. Uh, well, never. Um, it was

**[04:13]** Yes. Uh, well, never. Um, it was

**[04:13]** Yes. Uh, well, never. Um, it was definitely, you know, I think it was no

**[04:16]** definitely, you know, I think it was no

**[04:16]** definitely, you know, I think it was no matter where you're going, if you tell

**[04:17]** matter where you're going, if you tell

**[04:17]** matter where you're going, if you tell your parents you're leaving Harvard,

**[04:18]** your parents you're leaving Harvard,

**[04:18]** your parents you're leaving Harvard, it's going to be hard. Um, you tell your

**[04:20]** it's going to be hard. Um, you tell your

**[04:20]** it's going to be hard. Um, you tell your parents you're leaving school

**[04:21]** parents you're leaving school

**[04:21]** parents you're leaving school altogether, it's going to be difficult.

**[04:22]** altogether, it's going to be difficult.

**[04:22]** altogether, it's going to be difficult. Um and I think that you know it was

**[04:24]** Um and I think that you know it was

**[04:24]** Um and I think that you know it was actually um to to their credit you know

**[04:27]** actually um to to their credit you know

**[04:27]** actually um to to their credit you know I think even though it was difficult um

**[04:28]** I think even though it was difficult um

**[04:28]** I think even though it was difficult um that they were like that you know we

**[04:30]** that they were like that you know we

**[04:30]** that they were like that you know we trust you like you you must see

**[04:31]** trust you like you you must see

**[04:31]** trust you like you you must see something and and understand something

**[04:33]** something and and understand something

**[04:33]** something and and understand something from from where you sit that's hard for

**[04:34]** from from where you sit that's hard for

**[04:34]** from from where you sit that's hard for us to see from from halfway across the

**[04:36]** us to see from from halfway across the

**[04:36]** us to see from from halfway across the country. Um but yeah I think that that

**[04:38]** country. Um but yeah I think that that

**[04:38]** country. Um but yeah I think that that as you know did Stripe and uh and had a

**[04:43]** as you know did Stripe and uh and had a

**[04:43]** as you know did Stripe and uh and had a good time and and actually learned

**[04:44]** good time and and actually learned

**[04:44]** good time and and actually learned things um and uh turned out as a real

**[04:46]** things um and uh turned out as a real

**[04:46]** things um and uh turned out as a real company and not just uh uh you know just

**[04:48]** company and not just uh uh you know just

**[04:48]** company and not just uh uh you know just dropping out doing nothing. I I think

**[04:50]** dropping out doing nothing. I I think

**[04:50]** dropping out doing nothing. I I think that that they they really were were uh

**[04:52]** that that they they really were were uh

**[04:52]** that that they they really were were uh you know have have warmed up to it and

**[04:53]** you know have have warmed up to it and

**[04:53]** you know have have warmed up to it and so um

**[04:55]** so um

**[04:56]** so um I think they're very proud of you. Yes.

**[04:57]** I think they're very proud of you. Yes.

**[04:57]** I think they're very proud of you. Yes. Absolutely. So you you were with Stripe

**[04:59]** Absolutely. So you you were with Stripe

**[04:59]** Absolutely. So you you were with Stripe from 4 to 250 people as the first CTO


### [05:00 - 06:00]

**[05:01]** from 4 to 250 people as the first CTO

**[05:01]** from 4 to 250 people as the first CTO eventually. Um one thing I I found

**[05:03]** eventually. Um one thing I I found

**[05:03]** eventually. Um one thing I I found recently that Hacker News maybe doesn't

**[05:06]** recently that Hacker News maybe doesn't

**[05:06]** recently that Hacker News maybe doesn't know is apparently the call installation

**[05:08]** know is apparently the call installation

**[05:08]** know is apparently the call installation only happened like a handful of times.

**[05:09]** only happened like a handful of times.

**[05:09]** only happened like a handful of times. It wasn't like a thing at Stripe. Was

**[05:12]** It wasn't like a thing at Stripe. Was

**[05:12]** It wasn't like a thing at Stripe. Was that that's I think that's true. Um yeah

**[05:15]** that that's I think that's true. Um yeah

**[05:15]** that that's I think that's true. Um yeah it is it is the thing that that you know

**[05:17]** it is it is the thing that that you know

**[05:17]** it is it is the thing that that you know it's like survived the uh the It's an

**[05:19]** it's like survived the uh the It's an

**[05:19]** it's like survived the uh the It's an urban legend because it's like so cool.

**[05:21]** urban legend because it's like so cool.

**[05:21]** urban legend because it's like so cool. It's like you so customer obsessed.

**[05:22]** It's like you so customer obsessed.

**[05:22]** It's like you so customer obsessed. Anyway, so what else do people get wrong

**[05:24]** Anyway, so what else do people get wrong

**[05:24]** Anyway, so what else do people get wrong about early Stripe? Like why do we want

**[05:25]** about early Stripe? Like why do we want

**[05:26]** about early Stripe? Like why do we want to clear the air? Yeah. Well, I think

**[05:28]** to clear the air? Yeah. Well, I think

**[05:28]** to clear the air? Yeah. Well, I think people don't understand how hard it was,

**[05:30]** people don't understand how hard it was,

**[05:30]** people don't understand how hard it was, right? It was just like um like I

**[05:32]** right? It was just like um like I

**[05:32]** right? It was just like um like I remember um you know, first of all, the

**[05:34]** remember um you know, first of all, the

**[05:34]** remember um you know, first of all, the the kind of thing that we did a lot of

**[05:35]** the kind of thing that we did a lot of

**[05:35]** the kind of thing that we did a lot of is that we added all of our customers on

**[05:37]** is that we added all of our customers on

**[05:37]** is that we added all of our customers on Ghat. And so it was very much the case

**[05:39]** Ghat. And so it was very much the case

**[05:39]** Ghat. And so it was very much the case that we were in constant contact with

**[05:41]** that we were in constant contact with

**[05:41]** that we were in constant contact with them. And so even if you're not

**[05:42]** them. And so even if you're not

**[05:42]** them. And so even if you're not literally sitting over their their

**[05:43]** literally sitting over their their

**[05:43]** literally sitting over their their shoulder, you're doing the next best

**[05:44]** shoulder, you're doing the next best

**[05:44]** shoulder, you're doing the next best thing. Um, but I remember um like one I

**[05:49]** thing. Um, but I remember um like one I

**[05:49]** thing. Um, but I remember um like one I you know one one one day we realized

**[05:51]** you know one one one day we realized

**[05:51]** you know one one one day we realized that I you know the the the payment back

**[05:53]** that I you know the the the payment back

**[05:53]** that I you know the the the payment back end that we were on it just wasn't going

**[05:55]** end that we were on it just wasn't going

**[05:55]** end that we were on it just wasn't going to scale. Uh we absolutely needed to be

**[05:56]** to scale. Uh we absolutely needed to be

**[05:56]** to scale. Uh we absolutely needed to be on Wells Fargo and we got sort of the


### [06:00 - 07:00]

**[06:00]** on Wells Fargo and we got sort of the

**[06:00]** on Wells Fargo and we got sort of the deal done but now we need to do a

**[06:02]** deal done but now we need to do a

**[06:02]** deal done but now we need to do a technical integration. And they said

**[06:03]** technical integration. And they said

**[06:03]** technical integration. And they said well this technical integration is going

**[06:05]** well this technical integration is going

**[06:05]** well this technical integration is going to take like 9 months because that's how

**[06:07]** to take like 9 months because that's how

**[06:07]** to take like 9 months because that's how long it takes. And we're like that's

**[06:08]** long it takes. And we're like that's

**[06:08]** long it takes. And we're like that's crazy. Like you're a startup. Like we

**[06:10]** crazy. Like you're a startup. Like we

**[06:10]** crazy. Like you're a startup. Like we can't sit around waiting 9 months to get

**[06:11]** can't sit around waiting 9 months to get

**[06:11]** can't sit around waiting 9 months to get this thing done. Um and so actually in

**[06:14]** this thing done. Um and so actually in

**[06:14]** this thing done. Um and so actually in 24 hours uh we completed it uh by just

**[06:17]** 24 hours uh we completed it uh by just

**[06:17]** 24 hours uh we completed it uh by just basically treating it like a college

**[06:19]** basically treating it like a college

**[06:19]** basically treating it like a college problem set. Uh and it was you know I I

**[06:21]** problem set. Uh and it was you know I I

**[06:21]** problem set. Uh and it was you know I I was implementing everything. John was

**[06:23]** was implementing everything. John was

**[06:23]** was implementing everything. John was working from the top of this test script

**[06:24]** working from the top of this test script

**[06:24]** working from the top of this test script and testing everything and being like

**[06:25]** and testing everything and being like

**[06:25]** and testing everything and being like this is broken. Daryl was starting from

**[06:27]** this is broken. Daryl was starting from

**[06:27]** this is broken. Daryl was starting from the bottom and working his way up. And

**[06:29]** the bottom and working his way up. And

**[06:29]** the bottom and working his way up. And uh in the morning we got on with with

**[06:31]** uh in the morning we got on with with

**[06:31]** uh in the morning we got on with with the uh certifying person and we sent

**[06:35]** the uh certifying person and we sent

**[06:35]** the uh certifying person and we sent some some test messages and there was an

**[06:37]** some some test messages and there was an

**[06:37]** some some test messages and there was an error and the person's like all right

**[06:38]** error and the person's like all right

**[06:38]** error and the person's like all right I'll see you next week. Um because

**[06:39]** I'll see you next week. Um because

**[06:39]** I'll see you next week. Um because that's how all their customers operate,

**[06:41]** that's how all their customers operate,

**[06:41]** that's how all their customers operate, right? there's an error like you know

**[06:42]** right? there's an error like you know

**[06:42]** right? there's an error like you know clear you need to send it to your dev

**[06:43]** clear you need to send it to your dev

**[06:43]** clear you need to send it to your dev team and we were like no no no there

**[06:45]** team and we were like no no no there

**[06:45]** team and we were like no no no there must just be like like some sort of

**[06:46]** must just be like like some sort of

**[06:46]** must just be like like some sort of glitch in the system like and we just

**[06:48]** glitch in the system like and we just

**[06:48]** glitch in the system like and we just Patrick was just like talking to keep

**[06:49]** Patrick was just like talking to keep

**[06:49]** Patrick was just like talking to keep her on the line and frantically like I

**[06:51]** her on the line and frantically like I

**[06:51]** her on the line and frantically like I was there editing the code and so we got

**[06:54]** was there editing the code and so we got

**[06:54]** was there editing the code and so we got like five turns in uh and we actually

**[06:56]** like five turns in uh and we actually

**[06:56]** like five turns in uh and we actually failed uh but fortunately she was nice

**[06:58]** failed uh but fortunately she was nice

**[06:58]** failed uh but fortunately she was nice enough to reschedule two two hours later


### [07:00 - 08:00]

**[07:00]** enough to reschedule two two hours later

**[07:00]** enough to reschedule two two hours later uh and then we passed and so you realize

**[07:02]** uh and then we passed and so you realize

**[07:02]** uh and then we passed and so you realize that was like six weeks worth of normal

**[07:04]** that was like six weeks worth of normal

**[07:04]** that was like six weeks worth of normal dev work that you got done in that

**[07:05]** dev work that you got done in that

**[07:05]** dev work that you got done in that moment because you didn't just accept

**[07:07]** moment because you didn't just accept

**[07:07]** moment because you didn't just accept the like arbitrary constraints of how

**[07:09]** the like arbitrary constraints of how

**[07:09]** the like arbitrary constraints of how other organizations would work. Yeah.

**[07:11]** other organizations would work. Yeah.

**[07:11]** other organizations would work. Yeah. Yeah. Do I think there's a do you think

**[07:12]** Yeah. Do I think there's a do you think

**[07:12]** Yeah. Do I think there's a do you think there's a lot more opportunity like that

**[07:14]** there's a lot more opportunity like that

**[07:14]** there's a lot more opportunity like that in most jobs? Like how do you how do you

**[07:17]** in most jobs? Like how do you how do you

**[07:17]** in most jobs? Like how do you how do you advise other people to be that I guess

**[07:20]** advise other people to be that I guess

**[07:20]** advise other people to be that I guess fast or like to cut that many cycles?

**[07:22]** fast or like to cut that many cycles?

**[07:22]** fast or like to cut that many cycles? Yes. I mean I think that I the way I

**[07:24]** Yes. I mean I think that I the way I

**[07:24]** Yes. I mean I think that I the way I think about it is that if you think from

**[07:26]** think about it is that if you think from

**[07:26]** think about it is that if you think from first principles you can find where

**[07:29]** first principles you can find where

**[07:29]** first principles you can find where things need to be slow or done the way

**[07:32]** things need to be slow or done the way

**[07:32]** things need to be slow or done the way that they're normally done or whatever

**[07:33]** that they're normally done or whatever

**[07:33]** that they're normally done or whatever those things are those exist right the

**[07:35]** those things are those exist right the

**[07:36]** those things are those exist right the general principle of ah just don't worry

**[07:37]** general principle of ah just don't worry

**[07:37]** general principle of ah just don't worry about the constraints and just do the

**[07:38]** about the constraints and just do the

**[07:38]** about the constraints and just do the thing. Um, I think that that that is not

**[07:41]** thing. Um, I think that that that is not

**[07:41]** thing. Um, I think that that that is not 100% true. I think it's really about

**[07:43]** 100% true. I think it's really about

**[07:43]** 100% true. I think it's really about mapping to where is there unnecessary

**[07:45]** mapping to where is there unnecessary

**[07:45]** mapping to where is there unnecessary overhead that's there for constraints

**[07:47]** overhead that's there for constraints

**[07:47]** overhead that's there for constraints that are no longer applicable that that

**[07:49]** that are no longer applicable that that

**[07:49]** that are no longer applicable that that don't apply uh to your specific

**[07:50]** don't apply uh to your specific

**[07:50]** don't apply uh to your specific circumstance. And I think this is

**[07:52]** circumstance. And I think this is

**[07:52]** circumstance. And I think this is especially true in this world that we're

**[07:53]** especially true in this world that we're

**[07:53]** especially true in this world that we're in now with AI that's accelerating

**[07:55]** in now with AI that's accelerating

**[07:55]** in now with AI that's accelerating productivity so much. Yeah. Just fire

**[07:57]** productivity so much. Yeah. Just fire

**[07:57]** productivity so much. Yeah. Just fire off a codeex. Why not, right? Um, one


### [08:00 - 09:00]

**[08:00]** off a codeex. Why not, right? Um, one

**[08:00]** off a codeex. Why not, right? Um, one thing one thing one last thing about

**[08:01]** thing one thing one last thing about

**[08:01]** thing one thing one last thing about your sort of pre-openi life was

**[08:03]** your sort of pre-openi life was

**[08:03]** your sort of pre-openi life was independent study. I just I I found that

**[08:05]** independent study. I just I I found that

**[08:05]** independent study. I just I I found that just it's a recurrent theme from high

**[08:07]** just it's a recurrent theme from high

**[08:07]** just it's a recurrent theme from high school. You did recenter. I did. Um and

**[08:11]** school. You did recenter. I did. Um and

**[08:11]** school. You did recenter. I did. Um and your sbatical as well. So you've just

**[08:12]** your sbatical as well. So you've just

**[08:12]** your sbatical as well. So you've just done it repeatedly. What makes

**[08:14]** done it repeatedly. What makes

**[08:14]** done it repeatedly. What makes independent study effective? Like I

**[08:17]** independent study effective? Like I

**[08:17]** independent study effective? Like I think there's a lot of people who don't

**[08:18]** think there's a lot of people who don't

**[08:18]** think there's a lot of people who don't do a good job of it and kind of waste a

**[08:20]** do a good job of it and kind of waste a

**[08:20]** do a good job of it and kind of waste a year. What what what do you do that

**[08:22]** year. What what what do you do that

**[08:22]** year. What what what do you do that makes it so effective? Well, I think it

**[08:24]** makes it so effective? Well, I think it

**[08:24]** makes it so effective? Well, I think it was a key part of how I grew up. um you

**[08:27]** was a key part of how I grew up. um you

**[08:27]** was a key part of how I grew up. um you know in in uh in sixth grade my dad

**[08:30]** know in in uh in sixth grade my dad

**[08:30]** know in in uh in sixth grade my dad taught me algebra and in seventh grade

**[08:33]** taught me algebra and in seventh grade

**[08:33]** taught me algebra and in seventh grade showed up at the high school as the

**[08:34]** showed up at the high school as the

**[08:34]** showed up at the high school as the first time that you you track into

**[08:36]** first time that you you track into

**[08:36]** first time that you you track into advanced math pre-alggebra and we went

**[08:38]** advanced math pre-alggebra and we went

**[08:38]** advanced math pre-alggebra and we went to the teacher like can he skip uh this

**[08:40]** to the teacher like can he skip uh this

**[08:40]** to the teacher like can he skip uh this and go directly to the the eighth year

**[08:42]** and go directly to the the eighth year

**[08:42]** and go directly to the the eighth year the eighth grade course and the teacher

**[08:43]** the eighth grade course and the teacher

**[08:43]** the eighth grade course and the teacher looked at my mom and me very

**[08:45]** looked at my mom and me very

**[08:45]** looked at my mom and me very condescendingly and was like every

**[08:47]** condescendingly and was like every

**[08:47]** condescendingly and was like every parent believes that their child is

**[08:49]** parent believes that their child is

**[08:49]** parent believes that their child is special

**[08:51]** special

**[08:51]** special and after like a month of being in this

**[08:53]** and after like a month of being in this

**[08:53]** and after like a month of being in this teacher's class and you know I was

**[08:54]** teacher's class and you know I was

**[08:54]** teacher's class and you know I was paying no attention and just doing you

**[08:56]** paying no attention and just doing you

**[08:56]** paying no attention and just doing you know calcul calculator games in in the

**[08:57]** know calcul calculator games in in the

**[08:57]** know calcul calculator games in in the back and she'd try to trip me up and,

**[08:59]** back and she'd try to trip me up and,

**[08:59]** back and she'd try to trip me up and, you know, call me to answer questions


### [09:00 - 10:00]

**[09:00]** you know, call me to answer questions

**[09:00]** you know, call me to answer questions from the whiteboard and I would just get

**[09:02]** from the whiteboard and I would just get

**[09:02]** from the whiteboard and I would just get them all right. She was like, "All

**[09:03]** them all right. She was like, "All

**[09:03]** them all right. She was like, "All right, like fair enough. Uh, your your

**[09:04]** right, like fair enough. Uh, your your

**[09:04]** right, like fair enough. Uh, your your child should be uh in the next year."

**[09:06]** child should be uh in the next year."

**[09:06]** child should be uh in the next year." Um, and but then when I was in eighth

**[09:08]** Um, and but then when I was in eighth

**[09:08]** Um, and but then when I was in eighth grade, there was no more math left in my

**[09:10]** grade, there was no more math left in my

**[09:10]** grade, there was no more math left in my middle school. I didn't have a car, so I

**[09:12]** middle school. I didn't have a car, so I

**[09:12]** middle school. I didn't have a car, so I had to do online courses. And in that

**[09:14]** had to do online courses. And in that

**[09:14]** had to do online courses. And in that one year, I ended up doing three years

**[09:16]** one year, I ended up doing three years

**[09:16]** one year, I ended up doing three years worth of high school math. And so I

**[09:18]** worth of high school math. And so I

**[09:18]** worth of high school math. And so I think that for me a lot of it is about

**[09:20]** think that for me a lot of it is about

**[09:20]** think that for me a lot of it is about suddenly these if you're if you're

**[09:22]** suddenly these if you're if you're

**[09:22]** suddenly these if you're if you're excited about something independently

**[09:24]** excited about something independently

**[09:24]** excited about something independently it's something you want to do that you

**[09:26]** it's something you want to do that you

**[09:26]** it's something you want to do that you can break the constraints there as well.

**[09:27]** can break the constraints there as well.

**[09:27]** can break the constraints there as well. Uh you can do three years of math in one

**[09:29]** Uh you can do three years of math in one

**[09:29]** Uh you can do three years of math in one year and then it compounds because the

**[09:31]** year and then it compounds because the

**[09:31]** year and then it compounds because the next year I was at my high school

**[09:33]** next year I was at my high school

**[09:33]** next year I was at my high school finished math there and then all through

**[09:35]** finished math there and then all through

**[09:35]** finished math there and then all through 10th 11th 12th grade I I had you know no

**[09:39]** 10th 11th 12th grade I I had you know no

**[09:39]** 10th 11th 12th grade I I had you know no more math so I did have a car and I was

**[09:40]** more math so I did have a car and I was

**[09:40]** more math so I did have a car and I was able to go to University of North Dakota

**[09:42]** able to go to University of North Dakota

**[09:42]** able to go to University of North Dakota take whatever classes I wanted there.

**[09:44]** take whatever classes I wanted there.

**[09:44]** take whatever classes I wanted there. And so I think that that that kind of

**[09:47]** And so I think that that that kind of

**[09:47]** And so I think that that that kind of compounded compounded compounded to

**[09:48]** compounded compounded compounded to

**[09:48]** compounded compounded compounded to learning programming. And then I think

**[09:50]** learning programming. And then I think

**[09:50]** learning programming. And then I think that that the way I learned program is

**[09:52]** that that the way I learned program is

**[09:52]** that that the way I learned program is very much self-study just building

**[09:54]** very much self-study just building

**[09:54]** very much self-study just building things and and experiencing things out

**[09:55]** things and and experiencing things out

**[09:55]** things and and experiencing things out in the world. And so I think that the

**[09:57]** in the world. And so I think that the

**[09:57]** in the world. And so I think that the thing I would just advise is like if you


### [10:00 - 11:00]

**[10:00]** thing I would just advise is like if you

**[10:00]** thing I would just advise is like if you have an opportunity to explore and you

**[10:03]** have an opportunity to explore and you

**[10:03]** have an opportunity to explore and you have a passion, you're actually enjoying

**[10:04]** have a passion, you're actually enjoying

**[10:04]** have a passion, you're actually enjoying it, just go deep, right? And by the way,

**[10:07]** it, just go deep, right? And by the way,

**[10:07]** it, just go deep, right? And by the way, it's not always fun, right? I think that

**[10:09]** it's not always fun, right? I think that

**[10:09]** it's not always fun, right? I think that it is very easy to uh get kind of you

**[10:11]** it is very easy to uh get kind of you

**[10:11]** it is very easy to uh get kind of you know sort of feel like uh I got kind of

**[10:13]** know sort of feel like uh I got kind of

**[10:13]** know sort of feel like uh I got kind of bored but if you just push through those

**[10:15]** bored but if you just push through those

**[10:15]** bored but if you just push through those hurdles then I think that the that the

**[10:16]** hurdles then I think that the that the

**[10:16]** hurdles then I think that the that the reward is worth it. Yeah. You

**[10:17]** reward is worth it. Yeah. You

**[10:18]** reward is worth it. Yeah. You self-studied machine learning too like

**[10:19]** self-studied machine learning too like

**[10:19]** self-studied machine learning too like that was a whole period of your life. Um

**[10:21]** that was a whole period of your life. Um

**[10:22]** that was a whole period of your life. Um any particular highlights from there? It

**[10:23]** any particular highlights from there? It

**[10:23]** any particular highlights from there? It sounds like you talked to Jeff Hinton at

**[10:25]** sounds like you talked to Jeff Hinton at

**[10:25]** sounds like you talked to Jeff Hinton at one time. I did talk to Jeff Hinton.

**[10:27]** one time. I did talk to Jeff Hinton.

**[10:27]** one time. I did talk to Jeff Hinton. Yeah. Yes. And like was you know did

**[10:29]** Yeah. Yes. And like was you know did

**[10:30]** Yeah. Yes. And like was you know did that help or what was the most helpful

**[10:31]** that help or what was the most helpful

**[10:31]** that help or what was the most helpful thing like you became a machine learning

**[10:33]** thing like you became a machine learning

**[10:33]** thing like you became a machine learning practitioner? Well, so so when I when I

**[10:34]** practitioner? Well, so so when I when I

**[10:34]** practitioner? Well, so so when I when I started out, so you know, I'd been I'd

**[10:37]** started out, so you know, I'd been I'd

**[10:37]** started out, so you know, I'd been I'd been at Stripe. I was reading hacker

**[10:39]** been at Stripe. I was reading hacker

**[10:39]** been at Stripe. I was reading hacker news post about deep learning and yeah,

**[10:41]** news post about deep learning and yeah,

**[10:42]** news post about deep learning and yeah, it was like, you know, there's a deep

**[10:43]** it was like, you know, there's a deep

**[10:43]** it was like, you know, there's a deep learning for ACT like every day it felt

**[10:45]** learning for ACT like every day it felt

**[10:45]** learning for ACT like every day it felt like and this was, you know, 2013, 2014

**[10:48]** like and this was, you know, 2013, 2014

**[10:48]** like and this was, you know, 2013, 2014 and I was like, what is deep learning?

**[10:50]** and I was like, what is deep learning?

**[10:50]** and I was like, what is deep learning? and I knew like one person in the field

**[10:51]** and I knew like one person in the field

**[10:52]** and I knew like one person in the field and so I talked to them, they introduced

**[10:53]** and so I talked to them, they introduced

**[10:53]** and so I talked to them, they introduced me to some more people and then they

**[10:54]** me to some more people and then they

**[10:54]** me to some more people and then they introduced me to more people and the

**[10:56]** introduced me to more people and the

**[10:56]** introduced me to more people and the thing that surprised me was I kept

**[10:58]** thing that surprised me was I kept

**[10:58]** thing that surprised me was I kept getting introduced to a bunch of my


### [11:00 - 12:00]

**[11:00]** getting introduced to a bunch of my

**[11:00]** getting introduced to a bunch of my smartest friends from college and I was

**[11:01]** smartest friends from college and I was

**[11:02]** smartest friends from college and I was like that's interesting. All of these

**[11:03]** like that's interesting. All of these

**[11:03]** like that's interesting. All of these people ended up in this field like

**[11:05]** people ended up in this field like

**[11:05]** people ended up in this field like what's going on and I started to realize

**[11:07]** what's going on and I started to realize

**[11:07]** what's going on and I started to realize that that there was something real that

**[11:08]** that that there was something real that

**[11:08]** that that there was something real that was building right that was being

**[11:10]** was building right that was being

**[11:10]** was building right that was being developed that people were really making

**[11:12]** developed that people were really making

**[11:12]** developed that people were really making these systems do material new things

**[11:16]** these systems do material new things

**[11:16]** these systems do material new things that computers were not able to do

**[11:17]** that computers were not able to do

**[11:18]** that computers were not able to do before. And I was like that that is the

**[11:19]** before. And I was like that that is the

**[11:19]** before. And I was like that that is the thing. Um and so after I left Stripe,

**[11:23]** thing. Um and so after I left Stripe,

**[11:23]** thing. Um and so after I left Stripe, you know, I knew I wanted to do

**[11:24]** you know, I knew I wanted to do

**[11:24]** you know, I knew I wanted to do something in AI. Um start an AI company,

**[11:26]** something in AI. Um start an AI company,

**[11:26]** something in AI. Um start an AI company, but I didn't really know how to

**[11:28]** but I didn't really know how to

**[11:28]** but I didn't really know how to contribute, what my skills would be

**[11:30]** contribute, what my skills would be

**[11:30]** contribute, what my skills would be useful for. And uh so I was in New York

**[11:33]** useful for. And uh so I was in New York

**[11:33]** useful for. And uh so I was in New York and I was like, you know what, I'll

**[11:34]** and I was like, you know what, I'll

**[11:34]** and I was like, you know what, I'll build a GPU rig and see if I can do some

**[11:36]** build a GPU rig and see if I can do some

**[11:36]** build a GPU rig and see if I can do some Kaggle competitions. And so I went on

**[11:38]** Kaggle competitions. And so I went on

**[11:38]** Kaggle competitions. And so I went on Newegg and just like, you know, bought

**[11:40]** Newegg and just like, you know, bought

**[11:40]** Newegg and just like, you know, bought some uh some Titan X cards. And uh it

**[11:43]** some uh some Titan X cards. And uh it

**[11:43]** some uh some Titan X cards. And uh it was really cool, you know, physically

**[11:44]** was really cool, you know, physically

**[11:44]** was really cool, you know, physically assembling this machine. And uh you can

**[11:46]** assembling this machine. And uh you can

**[11:46]** assembling this machine. And uh you can find some some tweet from from 2015 when

**[11:49]** find some some tweet from from 2015 when

**[11:49]** find some some tweet from from 2015 when I powered it on. You see all this like

**[11:50]** I powered it on. You see all this like

**[11:50]** I powered it on. You see all this like green and all the fans going and I was

**[11:52]** green and all the fans going and I was

**[11:52]** green and all the fans going and I was like this this is what computers are

**[11:54]** like this this is what computers are

**[11:54]** like this this is what computers are meant to be.

**[11:55]** meant to be.

**[11:55]** meant to be. Uh I think many folks in the audience

**[11:57]** Uh I think many folks in the audience

**[11:57]** Uh I think many folks in the audience have that experience as well. Um


### [12:00 - 13:00]

**[12:00]** have that experience as well. Um

**[12:00]** have that experience as well. Um awesome. Okay. So what convinced you

**[12:03]** awesome. Okay. So what convinced you

**[12:03]** awesome. Okay. So what convinced you that AGI was possible? Like you you had

**[12:05]** that AGI was possible? Like you you had

**[12:06]** that AGI was possible? Like you you had a point where you were sort of

**[12:06]** a point where you were sort of

**[12:06]** a point where you were sort of disillusioned with it. You wrote you

**[12:08]** disillusioned with it. You wrote you

**[12:08]** disillusioned with it. You wrote you tried to write a chatbot. You didn't it

**[12:09]** tried to write a chatbot. You didn't it

**[12:09]** tried to write a chatbot. You didn't it didn't work. But what made you go all in

**[12:12]** didn't work. But what made you go all in

**[12:12]** didn't work. But what made you go all in on it? Yeah. Well, so you know, part of

**[12:14]** on it? Yeah. Well, so you know, part of

**[12:14]** on it? Yeah. Well, so you know, part of part of the journey for me was reading

**[12:15]** part of the journey for me was reading

**[12:15]** part of the journey for me was reading Alan Touring's 1950 paper, Computing

**[12:18]** Alan Touring's 1950 paper, Computing

**[12:18]** Alan Touring's 1950 paper, Computing Machinery, and Intelligence. This is the

**[12:19]** Machinery, and Intelligence. This is the

**[12:19]** Machinery, and Intelligence. This is the Touring test paper. How many people have

**[12:21]** Touring test paper. How many people have

**[12:21]** Touring test paper. How many people have read it? Read it. You fewer hands than

**[12:24]** read it? Read it. You fewer hands than

**[12:24]** read it? Read it. You fewer hands than than W3 schools. Uh but equally as

**[12:26]** than W3 schools. Uh but equally as

**[12:26]** than W3 schools. Uh but equally as important, uh worth reading. Uh the

**[12:30]** important, uh worth reading. Uh the

**[12:30]** important, uh worth reading. Uh the thing that is so fascinating to me is he

**[12:32]** thing that is so fascinating to me is he

**[12:32]** thing that is so fascinating to me is he lays out in the beginning, okay, Turing

**[12:33]** lays out in the beginning, okay, Turing

**[12:33]** lays out in the beginning, okay, Turing test, this idea of just does a machine

**[12:36]** test, this idea of just does a machine

**[12:36]** test, this idea of just does a machine think? Is it intelligent? And you can

**[12:38]** think? Is it intelligent? And you can

**[12:38]** think? Is it intelligent? And you can say it's intelligent if you know a human

**[12:40]** say it's intelligent if you know a human

**[12:40]** say it's intelligent if you know a human can't tell the difference between

**[12:41]** can't tell the difference between

**[12:41]** can't tell the difference between talking to it and talking to a human.

**[12:42]** talking to it and talking to a human.

**[12:42]** talking to it and talking to a human. Fine. But the thing that was that has

**[12:44]** Fine. But the thing that was that has

**[12:44]** Fine. But the thing that was that has not really become as embedded in the pop

**[12:47]** not really become as embedded in the pop

**[12:47]** not really become as embedded in the pop culture, but to me was so astounding was

**[12:50]** culture, but to me was so astounding was

**[12:50]** culture, but to me was so astounding was he said, "Well, how are you going to

**[12:51]** he said, "Well, how are you going to

**[12:51]** he said, "Well, how are you going to program an answer to this? You will

**[12:53]** program an answer to this? You will

**[12:53]** program an answer to this? You will never be able to write down all the

**[12:55]** never be able to write down all the

**[12:55]** never be able to write down all the rules. But what if you could build a

**[12:56]** rules. But what if you could build a

**[12:56]** rules. But what if you could build a child machine that learns like a human

**[12:58]** child machine that learns like a human

**[12:58]** child machine that learns like a human child and then you just apply rewards


### [13:00 - 14:00]

**[13:00]** child and then you just apply rewards

**[13:00]** child and then you just apply rewards and punishments and boom, it's going to

**[13:03]** and punishments and boom, it's going to

**[13:03]** and punishments and boom, it's going to uh it's going to to be able to to pass

**[13:05]** uh it's going to to be able to to pass

**[13:05]** uh it's going to to be able to to pass the test." And I was like that that is

**[13:07]** the test." And I was like that that is

**[13:07]** the test." And I was like that that is the kind of technology that we have to

**[13:08]** the kind of technology that we have to

**[13:08]** the kind of technology that we have to build because as a programmer you have

**[13:10]** build because as a programmer you have

**[13:10]** build because as a programmer you have to understand everything. You have to

**[13:12]** to understand everything. You have to

**[13:12]** to understand everything. You have to understand the rules of how to solve the

**[13:14]** understand the rules of how to solve the

**[13:14]** understand the rules of how to solve the problem. But what if the machine can

**[13:17]** problem. But what if the machine can

**[13:17]** problem. But what if the machine can understand things and solve problems

**[13:18]** understand things and solve problems

**[13:18]** understand things and solve problems that you yourself cannot understand.

**[13:20]** that you yourself cannot understand.

**[13:20]** that you yourself cannot understand. Like that feels fundamental, right? That

**[13:23]** Like that feels fundamental, right? That

**[13:23]** Like that feels fundamental, right? That feels like how you actually solve

**[13:24]** feels like how you actually solve

**[13:24]** feels like how you actually solve problems that are important to humanity.

**[13:26]** problems that are important to humanity.

**[13:26]** problems that are important to humanity. And I this was you know 20 2008 or so

**[13:30]** And I this was you know 20 2008 or so

**[13:30]** And I this was you know 20 2008 or so that I read this and I went to my

**[13:32]** that I read this and I went to my

**[13:32]** that I read this and I went to my professor and uh who was an NLP

**[13:34]** professor and uh who was an NLP

**[13:34]** professor and uh who was an NLP professor and I asked if I could do some

**[13:35]** professor and I asked if I could do some

**[13:36]** professor and I asked if I could do some research with him and he said yeah here

**[13:38]** research with him and he said yeah here

**[13:38]** research with him and he said yeah here are some pars trees and I was like okay

**[13:40]** are some pars trees and I was like okay

**[13:40]** are some pars trees and I was like okay this is not what Turing was talking

**[13:41]** this is not what Turing was talking

**[13:41]** this is not what Turing was talking about. Yeah. Um this is like word nets

**[13:43]** about. Yeah. Um this is like word nets

**[13:43]** about. Yeah. Um this is like word nets and the whole thing. Exactly. So it's

**[13:45]** and the whole thing. Exactly. So it's

**[13:45]** and the whole thing. Exactly. So it's like you you know definitely a little

**[13:47]** like you you know definitely a little

**[13:47]** like you you know definitely a little bit of trough of sorrow there. Um, but

**[13:49]** bit of trough of sorrow there. Um, but

**[13:49]** bit of trough of sorrow there. Um, but with deep learning, the thing about deep

**[13:51]** with deep learning, the thing about deep

**[13:51]** with deep learning, the thing about deep learning that's magic is that, you know,

**[13:53]** learning that's magic is that, you know,

**[13:53]** learning that's magic is that, you know, it really started in to show show

**[13:56]** it really started in to show show

**[13:56]** it really started in to show show promising results 2012 with with

**[13:57]** promising results 2012 with with

**[13:57]** promising results 2012 with with AlexNet, right? And and that it just

**[13:59]** AlexNet, right? And and that it just

**[13:59]** AlexNet, right? And and that it just blew everyone out of the water in the


### [14:00 - 15:00]

**[14:01]** blew everyone out of the water in the

**[14:01]** blew everyone out of the water in the imageet competition. And so suddenly you

**[14:03]** imageet competition. And so suddenly you

**[14:03]** imageet competition. And so suddenly you have this like general learning machine.

**[14:06]** have this like general learning machine.

**[14:06]** have this like general learning machine. You know, it's got a little bit of a

**[14:07]** You know, it's got a little bit of a

**[14:07]** You know, it's got a little bit of a prior in there of of of convolutions,

**[14:10]** prior in there of of of convolutions,

**[14:10]** prior in there of of of convolutions, but it's better than 40 years worth of

**[14:13]** but it's better than 40 years worth of

**[14:13]** but it's better than 40 years worth of computer vision research, right? People

**[14:15]** computer vision research, right? People

**[14:15]** computer vision research, right? People trying to write down all the rules as

**[14:16]** trying to write down all the rules as

**[14:16]** trying to write down all the rules as well as possible. And then people are

**[14:18]** well as possible. And then people are

**[14:18]** well as possible. And then people are like, well, okay, it works in vision,

**[14:19]** like, well, okay, it works in vision,

**[14:20]** like, well, okay, it works in vision, but it's never going to work in my

**[14:21]** but it's never going to work in my

**[14:21]** but it's never going to work in my field. It's never going to work in

**[14:22]** field. It's never going to work in

**[14:22]** field. It's never going to work in machine translation, never going to work

**[14:24]** machine translation, never going to work

**[14:24]** machine translation, never going to work in uh in, you know, in NLP, never going

**[14:27]** in uh in, you know, in NLP, never going

**[14:27]** in uh in, you know, in NLP, never going to work in this or that. And suddenly it

**[14:29]** to work in this or that. And suddenly it

**[14:29]** to work in this or that. And suddenly it starts being the best in all of those

**[14:30]** starts being the best in all of those

**[14:30]** starts being the best in all of those areas. Suddenly the walls between these

**[14:32]** areas. Suddenly the walls between these

**[14:32]** areas. Suddenly the walls between these departments are being torn down and

**[14:34]** departments are being torn down and

**[14:34]** departments are being torn down and you're like that that is what Terraring

**[14:36]** you're like that that is what Terraring

**[14:36]** you're like that that is what Terraring was talking about. And so I think for me

**[14:38]** was talking about. And so I think for me

**[14:38]** was talking about. And so I think for me just seeing the the type signature of

**[14:41]** just seeing the the type signature of

**[14:41]** just seeing the the type signature of this technology and by the way this

**[14:42]** this technology and by the way this

**[14:42]** this technology and by the way this technology is not new, right? neural

**[14:44]** technology is not new, right? neural

**[14:44]** technology is not new, right? neural nets were really like if you go back and

**[14:46]** nets were really like if you go back and

**[14:46]** nets were really like if you go back and read the uh the Mcculla Pitts uh neuron

**[14:48]** read the uh the Mcculla Pitts uh neuron

**[14:48]** read the uh the Mcculla Pitts uh neuron paper from like 1943 or so um I told

**[14:52]** paper from like 1943 or so um I told

**[14:52]** paper from like 1943 or so um I told people I told him to give homework to

**[14:53]** people I told him to give homework to

**[14:53]** people I told him to give homework to people. Okay. Yeah, there you go. Yes.

**[14:56]** people. Okay. Yeah, there you go. Yes.

**[14:56]** people. Okay. Yeah, there you go. Yes. Classes assigned. Um

**[14:58]** Classes assigned. Um

**[14:58]** Classes assigned. Um the there the the images in there they


### [15:00 - 16:00]

**[15:01]** the there the the images in there they

**[15:01]** the there the the images in there they look just like the kinds of images that

**[15:02]** look just like the kinds of images that

**[15:02]** look just like the kinds of images that you see now of just like you know layers

**[15:04]** you see now of just like you know layers

**[15:04]** you see now of just like you know layers of neurons and things like that. And so

**[15:06]** of neurons and things like that. And so

**[15:06]** of neurons and things like that. And so you just realize there's something

**[15:07]** you just realize there's something

**[15:07]** you just realize there's something deeply fundamental about what we're

**[15:09]** deeply fundamental about what we're

**[15:09]** deeply fundamental about what we're doing. And uh you can find these these

**[15:11]** doing. And uh you can find these these

**[15:11]** doing. And uh you can find these these uh you can find this paper um from 199

**[15:14]** uh you can find this paper um from 199

**[15:14]** uh you can find this paper um from 199 the 1990s talking about what caused the

**[15:16]** the 1990s talking about what caused the

**[15:16]** the 1990s talking about what caused the deep learning winters and that it was

**[15:18]** deep learning winters and that it was

**[15:18]** deep learning winters and that it was these neural net people. They have no

**[15:20]** these neural net people. They have no

**[15:20]** these neural net people. They have no new ideas. They just want to build

**[15:21]** new ideas. They just want to build

**[15:22]** new ideas. They just want to build bigger computers. And I'm like yes

**[15:24]** bigger computers. And I'm like yes

**[15:24]** bigger computers. And I'm like yes that's what we need to do. Um and so I

**[15:27]** that's what we need to do. Um and so I

**[15:27]** that's what we need to do. Um and so I think that all of this together just

**[15:28]** think that all of this together just

**[15:28]** think that all of this together just feels like we are we are to some extent

**[15:32]** feels like we are we are to some extent

**[15:32]** feels like we are we are to some extent continuing this wave this 70year

**[15:34]** continuing this wave this 70year

**[15:34]** continuing this wave this 70year history. Um and in many ways um you know

**[15:36]** history. Um and in many ways um you know

**[15:36]** history. Um and in many ways um you know the whole computing industry has been

**[15:38]** the whole computing industry has been

**[15:38]** the whole computing industry has been really trying to build up to the point

**[15:39]** really trying to build up to the point

**[15:39]** really trying to build up to the point that you can have machines that are able

**[15:42]** that you can have machines that are able

**[15:42]** that you can have machines that are able to perform the kinds of tasks that we're

**[15:44]** to perform the kinds of tasks that we're

**[15:44]** to perform the kinds of tasks that we're just starting to scratch the surface to

**[15:45]** just starting to scratch the surface to

**[15:45]** just starting to scratch the surface to solve new problems that humans cannot to

**[15:47]** solve new problems that humans cannot to

**[15:47]** solve new problems that humans cannot to be be assistive to us in our daily lives

**[15:50]** be be assistive to us in our daily lives

**[15:50]** be be assistive to us in our daily lives to not have to you know be typing with

**[15:52]** to not have to you know be typing with

**[15:52]** to not have to you know be typing with our with our you know meat sticks but

**[15:53]** our with our you know meat sticks but

**[15:53]** our with our you know meat sticks but instead to have something that you can

**[15:55]** instead to have something that you can

**[15:55]** instead to have something that you can interact with just like a person where

**[15:56]** interact with just like a person where

**[15:56]** interact with just like a person where the machine comes much closer to you

**[15:58]** the machine comes much closer to you

**[15:58]** the machine comes much closer to you rather than you closer to it and having


### [16:00 - 17:00]

**[16:00]** rather than you closer to it and having

**[16:00]** rather than you closer to it and having to learn assembly language or you know

**[16:01]** to learn assembly language or you know

**[16:01]** to learn assembly language or you know whatever it is. Um and so to me it felt

**[16:04]** whatever it is. Um and so to me it felt

**[16:04]** whatever it is. Um and so to me it felt like all of the factors were lined up

**[16:06]** like all of the factors were lined up

**[16:06]** like all of the factors were lined up and now we just need to build. Yeah. Um

**[16:09]** and now we just need to build. Yeah. Um

**[16:09]** and now we just need to build. Yeah. Um I I like that consistent theme that you

**[16:11]** I I like that consistent theme that you

**[16:11]** I I like that consistent theme that you keep coming back to. We just need to

**[16:12]** keep coming back to. We just need to

**[16:12]** keep coming back to. We just need to build. Um so in 2022 you wrote that it's

**[16:16]** build. Um so in 2022 you wrote that it's

**[16:16]** build. Um so in 2022 you wrote that it's time to be an ML engineer. Actually I

**[16:17]** time to be an ML engineer. Actually I

**[16:17]** time to be an ML engineer. Actually I have a personal friend uh who read that

**[16:19]** have a personal friend uh who read that

**[16:19]** have a personal friend uh who read that post and cold emailed you and joined

**[16:20]** post and cold emailed you and joined

**[16:20]** post and cold emailed you and joined OpenAI and all that. Um you said that

**[16:23]** OpenAI and all that. Um you said that

**[16:23]** OpenAI and all that. Um you said that great engineers are able to contribute

**[16:24]** great engineers are able to contribute

**[16:24]** great engineers are able to contribute at the same level as great researchers

**[16:25]** at the same level as great researchers

**[16:25]** at the same level as great researchers to future progress. Is that uh is that

**[16:28]** to future progress. Is that uh is that

**[16:28]** to future progress. Is that uh is that still true today? You know, I think a

**[16:30]** still true today? You know, I think a

**[16:30]** still true today? You know, I think a lot of engineers look at the researchers

**[16:31]** lot of engineers look at the researchers

**[16:31]** lot of engineers look at the researchers who are making millions of dollars and

**[16:33]** who are making millions of dollars and

**[16:33]** who are making millions of dollars and they're like, how do I contribute as

**[16:36]** they're like, how do I contribute as

**[16:36]** they're like, how do I contribute as much? You know, I I think it's

**[16:38]** much? You know, I I think it's

**[16:38]** much? You know, I I think it's absolutely if not even more true. Um I

**[16:41]** absolutely if not even more true. Um I

**[16:41]** absolutely if not even more true. Um I think that like if you look at the

**[16:42]** think that like if you look at the

**[16:42]** think that like if you look at the phases of deep learning research since

**[16:45]** phases of deep learning research since

**[16:45]** phases of deep learning research since 2012, I think at the beginning it really

**[16:47]** 2012, I think at the beginning it really

**[16:47]** 2012, I think at the beginning it really was um and this is kind of what I

**[16:49]** was um and this is kind of what I

**[16:49]** was um and this is kind of what I expected when we started OpenAI, you

**[16:50]** expected when we started OpenAI, you

**[16:50]** expected when we started OpenAI, you know, just like research scientists who

**[16:52]** know, just like research scientists who

**[16:52]** know, just like research scientists who had gotten a PhD who would go and kind

**[16:54]** had gotten a PhD who would go and kind

**[16:54]** had gotten a PhD who would go and kind of come up with ideas and test them out.

**[16:56]** of come up with ideas and test them out.

**[16:56]** of come up with ideas and test them out. And you know there's there's engineering

**[16:57]** And you know there's there's engineering

**[16:58]** And you know there's there's engineering to be done. If you actually look at

**[16:59]** to be done. If you actually look at

**[16:59]** to be done. If you actually look at Alexet itself, you know, it's


### [17:00 - 18:00]

**[17:00]** Alexet itself, you know, it's

**[17:00]** Alexet itself, you know, it's fundamentally the engineering of let's

**[17:03]** fundamentally the engineering of let's

**[17:03]** fundamentally the engineering of let's get fast convolutional kernels on a GPU.

**[17:05]** get fast convolutional kernels on a GPU.

**[17:05]** get fast convolutional kernels on a GPU. Um and and uh fun fun fact is people who

**[17:07]** Um and and uh fun fun fact is people who

**[17:08]** Um and and uh fun fun fact is people who were in the lab with Alexi at the time

**[17:10]** were in the lab with Alexi at the time

**[17:10]** were in the lab with Alexi at the time uh were actually felt very bad for him

**[17:12]** uh were actually felt very bad for him

**[17:12]** uh were actually felt very bad for him because they were like he has some fast

**[17:14]** because they were like he has some fast

**[17:14]** because they were like he has some fast com kernels for uh uh for you know some

**[17:17]** com kernels for uh uh for you know some

**[17:17]** com kernels for uh uh for you know some some image data set that doesn't really

**[17:20]** some image data set that doesn't really

**[17:20]** some image data set that doesn't really matter but you know Ilia was like well

**[17:22]** matter but you know Ilia was like well

**[17:22]** matter but you know Ilia was like well clearly we just need to apply this to

**[17:23]** clearly we just need to apply this to

**[17:23]** clearly we just need to apply this to imageet. It's going to be great right?

**[17:24]** imageet. It's going to be great right?

**[17:24]** imageet. It's going to be great right? Right? So it's like the combination of

**[17:26]** Right? So it's like the combination of

**[17:26]** Right? So it's like the combination of great engineering together with the idea

**[17:29]** great engineering together with the idea

**[17:29]** great engineering together with the idea of what to do with it, right? That

**[17:30]** of what to do with it, right? That

**[17:30]** of what to do with it, right? That that's what what makes the magic work.

**[17:33]** that's what what makes the magic work.

**[17:33]** that's what what makes the magic work. Um and uh the thing that I think is

**[17:38]** Um and uh the thing that I think is

**[17:38]** Um and uh the thing that I think is still true and even more true is okay,

**[17:40]** still true and even more true is okay,

**[17:40]** still true and even more true is okay, so the engineering required, it's now

**[17:43]** so the engineering required, it's now

**[17:43]** so the engineering required, it's now not just let's build some kernels, but

**[17:45]** not just let's build some kernels, but

**[17:45]** not just let's build some kernels, but let's build a system. Let's actually

**[17:46]** let's build a system. Let's actually

**[17:46]** let's build a system. Let's actually scale to 100,000 GPUs. Let's actually,

**[17:49]** scale to 100,000 GPUs. Let's actually,

**[17:49]** scale to 100,000 GPUs. Let's actually, you know, sort of do this crazy RL

**[17:51]** you know, sort of do this crazy RL

**[17:51]** you know, sort of do this crazy RL system that orchestrates things in all

**[17:52]** system that orchestrates things in all

**[17:52]** system that orchestrates things in all sorts of ways. Um, so the idea, if you

**[17:55]** sorts of ways. Um, so the idea, if you

**[17:55]** sorts of ways. Um, so the idea, if you don't have the idea, you're dead in the

**[17:57]** don't have the idea, you're dead in the

**[17:57]** don't have the idea, you're dead in the water. There's nothing to do. But if you

**[17:59]** water. There's nothing to do. But if you

**[17:59]** water. There's nothing to do. But if you don't have the engineering, that idea is


### [18:00 - 19:00]

**[18:01]** don't have the engineering, that idea is

**[18:01]** don't have the engineering, that idea is not going to it's not going to live and

**[18:02]** not going to it's not going to live and

**[18:02]** not going to it's not going to live and see the light of day. And so you need to

**[18:04]** see the light of day. And so you need to

**[18:04]** see the light of day. And so you need to have both of these coming together

**[18:05]** have both of these coming together

**[18:05]** have both of these coming together harmoniously. Yeah. I think that Ilia

**[18:07]** harmoniously. Yeah. I think that Ilia

**[18:07]** harmoniously. Yeah. I think that Ilia Alex relationship is really emblematic

**[18:10]** Alex relationship is really emblematic

**[18:10]** Alex relationship is really emblematic of like the research engineering

**[18:12]** of like the research engineering

**[18:12]** of like the research engineering partnership that now is the philosophy

**[18:14]** partnership that now is the philosophy

**[18:14]** partnership that now is the philosophy at OpenAI. That's right. Yeah. Yeah. And

**[18:16]** at OpenAI. That's right. Yeah. Yeah. And

**[18:16]** at OpenAI. That's right. Yeah. Yeah. And if you look at how open AI operates like

**[18:18]** if you look at how open AI operates like

**[18:18]** if you look at how open AI operates like I think from the very beginning we had

**[18:19]** I think from the very beginning we had

**[18:19]** I think from the very beginning we had this ethos of engineering and research

**[18:21]** this ethos of engineering and research

**[18:21]** this ethos of engineering and research be valued um and and work together um as

**[18:25]** be valued um and and work together um as

**[18:25]** be valued um and and work together um as partners and I think that that is

**[18:26]** partners and I think that that is

**[18:26]** partners and I think that that is something that we you know it's like

**[18:28]** something that we you know it's like

**[18:28]** something that we you know it's like something that we we really work at

**[18:29]** something that we we really work at

**[18:29]** something that we we really work at every day. Yeah. Uh it's my explicit

**[18:31]** every day. Yeah. Uh it's my explicit

**[18:31]** every day. Yeah. Uh it's my explicit goal to try to throw uh curveballs in

**[18:35]** goal to try to throw uh curveballs in

**[18:35]** goal to try to throw uh curveballs in this in this stuff. So uh in terms of

**[18:37]** this in this stuff. So uh in terms of

**[18:37]** this in this stuff. So uh in terms of the relationship between engineering and

**[18:38]** the relationship between engineering and

**[18:38]** the relationship between engineering and research, what did OpenAI do wrong in

**[18:41]** research, what did OpenAI do wrong in

**[18:41]** research, what did OpenAI do wrong in the early days that you do well now? Um

**[18:44]** the early days that you do well now? Um

**[18:44]** the early days that you do well now? Um well I think that the relationship

**[18:46]** well I think that the relationship

**[18:46]** well I think that the relationship between engineering and research the way

**[18:47]** between engineering and research the way

**[18:47]** between engineering and research the way I think about it is you never fully

**[18:49]** I think about it is you never fully

**[18:49]** I think about it is you never fully solve it right you just sort of solve

**[18:51]** solve it right you just sort of solve

**[18:51]** solve it right you just sort of solve the current level of problem and then

**[18:53]** the current level of problem and then

**[18:53]** the current level of problem and then you move on to the next level of

**[18:54]** you move on to the next level of

**[18:54]** you move on to the next level of sophistication and I noticed that

**[18:55]** sophistication and I noticed that

**[18:56]** sophistication and I noticed that actually the kinds of problems that we

**[18:57]** actually the kinds of problems that we

**[18:57]** actually the kinds of problems that we ran into were basically the same

**[18:59]** ran into were basically the same

**[18:59]** ran into were basically the same problems that had been run into at every


### [19:00 - 20:00]

**[19:00]** problems that had been run into at every

**[19:00]** problems that had been run into at every other lab and it was just like you know

**[19:02]** other lab and it was just like you know

**[19:02]** other lab and it was just like you know either we would be further along or that

**[19:04]** either we would be further along or that

**[19:04]** either we would be further along or that there' be a slightly different variant

**[19:05]** there' be a slightly different variant

**[19:05]** there' be a slightly different variant of it and so I think there's something

**[19:06]** of it and so I think there's something

**[19:06]** of it and so I think there's something deeply fundamental about this um so the

**[19:09]** deeply fundamental about this um so the

**[19:09]** deeply fundamental about this um so the the ve at the very beginning I could

**[19:11]** the ve at the very beginning I could

**[19:11]** the ve at the very beginning I could really see people who came from the

**[19:13]** really see people who came from the

**[19:13]** really see people who came from the engineering world, people came from the

**[19:14]** engineering world, people came from the

**[19:14]** engineering world, people came from the research world, just sort of thinking

**[19:16]** research world, just sort of thinking

**[19:16]** research world, just sort of thinking about system constraints very

**[19:17]** about system constraints very

**[19:17]** about system constraints very differently. And so as an engineer,

**[19:19]** differently. And so as an engineer,

**[19:19]** differently. And so as an engineer, you're like, hey, if I've got an

**[19:20]** you're like, hey, if I've got an

**[19:20]** you're like, hey, if I've got an interface, you should not care what's

**[19:21]** interface, you should not care what's

**[19:21]** interface, you should not care what's behind that interface. We agreed on the

**[19:23]** behind that interface. We agreed on the

**[19:23]** behind that interface. We agreed on the interface, I can implement however I

**[19:24]** interface, I can implement however I

**[19:24]** interface, I can implement however I want. Whereas if you're a researcher,

**[19:25]** want. Whereas if you're a researcher,

**[19:26]** want. Whereas if you're a researcher, you're like, if there's a bug anywhere

**[19:27]** you're like, if there's a bug anywhere

**[19:27]** you're like, if there's a bug anywhere in the system, all I'm going to get is

**[19:29]** in the system, all I'm going to get is

**[19:29]** in the system, all I'm going to get is just slightly degraded performance. Not

**[19:31]** just slightly degraded performance. Not

**[19:31]** just slightly degraded performance. Not going to get an exception, not going to

**[19:32]** going to get an exception, not going to

**[19:32]** going to get an exception, not going to get indications of where. And so I am

**[19:35]** get indications of where. And so I am

**[19:35]** get indications of where. And so I am responsible for understanding

**[19:37]** responsible for understanding

**[19:37]** responsible for understanding everything. the interfaces they don't

**[19:38]** everything. the interfaces they don't

**[19:38]** everything. the interfaces they don't matter unless they're like truly rock

**[19:40]** matter unless they're like truly rock

**[19:40]** matter unless they're like truly rock solid and I can just like never think

**[19:42]** solid and I can just like never think

**[19:42]** solid and I can just like never think about it which is a pretty high bar. um

**[19:45]** about it which is a pretty high bar. um

**[19:45]** about it which is a pretty high bar. um then I am actually responsible for for

**[19:47]** then I am actually responsible for for

**[19:47]** then I am actually responsible for for this code and that causes friction right

**[19:49]** this code and that causes friction right

**[19:49]** this code and that causes friction right because then how do you actually work

**[19:50]** because then how do you actually work

**[19:50]** because then how do you actually work together and I saw a project very early

**[19:52]** together and I saw a project very early

**[19:52]** together and I saw a project very early on where that you know the the people

**[19:55]** on where that you know the the people

**[19:55]** on where that you know the the people from the engineering background would

**[19:56]** from the engineering background would

**[19:56]** from the engineering background would write the code and then there'd be this

**[19:57]** write the code and then there'd be this

**[19:57]** write the code and then there'd be this big debate over every single line and I


### [20:00 - 21:00]

**[20:00]** big debate over every single line and I

**[20:00]** big debate over every single line and I was just like this is never going to

**[20:01]** was just like this is never going to

**[20:01]** was just like this is never going to move it's going to be so slow and

**[20:03]** move it's going to be so slow and

**[20:03]** move it's going to be so slow and instead the way that we ended up

**[20:05]** instead the way that we ended up

**[20:05]** instead the way that we ended up proceeding was um so I actually worked

**[20:07]** proceeding was um so I actually worked

**[20:07]** proceeding was um so I actually worked on that directly and I'd come up with

**[20:08]** on that directly and I'd come up with

**[20:08]** on that directly and I'd come up with like five ideas at a time someone from

**[20:10]** like five ideas at a time someone from

**[20:10]** like five ideas at a time someone from the research side would say these four

**[20:11]** the research side would say these four

**[20:11]** the research side would say these four are bad I'd be like great that's all I

**[20:13]** are bad I'd be like great that's all I

**[20:13]** are bad I'd be like great that's all I wanted Right. And so the value that I

**[20:16]** wanted Right. And so the value that I

**[20:16]** wanted Right. And so the value that I think we've really realized is critical

**[20:17]** think we've really realized is critical

**[20:17]** think we've really realized is critical and that I tell people from from the

**[20:19]** and that I tell people from from the

**[20:20]** and that I tell people from from the engineering world coming into OpenAI um

**[20:22]** engineering world coming into OpenAI um

**[20:22]** engineering world coming into OpenAI um is technical humility. Right? It's like

**[20:24]** is technical humility. Right? It's like

**[20:24]** is technical humility. Right? It's like you're coming in because you have skills

**[20:26]** you're coming in because you have skills

**[20:26]** you're coming in because you have skills that are important. But it's a totally

**[20:28]** that are important. But it's a totally

**[20:28]** that are important. But it's a totally different environment from you know

**[20:29]** different environment from you know

**[20:29]** different environment from you know something like a traditional web startup

**[20:32]** something like a traditional web startup

**[20:32]** something like a traditional web startup and figuring out when those intuitions

**[20:34]** and figuring out when those intuitions

**[20:34]** and figuring out when those intuitions apply and figuring out like when to

**[20:37]** apply and figuring out like when to

**[20:37]** apply and figuring out like when to leave them at the door is super hard.

**[20:39]** leave them at the door is super hard.

**[20:39]** leave them at the door is super hard. And so the most important thing is to

**[20:40]** And so the most important thing is to

**[20:40]** And so the most important thing is to like come in really really listen and

**[20:43]** like come in really really listen and

**[20:43]** like come in really really listen and kind of assume that that that there's

**[20:45]** kind of assume that that that there's

**[20:45]** kind of assume that that that there's something that you're missing until you

**[20:46]** something that you're missing until you

**[20:46]** something that you're missing until you deeply understand the why and then at

**[20:48]** deeply understand the why and then at

**[20:48]** deeply understand the why and then at that point great make the change like

**[20:50]** that point great make the change like

**[20:50]** that point great make the change like change the the the architecture change

**[20:52]** change the the the architecture change

**[20:52]** change the the the architecture change the abstractions. Um but I think that

**[20:54]** the abstractions. Um but I think that

**[20:54]** the abstractions. Um but I think that that kind of approach of just really

**[20:57]** that kind of approach of just really

**[20:57]** that kind of approach of just really really read and listen and understand

**[20:59]** really read and listen and understand

**[20:59]** really read and listen and understand with that humility um that that is I


### [21:00 - 22:00]

**[21:01]** with that humility um that that is I

**[21:02]** with that humility um that that is I think a really key determiner. Yeah.

**[21:03]** think a really key determiner. Yeah.

**[21:03]** think a really key determiner. Yeah. Awesome. Um we're going to tell some

**[21:05]** Awesome. Um we're going to tell some

**[21:05]** Awesome. Um we're going to tell some stories from recent launches of OpenAI,

**[21:08]** stories from recent launches of OpenAI,

**[21:08]** stories from recent launches of OpenAI, the greatest hits. Uh so one of the

**[21:10]** the greatest hits. Uh so one of the

**[21:10]** the greatest hits. Uh so one of the things that is is kind of interesting is

**[21:12]** things that is is kind of interesting is

**[21:12]** things that is is kind of interesting is just scaling in general. Everything

**[21:14]** just scaling in general. Everything

**[21:14]** just scaling in general. Everything breaks at different orders of magnitude.

**[21:15]** breaks at different orders of magnitude.

**[21:16]** breaks at different orders of magnitude. So in when chatbt launched you got a

**[21:17]** So in when chatbt launched you got a

**[21:18]** So in when chatbt launched you got a million users in 5 days. This year when

**[21:20]** million users in 5 days. This year when

**[21:20]** million users in 5 days. This year when 40 IG gen launched, you got 100 million

**[21:22]** 40 IG gen launched, you got 100 million

**[21:22]** 40 IG gen launched, you got 100 million users in 5 days. How do those two

**[21:25]** users in 5 days. How do those two

**[21:25]** users in 5 days. How do those two periods compare? Uh they echo very

**[21:29]** periods compare? Uh they echo very

**[21:29]** periods compare? Uh they echo very similarly in a lot of ways. You know,

**[21:31]** similarly in a lot of ways. You know,

**[21:31]** similarly in a lot of ways. You know, the thing about chatbt, uh, it was

**[21:33]** the thing about chatbt, uh, it was

**[21:33]** the thing about chatbt, uh, it was supposed to be a low-key research

**[21:35]** supposed to be a low-key research

**[21:35]** supposed to be a low-key research preview and we put it out very, you

**[21:38]** preview and we put it out very, you

**[21:38]** preview and we put it out very, you know, sort of chilly and then suddenly

**[21:41]** know, sort of chilly and then suddenly

**[21:41]** know, sort of chilly and then suddenly everything was down and we, you know, we

**[21:45]** everything was down and we, you know, we

**[21:45]** everything was down and we, you know, we kind of anticipated that chat GBT would

**[21:47]** kind of anticipated that chat GBT would

**[21:47]** kind of anticipated that chat GBT would be a very popular thing, but we thought

**[21:48]** be a very popular thing, but we thought

**[21:48]** be a very popular thing, but we thought that GPT4 would be necessary to get it.

**[21:51]** that GPT4 would be necessary to get it.

**[21:51]** that GPT4 would be necessary to get it. Had it internally as well, so you just

**[21:52]** Had it internally as well, so you just

**[21:52]** Had it internally as well, so you just weren't impressed by Exactly. Right.

**[21:54]** weren't impressed by Exactly. Right.

**[21:54]** weren't impressed by Exactly. Right. It's like you, that's the other thing

**[21:55]** It's like you, that's the other thing

**[21:55]** It's like you, that's the other thing about this field is you update so

**[21:56]** about this field is you update so

**[21:56]** about this field is you update so quickly, right? It's like you see magic

**[21:58]** quickly, right? It's like you see magic

**[21:58]** quickly, right? It's like you see magic and you're like this is the most amazing


### [22:00 - 23:00]

**[22:00]** and you're like this is the most amazing

**[22:00]** and you're like this is the most amazing thing I've ever seen and then you're

**[22:01]** thing I've ever seen and then you're

**[22:01]** thing I've ever seen and then you're like well why can't it like you know why

**[22:03]** like well why can't it like you know why

**[22:03]** like well why can't it like you know why why can't it like merge you know 10 PRs

**[22:05]** why can't it like merge you know 10 PRs

**[22:05]** why can't it like merge you know 10 PRs for me. Exactly. Um and the image gen

**[22:08]** for me. Exactly. Um and the image gen

**[22:08]** for me. Exactly. Um and the image gen moment was very similar in terms of it

**[22:12]** moment was very similar in terms of it

**[22:12]** moment was very similar in terms of it was just so so loved and so popular and

**[22:16]** was just so so loved and so popular and

**[22:16]** was just so so loved and so popular and it just went viral in in ways that you

**[22:18]** it just went viral in in ways that you

**[22:18]** it just went viral in in ways that you know just like the numbers were just off

**[22:20]** know just like the numbers were just off

**[22:20]** know just like the numbers were just off the charts. And so internally we

**[22:22]** the charts. And so internally we

**[22:22]** the charts. And so internally we actually did something that we really

**[22:23]** actually did something that we really

**[22:24]** actually did something that we really really try not to do um which is we

**[22:25]** really try not to do um which is we

**[22:26]** really try not to do um which is we pulled a bunch of compute from research

**[22:27]** pulled a bunch of compute from research

**[22:27]** pulled a bunch of compute from research for both of these launches actually um

**[22:29]** for both of these launches actually um

**[22:29]** for both of these launches actually um because that's mortgaging the future um

**[22:31]** because that's mortgaging the future um

**[22:31]** because that's mortgaging the future um to make make the system work um but if

**[22:34]** to make make the system work um but if

**[22:34]** to make make the system work um but if you can actually deliver and keep up

**[22:36]** you can actually deliver and keep up

**[22:36]** you can actually deliver and keep up with demand then of course people get to

**[22:37]** with demand then of course people get to

**[22:37]** with demand then of course people get to experience the magic and I think that um

**[22:39]** experience the magic and I think that um

**[22:39]** experience the magic and I think that um that that that's something that is

**[22:40]** that that that's something that is

**[22:40]** that that that's something that is really worthwhile and it's really

**[22:42]** really worthwhile and it's really

**[22:42]** really worthwhile and it's really important to sort of you know maximize

**[22:44]** important to sort of you know maximize

**[22:44]** important to sort of you know maximize those moments. Um, so I think that that

**[22:46]** those moments. Um, so I think that that

**[22:46]** those moments. Um, so I think that that that we really have that same ethos of

**[22:49]** that we really have that same ethos of

**[22:49]** that we really have that same ethos of really serving the user, really trying

**[22:51]** really serving the user, really trying

**[22:51]** really serving the user, really trying to push for the technology and just do

**[22:53]** to push for the technology and just do

**[22:53]** to push for the technology and just do things that are materially new that no

**[22:55]** things that are materially new that no

**[22:55]** things that are materially new that no one's ever seen before. Um, and then

**[22:57]** one's ever seen before. Um, and then

**[22:57]** one's ever seen before. Um, and then whatever it takes to get those out into

**[22:58]** whatever it takes to get those out into

**[22:58]** whatever it takes to get those out into the world and make those successful that


### [23:00 - 24:00]

**[23:00]** the world and make those successful that

**[23:00]** the world and make those successful that that's what we do. Amazing. Um, well, I

**[23:02]** that's what we do. Amazing. Um, well, I

**[23:02]** that's what we do. Amazing. Um, well, I mean, incredible job. U GPT4 launch. So

**[23:06]** mean, incredible job. U GPT4 launch. So

**[23:06]** mean, incredible job. U GPT4 launch. So I am told your wife drew the joke

**[23:08]** I am told your wife drew the joke

**[23:08]** I am told your wife drew the joke website. That's true. Yeah. Fun fun fun

**[23:11]** website. That's true. Yeah. Fun fun fun

**[23:12]** website. That's true. Yeah. Fun fun fun Easter egg. My handwriting was so bad uh

**[23:14]** Easter egg. My handwriting was so bad uh

**[23:14]** Easter egg. My handwriting was so bad uh that even our AI couldn't tell what to

**[23:16]** that even our AI couldn't tell what to

**[23:16]** that even our AI couldn't tell what to do with it. Um so like uh apparently did

**[23:18]** do with it. Um so like uh apparently did

**[23:18]** do with it. Um so like uh apparently did you improvise some of this? I I I heard

**[23:21]** you improvise some of this? I I I heard

**[23:21]** you improvise some of this? I I I heard I gravine. Yeah, definitely. Definitely

**[23:24]** I gravine. Yeah, definitely. Definitely

**[23:24]** I gravine. Yeah, definitely. Definitely like you know usually when I when I do

**[23:26]** like you know usually when I when I do

**[23:26]** like you know usually when I when I do these kinds of demos like I've tested

**[23:27]** these kinds of demos like I've tested

**[23:27]** these kinds of demos like I've tested the general shape of them ahead of time.

**[23:29]** the general shape of them ahead of time.

**[23:29]** the general shape of them ahead of time. Uh but I've always had like it's very

**[23:31]** Uh but I've always had like it's very

**[23:31]** Uh but I've always had like it's very easy in this field to have ones that are

**[23:32]** easy in this field to have ones that are

**[23:32]** easy in this field to have ones that are just like if you slightly typo a

**[23:34]** just like if you slightly typo a

**[23:34]** just like if you slightly typo a character or something then the demo

**[23:35]** character or something then the demo

**[23:35]** character or something then the demo will not work. I don't like doing those.

**[23:37]** will not work. I don't like doing those.

**[23:37]** will not work. I don't like doing those. I like to have some robustness to it. So

**[23:39]** I like to have some robustness to it. So

**[23:39]** I like to have some robustness to it. So there's always variation in terms of of

**[23:40]** there's always variation in terms of of

**[23:40]** there's always variation in terms of of what actually ends up get being shown.

**[23:42]** what actually ends up get being shown.

**[23:42]** what actually ends up get being shown. To me, this was the first time I think

**[23:45]** To me, this was the first time I think

**[23:45]** To me, this was the first time I think the world ever saw vibe coding. Um, it

**[23:47]** the world ever saw vibe coding. Um, it

**[23:47]** the world ever saw vibe coding. Um, it is now a thing. What are your thoughts

**[23:49]** is now a thing. What are your thoughts

**[23:49]** is now a thing. What are your thoughts on vibe coding? Uh, well, I think that

**[23:52]** on vibe coding? Uh, well, I think that

**[23:52]** on vibe coding? Uh, well, I think that vibe coding is amazing as an empowerment

**[23:56]** vibe coding is amazing as an empowerment

**[23:56]** vibe coding is amazing as an empowerment mechanism, right? I think it's sort of a

**[23:58]** mechanism, right? I think it's sort of a

**[23:58]** mechanism, right? I think it's sort of a representation of what is to come. And I


### [24:00 - 25:00]

**[24:00]** representation of what is to come. And I

**[24:00]** representation of what is to come. And I think that the specifics of what vibe

**[24:02]** think that the specifics of what vibe

**[24:02]** think that the specifics of what vibe coding is, I think that's going to

**[24:04]** coding is, I think that's going to

**[24:04]** coding is, I think that's going to change over time, right? I think that

**[24:06]** change over time, right? I think that

**[24:06]** change over time, right? I think that you look at even things like codeex like

**[24:07]** you look at even things like codeex like

**[24:07]** you look at even things like codeex like to some extent I think our vision is

**[24:10]** to some extent I think our vision is

**[24:10]** to some extent I think our vision is that as you start to have agents that

**[24:13]** that as you start to have agents that

**[24:13]** that as you start to have agents that really work that you can have not just

**[24:15]** really work that you can have not just

**[24:15]** really work that you can have not just one copy not just 10 copies but you can

**[24:17]** one copy not just 10 copies but you can

**[24:17]** one copy not just 10 copies but you can have a hundred or thousand or 10,000 or

**[24:19]** have a hundred or thousand or 10,000 or

**[24:19]** have a hundred or thousand or 10,000 or 100 thousand of these things running

**[24:22]** 100 thousand of these things running

**[24:22]** 100 thousand of these things running you're going to want to treat them much

**[24:23]** you're going to want to treat them much

**[24:23]** you're going to want to treat them much more like a co-orker right that you're

**[24:24]** more like a co-orker right that you're

**[24:24]** more like a co-orker right that you're going to want them off in the cloud

**[24:25]** going to want them off in the cloud

**[24:25]** going to want them off in the cloud doing stuff being able to hook hook up

**[24:27]** doing stuff being able to hook hook up

**[24:27]** doing stuff being able to hook hook up to all sorts of things you're asleep

**[24:28]** to all sorts of things you're asleep

**[24:28]** to all sorts of things you're asleep your laptop's closed it should still be

**[24:30]** your laptop's closed it should still be

**[24:30]** your laptop's closed it should still be working um I think that that the the you

**[24:32]** working um I think that that the the you

**[24:32]** working um I think that that the the you know current conception of of vibe

**[24:34]** know current conception of of vibe

**[24:34]** know current conception of of vibe coding in an interactive loop. Um, you

**[24:36]** coding in an interactive loop. Um, you

**[24:36]** coding in an interactive loop. Um, you know, that that's something that I I

**[24:38]** know, that that's something that I I

**[24:38]** know, that that's something that I I think is like, you know, it's it's I

**[24:41]** think is like, you know, it's it's I

**[24:41]** think is like, you know, it's it's I Okay, so my my prediction of what will

**[24:42]** Okay, so my my prediction of what will

**[24:42]** Okay, so my my prediction of what will happen is like I think there's going to

**[24:43]** happen is like I think there's going to

**[24:43]** happen is like I think there's going to be more and more of that happening, but

**[24:45]** be more and more of that happening, but

**[24:45]** be more and more of that happening, but I think that the agentic stuff is going

**[24:46]** I think that the agentic stuff is going

**[24:46]** I think that the agentic stuff is going to also really intercept and overtake.

**[24:48]** to also really intercept and overtake.

**[24:48]** to also really intercept and overtake. And I think that all of this is just

**[24:50]** And I think that all of this is just

**[24:50]** And I think that all of this is just going to result in just way more systems

**[24:52]** going to result in just way more systems

**[24:52]** going to result in just way more systems being built. Um, and the thing that that

**[24:55]** being built. Um, and the thing that that

**[24:55]** being built. Um, and the thing that that I think is also very interesting is that

**[24:57]** I think is also very interesting is that

**[24:57]** I think is also very interesting is that a lot of the vibe coding kind of demos

**[24:59]** a lot of the vibe coding kind of demos

**[24:59]** a lot of the vibe coding kind of demos and and the cool the cool flashy stuff.


### [25:00 - 26:00]

**[25:01]** and and the cool the cool flashy stuff.

**[25:01]** and and the cool the cool flashy stuff. Um, for example, make making the joke

**[25:03]** Um, for example, make making the joke

**[25:03]** Um, for example, make making the joke website, it's making an app from

**[25:05]** website, it's making an app from

**[25:05]** website, it's making an app from scratch. But the thing that I think will

**[25:08]** scratch. But the thing that I think will

**[25:08]** scratch. But the thing that I think will really be new and transformative and is

**[25:10]** really be new and transformative and is

**[25:10]** really be new and transformative and is starting to really happen is being able

**[25:12]** starting to really happen is being able

**[25:12]** starting to really happen is being able to transform existing applications to go

**[25:15]** to transform existing applications to go

**[25:15]** to transform existing applications to go deeper. Um, and that be able to, you

**[25:17]** deeper. Um, and that be able to, you

**[25:17]** deeper. Um, and that be able to, you know, like I think so many companies are

**[25:19]** know, like I think so many companies are

**[25:19]** know, like I think so many companies are sitting on legacy code bases and doing

**[25:21]** sitting on legacy code bases and doing

**[25:21]** sitting on legacy code bases and doing migrations and updating libraries and

**[25:23]** migrations and updating libraries and

**[25:24]** migrations and updating libraries and changing your cobalt language to

**[25:25]** changing your cobalt language to

**[25:25]** changing your cobalt language to something else is so hard and is

**[25:28]** something else is so hard and is

**[25:28]** something else is so hard and is actually just not very fun for humans.

**[25:30]** actually just not very fun for humans.

**[25:30]** actually just not very fun for humans. And uh, I think we're starting to get AI

**[25:32]** And uh, I think we're starting to get AI

**[25:32]** And uh, I think we're starting to get AI that are able to really tackle those

**[25:34]** that are able to really tackle those

**[25:34]** that are able to really tackle those problems. And so the thing that I love

**[25:35]** problems. And so the thing that I love

**[25:35]** problems. And so the thing that I love about where Vibe coding started has

**[25:37]** about where Vibe coding started has

**[25:37]** about where Vibe coding started has really been like with the most like just

**[25:39]** really been like with the most like just

**[25:39]** really been like with the most like just like make cool apps kind of thing. And

**[25:41]** like make cool apps kind of thing. And

**[25:41]** like make cool apps kind of thing. And it's starting to become much more like

**[25:42]** it's starting to become much more like

**[25:42]** it's starting to become much more like serious software engineering. And I

**[25:44]** serious software engineering. And I

**[25:44]** serious software engineering. And I think that going even deeper to just

**[25:45]** think that going even deeper to just

**[25:45]** think that going even deeper to just like making it possible to just move so

**[25:49]** like making it possible to just move so

**[25:49]** like making it possible to just move so much faster as a company. Um that's I

**[25:51]** much faster as a company. Um that's I

**[25:51]** much faster as a company. Um that's I think where where we're headed. Yep. Uh

**[25:53]** think where where we're headed. Yep. Uh

**[25:53]** think where where we're headed. Yep. Uh speaking of codeex, I've heard that

**[25:55]** speaking of codeex, I've heard that

**[25:55]** speaking of codeex, I've heard that you've just it's kind of your baby a

**[25:57]** you've just it's kind of your baby a

**[25:57]** you've just it's kind of your baby a little bit. Um and you've started I


### [26:00 - 27:00]

**[26:00]** little bit. Um and you've started I

**[26:00]** little bit. Um and you've started I think on the live stream you were

**[26:01]** think on the live stream you were

**[26:01]** think on the live stream you were talking a lot about just make things

**[26:02]** talking a lot about just make things

**[26:02]** talking a lot about just make things modular and well doumented and all that

**[26:04]** modular and well doumented and all that

**[26:04]** modular and well doumented and all that good stuff. Like how do you think codeex

**[26:08]** good stuff. Like how do you think codeex

**[26:08]** good stuff. Like how do you think codeex changes the way that we code? Um well I

**[26:10]** changes the way that we code? Um well I

**[26:10]** changes the way that we code? Um well I definitely think that that it's an

**[26:11]** definitely think that that it's an

**[26:12]** definitely think that that it's an overstatement to say it's it's my baby.

**[26:13]** overstatement to say it's it's my baby.

**[26:13]** overstatement to say it's it's my baby. like I think that there's um a really

**[26:15]** like I think that there's um a really

**[26:15]** like I think that there's um a really incredible team um and and uh that you

**[26:17]** incredible team um and and uh that you

**[26:18]** incredible team um and and uh that you know I've I've been trying to support

**[26:19]** know I've I've been trying to support

**[26:19]** know I've I've been trying to support them and and and their vision and um but

**[26:22]** them and and and their vision and um but

**[26:22]** them and and and their vision and um but I think that that the direction is

**[26:23]** I think that that the direction is

**[26:23]** I think that that the direction is something that is like just so um so

**[26:27]** something that is like just so um so

**[26:27]** something that is like just so um so compelling and incredible to me. Um the

**[26:29]** compelling and incredible to me. Um the

**[26:29]** compelling and incredible to me. Um the way that that uh and sorry could you

**[26:31]** way that that uh and sorry could you

**[26:32]** way that that uh and sorry could you repeat the the how how does codeex

**[26:33]** repeat the the how how does codeex

**[26:33]** repeat the the how how does codeex change that we the way that we code? I

**[26:35]** change that we the way that we code? I

**[26:35]** change that we the way that we code? I see. Yeah. The thing that has been most

**[26:37]** see. Yeah. The thing that has been most

**[26:37]** see. Yeah. The thing that has been most interesting to see has been when you

**[26:40]** interesting to see has been when you

**[26:40]** interesting to see has been when you realize that the way you structure your

**[26:42]** realize that the way you structure your

**[26:42]** realize that the way you structure your codebase

**[26:43]** codebase

**[26:43]** codebase determines how much you can get out of

**[26:45]** determines how much you can get out of

**[26:45]** determines how much you can get out of codecs, right? That the if you match the

**[26:49]** codecs, right? That the if you match the

**[26:49]** codecs, right? That the if you match the strength of like basically all of our

**[26:50]** strength of like basically all of our

**[26:50]** strength of like basically all of our existing code bases are kind of matched

**[26:51]** existing code bases are kind of matched

**[26:51]** existing code bases are kind of matched to the strengths of humans. But if you

**[26:53]** to the strengths of humans. But if you

**[26:53]** to the strengths of humans. But if you match instead to the strengths of models

**[26:54]** match instead to the strengths of models

**[26:54]** match instead to the strengths of models which are sort of very lopsided, right?

**[26:56]** which are sort of very lopsided, right?

**[26:56]** which are sort of very lopsided, right? models are able to handle way more like

**[26:59]** models are able to handle way more like

**[26:59]** models are able to handle way more like diversity of stuff but also are not not


### [27:00 - 28:00]

**[27:01]** diversity of stuff but also are not not

**[27:01]** diversity of stuff but also are not not able to like sort of necessarily connect

**[27:03]** able to like sort of necessarily connect

**[27:03]** able to like sort of necessarily connect deep ideas as much as humans are right

**[27:05]** deep ideas as much as humans are right

**[27:05]** deep ideas as much as humans are right now. And so what you kind of want to do

**[27:08]** now. And so what you kind of want to do

**[27:08]** now. And so what you kind of want to do is make smaller modules that are well

**[27:12]** is make smaller modules that are well

**[27:12]** is make smaller modules that are well tested that have tests that can be run

**[27:14]** tested that have tests that can be run

**[27:14]** tested that have tests that can be run very quickly um and then fill in the

**[27:17]** very quickly um and then fill in the

**[27:17]** very quickly um and then fill in the details. the model will just do that

**[27:18]** details. the model will just do that

**[27:18]** details. the model will just do that right and it'll run the test itself and

**[27:21]** right and it'll run the test itself and

**[27:21]** right and it'll run the test itself and the connections between these different

**[27:23]** the connections between these different

**[27:23]** the connections between these different components kind of the architecture

**[27:24]** components kind of the architecture

**[27:24]** components kind of the architecture diagram like that's actually pretty easy

**[27:26]** diagram like that's actually pretty easy

**[27:26]** diagram like that's actually pretty easy to do and then it's the like filling out

**[27:28]** to do and then it's the like filling out

**[27:28]** to do and then it's the like filling out all the details that is often very

**[27:29]** all the details that is often very

**[27:30]** all the details that is often very difficult and if you if you actually do

**[27:32]** difficult and if you if you actually do

**[27:32]** difficult and if you if you actually do that you know what I described also

**[27:34]** that you know what I described also

**[27:34]** that you know what I described also sounds a lot like good software

**[27:35]** sounds a lot like good software

**[27:35]** sounds a lot like good software engineering practice um but it's just

**[27:37]** engineering practice um but it's just

**[27:37]** engineering practice um but it's just like sometimes because humans are are

**[27:39]** like sometimes because humans are are

**[27:39]** like sometimes because humans are are capable of holding more of this like

**[27:41]** capable of holding more of this like

**[27:41]** capable of holding more of this like conceptual abstraction in our head we

**[27:43]** conceptual abstraction in our head we

**[27:43]** conceptual abstraction in our head we just don't do it right that like yeah

**[27:45]** just don't do it right that like yeah

**[27:45]** just don't do it right that like yeah it's like you know it's a lot of work to

**[27:46]** it's like you know it's a lot of work to

**[27:46]** it's like you know it's a lot of work to write these tests and to you know to

**[27:48]** write these tests and to you know to

**[27:48]** write these tests and to you know to flesh them out and that you know the

**[27:51]** flesh them out and that you know the

**[27:51]** flesh them out and that you know the model's going to run like these tests

**[27:52]** model's going to run like these tests

**[27:52]** model's going to run like these tests like a hundred times or a thousand times

**[27:53]** like a hundred times or a thousand times

**[27:54]** like a hundred times or a thousand times more than you will and so it's going to

**[27:55]** more than you will and so it's going to

**[27:55]** more than you will and so it's going to care like way way more. So in some ways

**[27:58]** care like way way more. So in some ways

**[27:58]** care like way way more. So in some ways that the direction we want to go is

**[27:59]** that the direction we want to go is


### [28:00 - 29:00]

**[28:00]** that the direction we want to go is build our code bases for more junior

**[28:02]** build our code bases for more junior

**[28:02]** build our code bases for more junior developers um in order to actually get

**[28:04]** developers um in order to actually get

**[28:04]** developers um in order to actually get the most out of these models. Um, now

**[28:07]** the most out of these models. Um, now

**[28:07]** the most out of these models. Um, now it'll be very interesting to see as we

**[28:09]** it'll be very interesting to see as we

**[28:09]** it'll be very interesting to see as we increase the model capability, does this

**[28:11]** increase the model capability, does this

**[28:11]** increase the model capability, does this particular way of structuring code bases

**[28:14]** particular way of structuring code bases

**[28:14]** particular way of structuring code bases remain constant? And I kind of think

**[28:16]** remain constant? And I kind of think

**[28:16]** remain constant? And I kind of think that it's a pretty good idea because

**[28:18]** that it's a pretty good idea because

**[28:18]** that it's a pretty good idea because again, it starts to match what you

**[28:20]** again, it starts to match what you

**[28:20]** again, it starts to match what you should be doing for for maintainability

**[28:22]** should be doing for for maintainability

**[28:22]** should be doing for for maintainability for humans. Um, but yeah, I think that

**[28:24]** for humans. Um, but yeah, I think that

**[28:24]** for humans. Um, but yeah, I think that to me that the really sort of exciting

**[28:27]** to me that the really sort of exciting

**[28:27]** to me that the really sort of exciting thing to think about for the future of

**[28:28]** thing to think about for the future of

**[28:28]** thing to think about for the future of software engineering is what of our

**[28:31]** software engineering is what of our

**[28:31]** software engineering is what of our practices that we kind of just cut

**[28:33]** practices that we kind of just cut

**[28:33]** practices that we kind of just cut corners for do we actually really need

**[28:35]** corners for do we actually really need

**[28:35]** corners for do we actually really need to bring back in order to get the most

**[28:37]** to bring back in order to get the most

**[28:37]** to bring back in order to get the most out of our systems? Yeah. Um, can you

**[28:39]** out of our systems? Yeah. Um, can you

**[28:39]** out of our systems? Yeah. Um, can you put numbers on like ballpark numbers on

**[28:41]** put numbers on like ballpark numbers on

**[28:41]** put numbers on like ballpark numbers on the amount of productivity you guys are

**[28:43]** the amount of productivity you guys are

**[28:43]** the amount of productivity you guys are seeing with codecs internally? Um, I

**[28:46]** seeing with codecs internally? Um, I

**[28:46]** seeing with codecs internally? Um, I yeah, I don't know what the latest

**[28:47]** yeah, I don't know what the latest

**[28:47]** yeah, I don't know what the latest numbers are. I mean, there's definitely

**[28:48]** numbers are. I mean, there's definitely

**[28:48]** numbers are. I mean, there's definitely double digit percent of our of our PRs

**[28:51]** double digit percent of our of our PRs

**[28:51]** double digit percent of our of our PRs are written low low double digit um

**[28:54]** are written low low double digit um

**[28:54]** are written low low double digit um written entirely by codecs. Um and

**[28:56]** written entirely by codecs. Um and

**[28:56]** written entirely by codecs. Um and that's super cool to see. Um but it's

**[28:58]** that's super cool to see. Um but it's

**[28:58]** that's super cool to see. Um but it's also like you know that it's not the


### [29:00 - 30:00]

**[29:01]** also like you know that it's not the

**[29:01]** also like you know that it's not the only system that we use internally and I

**[29:03]** only system that we use internally and I

**[29:03]** only system that we use internally and I think that um to me it's it's still in

**[29:05]** think that um to me it's it's still in

**[29:05]** think that um to me it's it's still in the very very early days. Um it's been

**[29:07]** the very very early days. Um it's been

**[29:07]** the very very early days. Um it's been exciting to see some of the external

**[29:08]** exciting to see some of the external

**[29:08]** exciting to see some of the external metrics. Um like I think we had 24,000

**[29:11]** metrics. Um like I think we had 24,000

**[29:11]** metrics. Um like I think we had 24,000 uh PRs that were merged in like the last

**[29:14]** uh PRs that were merged in like the last

**[29:14]** uh PRs that were merged in like the last day uh in in public GitHub repositories.

**[29:17]** day uh in in public GitHub repositories.

**[29:17]** day uh in in public GitHub repositories. And so it's just like yeah, this stuff

**[29:19]** And so it's just like yeah, this stuff

**[29:19]** And so it's just like yeah, this stuff is all just getting started. Yeah, it's

**[29:21]** is all just getting started. Yeah, it's

**[29:21]** is all just getting started. Yeah, it's doing a lot of work. Uh guest question

**[29:23]** doing a lot of work. Uh guest question

**[29:23]** doing a lot of work. Uh guest question from Dylan Patel on scaling and uh

**[29:26]** from Dylan Patel on scaling and uh

**[29:26]** from Dylan Patel on scaling and uh reliability. Um so as we're doing more

**[29:29]** reliability. Um so as we're doing more

**[29:29]** reliability. Um so as we're doing more tasks that take longer and utilize more

**[29:31]** tasks that take longer and utilize more

**[29:31]** tasks that take longer and utilize more GPUs, they're also just unreliable. They

**[29:34]** GPUs, they're also just unreliable. They

**[29:34]** GPUs, they're also just unreliable. They fail a lot, right? And and this is just

**[29:35]** fail a lot, right? And and this is just

**[29:35]** fail a lot, right? And and this is just well known. Um so this causes training

**[29:37]** well known. Um so this causes training

**[29:37]** well known. Um so this causes training to fail as well. So like but like you

**[29:40]** to fail as well. So like but like you

**[29:40]** to fail as well. So like but like you know you you've mentioned that you can

**[29:41]** know you you've mentioned that you can

**[29:41]** know you you've mentioned that you can sort of just restart a run and that's

**[29:43]** sort of just restart a run and that's

**[29:43]** sort of just restart a run and that's okay. like how do you deal with this

**[29:44]** okay. like how do you deal with this

**[29:44]** okay. like how do you deal with this when you have to train long horizon

**[29:46]** when you have to train long horizon

**[29:46]** when you have to train long horizon agents, right? Because you can't really

**[29:47]** agents, right? Because you can't really

**[29:47]** agents, right? Because you can't really restart something that has a trajectory

**[29:49]** restart something that has a trajectory

**[29:49]** restart something that has a trajectory that's kind of halfway that is maybe

**[29:51]** that's kind of halfway that is maybe

**[29:51]** that's kind of halfway that is maybe nondeterministic. Yeah. I mean, I think

**[29:53]** nondeterministic. Yeah. I mean, I think

**[29:53]** nondeterministic. Yeah. I mean, I think that there's a bunch of problems that

**[29:56]** that there's a bunch of problems that

**[29:56]** that there's a bunch of problems that you kind of solve and then you make the

**[29:58]** you kind of solve and then you make the

**[29:58]** you kind of solve and then you make the models more capable and then you have to


### [30:00 - 31:00]

**[30:00]** models more capable and then you have to

**[30:00]** models more capable and then you have to resolve them. And so, yeah, when the the

**[30:03]** resolve them. And so, yeah, when the the

**[30:03]** resolve them. And so, yeah, when the the rollouts are short, you know, 30

**[30:05]** rollouts are short, you know, 30

**[30:05]** rollouts are short, you know, 30 seconds, you kind of don't care that

**[30:06]** seconds, you kind of don't care that

**[30:06]** seconds, you kind of don't care that much about this problem. If they're

**[30:08]** much about this problem. If they're

**[30:08]** much about this problem. If they're going to be days now, you really care

**[30:11]** going to be days now, you really care

**[30:11]** going to be days now, you really care about this problem. Yep. And you have to

**[30:12]** about this problem. Yep. And you have to

**[30:12]** about this problem. Yep. And you have to start thinking about how to snapshot

**[30:14]** start thinking about how to snapshot

**[30:14]** start thinking about how to snapshot state and a bunch of things like that.

**[30:16]** state and a bunch of things like that.

**[30:16]** state and a bunch of things like that. Um the short answer is that I think that

**[30:18]** Um the short answer is that I think that

**[30:18]** Um the short answer is that I think that there's a this like ladder of complexity

**[30:20]** there's a this like ladder of complexity

**[30:20]** there's a this like ladder of complexity that you keep climbing with these

**[30:23]** that you keep climbing with these

**[30:23]** that you keep climbing with these training systems and it goes from you

**[30:24]** training systems and it goes from you

**[30:24]** training systems and it goes from you know like couple years ago all that we

**[30:26]** know like couple years ago all that we

**[30:26]** know like couple years ago all that we cared about was just doing good

**[30:27]** cared about was just doing good

**[30:28]** cared about was just doing good oldfashioned free training, right? And

**[30:29]** oldfashioned free training, right? And

**[30:29]** oldfashioned free training, right? And that's like very checkpointable. Um and

**[30:31]** that's like very checkpointable. Um and

**[30:31]** that's like very checkpointable. Um and even there it's not trivial, right? It's

**[30:33]** even there it's not trivial, right? It's

**[30:33]** even there it's not trivial, right? It's like you know if you go from

**[30:34]** like you know if you go from

**[30:34]** like you know if you go from checkpointing once in a while to like

**[30:36]** checkpointing once in a while to like

**[30:36]** checkpointing once in a while to like you want to checkpoint every single step

**[30:37]** you want to checkpoint every single step

**[30:37]** you want to checkpoint every single step now you need to think really hard about

**[30:39]** now you need to think really hard about

**[30:39]** now you need to think really hard about about how you're going to avoid copies

**[30:41]** about how you're going to avoid copies

**[30:41]** about how you're going to avoid copies and blocking and all these things um

**[30:43]** and blocking and all these things um

**[30:43]** and blocking and all these things um then for something like these more

**[30:45]** then for something like these more

**[30:45]** then for something like these more complicated RL systems there's still

**[30:46]** complicated RL systems there's still

**[30:46]** complicated RL systems there's still checkpoint in terms of you know maybe

**[30:49]** checkpoint in terms of you know maybe

**[30:49]** checkpoint in terms of you know maybe you care about uh you know checkpointing

**[30:51]** you care about uh you know checkpointing

**[30:51]** you care about uh you know checkpointing your cache so you don't have to recmp

**[30:52]** your cache so you don't have to recmp

**[30:52]** your cache so you don't have to recmp compute everything um and the nice thing

**[30:54]** compute everything um and the nice thing

**[30:54]** compute everything um and the nice thing about our systems is that you know

**[30:55]** about our systems is that you know

**[30:55]** about our systems is that you know language models are their state is very

**[30:58]** language models are their state is very

**[30:58]** language models are their state is very explicit right and it's something that


### [31:00 - 32:00]

**[31:00]** explicit right and it's something that

**[31:00]** explicit right and it's something that actually can be stored um something you

**[31:02]** actually can be stored um something you

**[31:02]** actually can be stored um something you actually can can handle. Whereas if you

**[31:04]** actually can can handle. Whereas if you

**[31:04]** actually can can handle. Whereas if you have tools that you're hooked up to that

**[31:05]** have tools that you're hooked up to that

**[31:05]** have tools that you're hooked up to that are themselves stateful, maybe those are

**[31:07]** are themselves stateful, maybe those are

**[31:07]** are themselves stateful, maybe those are not something you can restart and

**[31:09]** not something you can restart and

**[31:09]** not something you can restart and recover from. And so I think that that

**[31:11]** recover from. And so I think that that

**[31:12]** recover from. And so I think that that if you consider the whole system end to

**[31:14]** if you consider the whole system end to

**[31:14]** if you consider the whole system end to end, thinking about what checkpoint

**[31:16]** end, thinking about what checkpoint

**[31:16]** end, thinking about what checkpoint ability looks like. And there's also a

**[31:17]** ability looks like. And there's also a

**[31:17]** ability looks like. And there's also a question of maybe it just doesn't

**[31:18]** question of maybe it just doesn't

**[31:18]** question of maybe it just doesn't matter, right? Maybe it's fine that you

**[31:20]** matter, right? Maybe it's fine that you

**[31:20]** matter, right? Maybe it's fine that you restart the system and you get some

**[31:21]** restart the system and you get some

**[31:21]** restart the system and you get some little wiggle in your graph, but these

**[31:23]** little wiggle in your graph, but these

**[31:23]** little wiggle in your graph, but these models are smart. Yeah. Right. That they

**[31:25]** models are smart. Yeah. Right. That they

**[31:25]** models are smart. Yeah. Right. That they can handle it. Um, one thing we're

**[31:27]** can handle it. Um, one thing we're

**[31:27]** can handle it. Um, one thing we're looking at tomorrow that's launching is

**[31:28]** looking at tomorrow that's launching is

**[31:28]** looking at tomorrow that's launching is maybe you can sort of take over the VM

**[31:30]** maybe you can sort of take over the VM

**[31:30]** maybe you can sort of take over the VM and checkpoint the VM state and restart

**[31:32]** and checkpoint the VM state and restart

**[31:32]** and checkpoint the VM state and restart it. Yep. Um, I think we have a dialin

**[31:35]** it. Yep. Um, I think we have a dialin

**[31:35]** it. Yep. Um, I think we have a dialin call-in question from Paris. Um, if

**[31:38]** call-in question from Paris. Um, if

**[31:38]** call-in question from Paris. Um, if someone can play the video

**[31:45]** Oh, I wish I could be there to ask you

**[31:45]** Oh, I wish I could be there to ask you in person. One of the questions that I

**[31:46]** in person. One of the questions that I

**[31:46]** in person. One of the questions that I have is in this new world, the work the

**[31:49]** have is in this new world, the work the

**[31:49]** have is in this new world, the work the workloads in the data center in the in

**[31:51]** workloads in the data center in the in

**[31:51]** workloads in the data center in the in the AI infrastructure is going to be

**[31:52]** the AI infrastructure is going to be

**[31:52]** the AI infrastructure is going to be incredibly diverse. on the one hand

**[31:53]** incredibly diverse. on the one hand

**[31:54]** incredibly diverse. on the one hand agents that are doing deep research and

**[31:55]** agents that are doing deep research and

**[31:55]** agents that are doing deep research and they're thinking they're reasoning

**[31:56]** they're thinking they're reasoning

**[31:56]** they're thinking they're reasoning they're planning and they're working

**[31:58]** they're planning and they're working

**[31:58]** they're planning and they're working with other agents and they're you know

**[31:59]** with other agents and they're you know

**[31:59]** with other agents and they're you know working on a lot of memory they have


### [32:00 - 33:00]

**[32:00]** working on a lot of memory they have

**[32:00]** working on a lot of memory they have large context on one hand some of it you

**[32:02]** large context on one hand some of it you

**[32:02]** large context on one hand some of it you also want to think as fast as possible

**[32:03]** also want to think as fast as possible

**[32:03]** also want to think as fast as possible so you know how do you how do you create

**[32:05]** so you know how do you how do you create

**[32:05]** so you know how do you how do you create uh an AI infrastructure that is

**[32:07]** uh an AI infrastructure that is

**[32:07]** uh an AI infrastructure that is optimized for workloads that have to

**[32:09]** optimized for workloads that have to

**[32:09]** optimized for workloads that have to that have a lot of prefill a lot of

**[32:10]** that have a lot of prefill a lot of

**[32:10]** that have a lot of prefill a lot of decode a lot of something in between on

**[32:12]** decode a lot of something in between on

**[32:12]** decode a lot of something in between on the one hand and on the other hand uh

**[32:14]** the one hand and on the other hand uh

**[32:14]** the one hand and on the other hand uh the type of workloads that I'm super

**[32:15]** the type of workloads that I'm super

**[32:15]** the type of workloads that I'm super excited about these multimodal vision

**[32:18]** excited about these multimodal vision

**[32:18]** excited about these multimodal vision and speech AIs that are essentially your

**[32:20]** and speech AIs that are essentially your

**[32:20]** and speech AIs that are essentially your R2-D2 your companion it's on all the

**[32:21]** R2-D2 your companion it's on all the

**[32:22]** R2-D2 your companion it's on all the time it's instantly aail available to

**[32:23]** time it's instantly aail available to

**[32:23]** time it's instantly aail available to you and so these two workloads one of

**[32:25]** you and so these two workloads one of

**[32:25]** you and so these two workloads one of the one of them super uh compute

**[32:27]** the one of them super uh compute

**[32:27]** the one of them super uh compute intensive and take might take a long

**[32:28]** intensive and take might take a long

**[32:28]** intensive and take might take a long time and um uh you know test time

**[32:31]** time and um uh you know test time

**[32:31]** time and um uh you know test time scaling and all that on the other hand

**[32:32]** scaling and all that on the other hand

**[32:32]** scaling and all that on the other hand wants to be very low latency so what

**[32:33]** wants to be very low latency so what

**[32:33]** wants to be very low latency so what does what does a future AI

**[32:34]** does what does a future AI

**[32:34]** does what does a future AI infrastructure look like that's that's

**[32:36]** infrastructure look like that's that's

**[32:36]** infrastructure look like that's that's as flexible as possible um as performant

**[32:38]** as flexible as possible um as performant

**[32:38]** as flexible as possible um as performant as possible low latency high throughput

**[32:40]** as possible low latency high throughput

**[32:40]** as possible low latency high throughput you know all of that uh is just

**[32:42]** you know all of that uh is just

**[32:42]** you know all of that uh is just incredibly complex so how how you think

**[32:44]** incredibly complex so how how you think

**[32:44]** incredibly complex so how how you think through that and and what kind of an AI

**[32:45]** through that and and what kind of an AI

**[32:45]** through that and and what kind of an AI infrastructure would you would you think

**[32:47]** infrastructure would you would you think

**[32:47]** infrastructure would you would you think uh would be ideal going forward

**[32:49]** uh would be ideal going forward

**[32:49]** uh would be ideal going forward well with lot lots of GPUs of

**[32:53]** well with lot lots of GPUs of

**[32:53]** well with lot lots of GPUs of So, so if I were to summarize, uh,

**[32:56]** So, so if I were to summarize, uh,

**[32:56]** So, so if I were to summarize, uh, Jensen wants you to tell him what to

**[32:58]** Jensen wants you to tell him what to

**[32:58]** Jensen wants you to tell him what to build.


### [33:00 - 34:00]

**[33:03]** What would be your dream? Uh, but also

**[33:03]** What would be your dream? Uh, but also like there's just two needs. There's two

**[33:04]** like there's just two needs. There's two

**[33:04]** like there's just two needs. There's two kinds of infra. There's there's long

**[33:06]** kinds of infra. There's there's long

**[33:06]** kinds of infra. There's there's long compute and there's real time. Now, now,

**[33:08]** compute and there's real time. Now, now,

**[33:08]** compute and there's real time. Now, now, now. Yes. Yes. I mean, it's it it is

**[33:11]** now. Yes. Yes. I mean, it's it it is

**[33:11]** now. Yes. Yes. I mean, it's it it is hard, right? Because I mean, this codees

**[33:15]** hard, right? Because I mean, this codees

**[33:15]** hard, right? Because I mean, this codees problem, it is a mind-boggling one. And

**[33:17]** problem, it is a mind-boggling one. And

**[33:17]** problem, it is a mind-boggling one. And so, you know, I'm a software person by

**[33:18]** so, you know, I'm a software person by

**[33:18]** so, you know, I'm a software person by by background and that, you know, we

**[33:20]** by background and that, you know, we

**[33:20]** by background and that, you know, we think we're we're off here just like

**[33:21]** think we're we're off here just like

**[33:21]** think we're we're off here just like writing the software for AGI and then

**[33:23]** writing the software for AGI and then

**[33:23]** writing the software for AGI and then you realize you have to do like these

**[33:24]** you realize you have to do like these

**[33:24]** you realize you have to do like these massive infrastructure projects, right?

**[33:26]** massive infrastructure projects, right?

**[33:26]** massive infrastructure projects, right? Like that's not how we set out, but it

**[33:28]** Like that's not how we set out, but it

**[33:28]** Like that's not how we set out, but it actually kind of makes sense in the end,

**[33:29]** actually kind of makes sense in the end,

**[33:30]** actually kind of makes sense in the end, right? If we're going to build something

**[33:31]** right? If we're going to build something

**[33:31]** right? If we're going to build something that's going to be transformative to the

**[33:32]** that's going to be transformative to the

**[33:32]** that's going to be transformative to the world, like yeah, probably it's going to

**[33:34]** world, like yeah, probably it's going to

**[33:34]** world, like yeah, probably it's going to require some some, you know, maybe the

**[33:35]** require some some, you know, maybe the

**[33:35]** require some some, you know, maybe the biggest physical machines that humanity

**[33:37]** biggest physical machines that humanity

**[33:37]** biggest physical machines that humanity has ever created, like kind of type

**[33:38]** has ever created, like kind of type

**[33:38]** has ever created, like kind of type checks. Um, so I think that the that

**[33:42]** checks. Um, so I think that the that

**[33:42]** checks. Um, so I think that the that there's two answers. Like the naive

**[33:44]** there's two answers. Like the naive

**[33:44]** there's two answers. Like the naive answer is, okay, yeah, you want two

**[33:45]** answer is, okay, yeah, you want two

**[33:45]** answer is, okay, yeah, you want two kinds of accelerators. You want one

**[33:47]** kinds of accelerators. You want one

**[33:47]** kinds of accelerators. You want one that's really computed, one that's very

**[33:48]** that's really computed, one that's very

**[33:48]** that's really computed, one that's very latency optimized. Um, throw like tons

**[33:51]** latency optimized. Um, throw like tons

**[33:51]** latency optimized. Um, throw like tons of of HBM on one of those and, you know,

**[33:53]** of of HBM on one of those and, you know,

**[33:53]** of of HBM on one of those and, you know, ton tons of tons of comput on the other.

**[33:54]** ton tons of tons of comput on the other.

**[33:54]** ton tons of tons of comput on the other. You're all good. Um, now one thing

**[33:57]** You're all good. Um, now one thing

**[33:57]** You're all good. Um, now one thing that's really difficult is predicting

**[33:58]** that's really difficult is predicting

**[33:58]** that's really difficult is predicting the ratios, right? Now you have a new


### [34:00 - 35:00]

**[34:00]** the ratios, right? Now you have a new

**[34:00]** the ratios, right? Now you have a new problem you have to think about. And if

**[34:02]** problem you have to think about. And if

**[34:02]** problem you have to think about. And if you get the balance wrong, suddenly

**[34:03]** you get the balance wrong, suddenly

**[34:03]** you get the balance wrong, suddenly you're going to have a whole part of

**[34:04]** you're going to have a whole part of

**[34:04]** you're going to have a whole part of your fleet that's just useless. Yep. And

**[34:06]** your fleet that's just useless. Yep. And

**[34:06]** your fleet that's just useless. Yep. And that sounds really scary. Um, now the

**[34:07]** that sounds really scary. Um, now the

**[34:07]** that sounds really scary. Um, now the thing is because the way that these

**[34:09]** thing is because the way that these

**[34:09]** thing is because the way that these things work is there's no requirements

**[34:11]** things work is there's no requirements

**[34:12]** things work is there's no requirements in this field. There's no constraints in

**[34:13]** in this field. There's no constraints in

**[34:13]** in this field. There's no constraints in this field. there's just sort of this

**[34:16]** this field. there's just sort of this

**[34:16]** this field. there's just sort of this linear program that people are

**[34:17]** linear program that people are

**[34:18]** linear program that people are optimizing and so yeah if you give our

**[34:20]** optimizing and so yeah if you give our

**[34:20]** optimizing and so yeah if you give our engineers some sort of misbalance of

**[34:22]** engineers some sort of misbalance of

**[34:22]** engineers some sort of misbalance of resources like we will find ways to

**[34:25]** resources like we will find ways to

**[34:25]** resources like we will find ways to utilize it maybe at great pain right but

**[34:27]** utilize it maybe at great pain right but

**[34:27]** utilize it maybe at great pain right but an example of this is you know you've

**[34:28]** an example of this is you know you've

**[34:28]** an example of this is you know you've seen the whole field move towards

**[34:30]** seen the whole field move towards

**[34:30]** seen the whole field move towards mixture of experts and to some extent

**[34:31]** mixture of experts and to some extent

**[34:31]** mixture of experts and to some extent what mixture of experts is is saying

**[34:33]** what mixture of experts is is saying

**[34:33]** what mixture of experts is is saying well we have all this DRAM sitting

**[34:34]** well we have all this DRAM sitting

**[34:34]** well we have all this DRAM sitting around that isn't being used for

**[34:36]** around that isn't being used for

**[34:36]** around that isn't being used for anything because the balance is wrong

**[34:38]** anything because the balance is wrong

**[34:38]** anything because the balance is wrong fine we'll fill up with parameters and

**[34:39]** fine we'll fill up with parameters and

**[34:39]** fine we'll fill up with parameters and we'll actually not cost any compute and

**[34:42]** we'll actually not cost any compute and

**[34:42]** we'll actually not cost any compute and we'll just get extra ML comput

**[34:44]** we'll just get extra ML comput

**[34:44]** we'll just get extra ML comput efficiency out of it like boom there you

**[34:46]** efficiency out of it like boom there you

**[34:46]** efficiency out of it like boom there you go and so I think that there is some of

**[34:48]** go and so I think that there is some of

**[34:48]** go and so I think that there is some of that where if you get the balance wrong

**[34:49]** that where if you get the balance wrong

**[34:49]** that where if you get the balance wrong it's actually not the end of the world

**[34:51]** it's actually not the end of the world

**[34:51]** it's actually not the end of the world um homogeneity of accelerators is like a

**[34:54]** um homogeneity of accelerators is like a

**[34:54]** um homogeneity of accelerators is like a very nice default to start um but I

**[34:56]** very nice default to start um but I

**[34:56]** very nice default to start um but I think that that that ending up with

**[34:58]** think that that that ending up with

**[34:58]** think that that that ending up with purpose-built accelerators is also not


### [35:00 - 36:00]

**[35:00]** purpose-built accelerators is also not

**[35:00]** purpose-built accelerators is also not super crazy and the more that we move to

**[35:02]** super crazy and the more that we move to

**[35:02]** super crazy and the more that we move to these world these worlds where it's the

**[35:04]** these world these worlds where it's the

**[35:04]** these world these worlds where it's the just dollars of capex for this

**[35:05]** just dollars of capex for this

**[35:05]** just dollars of capex for this infrastructure starts to become so eye

**[35:07]** infrastructure starts to become so eye

**[35:07]** infrastructure starts to become so eye watering then starting to hyper optimize

**[35:10]** watering then starting to hyper optimize

**[35:10]** watering then starting to hyper optimize for some of these workloads is pretty

**[35:11]** for some of these workloads is pretty

**[35:11]** for some of these workloads is pretty reasonable um but I think the jury a

**[35:13]** reasonable um but I think the jury a

**[35:13]** reasonable um but I think the jury a little bit out because if you think

**[35:14]** little bit out because if you think

**[35:14]** little bit out because if you think about it that the research is just

**[35:16]** about it that the research is just

**[35:16]** about it that the research is just moving so fast and to some extent that

**[35:18]** moving so fast and to some extent that

**[35:18]** moving so fast and to some extent that dominates everything else. Um okay I

**[35:21]** dominates everything else. Um okay I

**[35:21]** dominates everything else. Um okay I wasn't planning to ask this but you just

**[35:22]** wasn't planning to ask this but you just

**[35:22]** wasn't planning to ask this but you just brought up the research stuff. Can you

**[35:23]** brought up the research stuff. Can you

**[35:24]** brought up the research stuff. Can you rank current scaling bottlenecks for

**[35:25]** rank current scaling bottlenecks for

**[35:25]** rank current scaling bottlenecks for GBT6? Ah compute data algorithms power

**[35:30]** GBT6? Ah compute data algorithms power

**[35:30]** GBT6? Ah compute data algorithms power money. Yes.

**[35:33]** money. Yes.

**[35:33]** money. Yes. I mean which one's which one's like the

**[35:35]** I mean which one's which one's like the

**[35:35]** I mean which one's which one's like the you know number one and two? Which one

**[35:36]** you know number one and two? Which one

**[35:36]** you know number one and two? Which one are you are you like most rate limited

**[35:38]** are you are you like most rate limited

**[35:38]** are you are you like most rate limited on? I mean look I think we are in a

**[35:40]** on? I mean look I think we are in a

**[35:40]** on? I mean look I think we are in a world where basic research is back. I

**[35:42]** world where basic research is back. I

**[35:42]** world where basic research is back. I think that is really amazing, right?

**[35:44]** think that is really amazing, right?

**[35:44]** think that is really amazing, right? There was this period. Yeah, basic

**[35:46]** There was this period. Yeah, basic

**[35:46]** There was this period. Yeah, basic research. Um there was a period where it

**[35:48]** research. Um there was a period where it

**[35:48]** research. Um there was a period where it felt like all right, we got a

**[35:49]** felt like all right, we got a

**[35:49]** felt like all right, we got a transformer, let's just scale it, you

**[35:51]** transformer, let's just scale it, you

**[35:51]** transformer, let's just scale it, you know, and um I find those problems very

**[35:54]** know, and um I find those problems very

**[35:54]** know, and um I find those problems very exciting. I have a lot of fun just like

**[35:56]** exciting. I have a lot of fun just like

**[35:56]** exciting. I have a lot of fun just like you got a very well- definfined hard

**[35:57]** you got a very well- definfined hard

**[35:57]** you got a very well- definfined hard problem. You want to just move the

**[35:58]** problem. You want to just move the

**[35:58]** problem. You want to just move the number up and to the right. Um but it


### [36:00 - 37:00]

**[36:01]** number up and to the right. Um but it

**[36:01]** number up and to the right. Um but it also is a little intellectually

**[36:02]** also is a little intellectually

**[36:02]** also is a little intellectually dissatisfying in some ways. It's like

**[36:04]** dissatisfying in some ways. It's like

**[36:04]** dissatisfying in some ways. It's like that it feels like there's more to life

**[36:05]** that it feels like there's more to life

**[36:05]** that it feels like there's more to life than just, you know, attention is all

**[36:07]** than just, you know, attention is all

**[36:07]** than just, you know, attention is all you need paper, you know, in in in

**[36:09]** you need paper, you know, in in in

**[36:09]** you need paper, you know, in in in vanilla form. Um and so I think that

**[36:12]** vanilla form. Um and so I think that

**[36:12]** vanilla form. Um and so I think that what we've started to see is that we're

**[36:14]** what we've started to see is that we're

**[36:14]** what we've started to see is that we're operating at a scale now um where we've

**[36:18]** operating at a scale now um where we've

**[36:18]** operating at a scale now um where we've pushed the compute, we've pushed the

**[36:20]** pushed the compute, we've pushed the

**[36:20]** pushed the compute, we've pushed the data so far that you can start to get

**[36:23]** data so far that you can start to get

**[36:23]** data so far that you can start to get you start to have algorithms is like

**[36:25]** you start to have algorithms is like

**[36:25]** you start to have algorithms is like again just back as as a important and

**[36:27]** again just back as as a important and

**[36:27]** again just back as as a important and really almost a long pole um in in terms

**[36:30]** really almost a long pole um in in terms

**[36:30]** really almost a long pole um in in terms of future progress. And so um all of

**[36:32]** of future progress. And so um all of

**[36:32]** of future progress. And so um all of these things they're all they're all

**[36:33]** these things they're all they're all

**[36:33]** these things they're all they're all important poles of the tent. And you

**[36:35]** important poles of the tent. And you

**[36:35]** important poles of the tent. And you know on any one day uh it might look a

**[36:37]** know on any one day uh it might look a

**[36:37]** know on any one day uh it might look a little lopsided one way or another. Um

**[36:39]** little lopsided one way or another. Um

**[36:39]** little lopsided one way or another. Um but yeah, fundamentally I think it's

**[36:40]** but yeah, fundamentally I think it's

**[36:40]** but yeah, fundamentally I think it's like you want to keep these all in

**[36:41]** like you want to keep these all in

**[36:42]** like you want to keep these all in balance. Um and it's really exciting to

**[36:43]** balance. Um and it's really exciting to

**[36:43]** balance. Um and it's really exciting to see things like like the RL paradigm.

**[36:45]** see things like like the RL paradigm.

**[36:45]** see things like like the RL paradigm. That's something that we invested in

**[36:46]** That's something that we invested in

**[36:46]** That's something that we invested in very deliberately uh for for for

**[36:48]** very deliberately uh for for for

**[36:48]** very deliberately uh for for for multiple years. It was like when we

**[36:50]** multiple years. It was like when we

**[36:50]** multiple years. It was like when we trained GPD4 um the very first thing

**[36:52]** trained GPD4 um the very first thing

**[36:52]** trained GPD4 um the very first thing like I think it was really interesting

**[36:53]** like I think it was really interesting

**[36:54]** like I think it was really interesting was when you we talked to GPD4 for the

**[36:56]** was when you we talked to GPD4 for the

**[36:56]** was when you we talked to GPD4 for the first time we were like is this an AGI?

**[36:59]** first time we were like is this an AGI?

**[36:59]** first time we were like is this an AGI? Like it's clearly not an AGI but it's


### [37:00 - 38:00]

**[37:00]** Like it's clearly not an AGI but it's

**[37:00]** Like it's clearly not an AGI but it's really hard to say why right is like

**[37:03]** really hard to say why right is like

**[37:03]** really hard to say why right is like there's something about it. It's so

**[37:04]** there's something about it. It's so

**[37:04]** there's something about it. It's so fluid and smooth but but somehow it

**[37:06]** fluid and smooth but but somehow it

**[37:06]** fluid and smooth but but somehow it falls off the rails. is like, well, we

**[37:07]** falls off the rails. is like, well, we

**[37:07]** falls off the rails. is like, well, we got to solve that reliability problem.

**[37:09]** got to solve that reliability problem.

**[37:09]** got to solve that reliability problem. And you're like, well, it has never

**[37:11]** And you're like, well, it has never

**[37:11]** And you're like, well, it has never actually experienced the world, right?

**[37:13]** actually experienced the world, right?

**[37:13]** actually experienced the world, right? It's like someone who's just read all

**[37:14]** It's like someone who's just read all

**[37:14]** It's like someone who's just read all the books or, you know, sort of read,

**[37:16]** the books or, you know, sort of read,

**[37:16]** the books or, you know, sort of read, you know, sort of observed the world,

**[37:17]** you know, sort of observed the world,

**[37:18]** you know, sort of observed the world, has observed the world, um, and, uh,

**[37:21]** has observed the world, um, and, uh,

**[37:21]** has observed the world, um, and, uh, never experienced it itself, right? It's

**[37:22]** never experienced it itself, right? It's

**[37:22]** never experienced it itself, right? It's like, you know, sort of just, you know,

**[37:24]** like, you know, sort of just, you know,

**[37:24]** like, you know, sort of just, you know, watching it through through a pane of

**[37:25]** watching it through through a pane of

**[37:25]** watching it through through a pane of glass or something. And, uh, and and

**[37:28]** glass or something. And, uh, and and

**[37:28]** glass or something. And, uh, and and that to me is I, you know, was something

**[37:31]** that to me is I, you know, was something

**[37:31]** that to me is I, you know, was something we were just like, okay, clearly we need

**[37:32]** we were just like, okay, clearly we need

**[37:32]** we were just like, okay, clearly we need a different paradigm. And we just pushed

**[37:34]** a different paradigm. And we just pushed

**[37:34]** a different paradigm. And we just pushed on it until we made it really work. And

**[37:36]** on it until we made it really work. And

**[37:36]** on it until we made it really work. And I think that that remains true today

**[37:37]** I think that that remains true today

**[37:37]** I think that that remains true today that there's other very clear missing

**[37:39]** that there's other very clear missing

**[37:39]** that there's other very clear missing capabilities um that we just need to

**[37:41]** capabilities um that we just need to

**[37:41]** capabilities um that we just need to keep pushing and we will we will get

**[37:43]** keep pushing and we will we will get

**[37:43]** keep pushing and we will we will get there. Awesome. Um broadening out just

**[37:46]** there. Awesome. Um broadening out just

**[37:46]** there. Awesome. Um broadening out just from from just broad opening eye things.

**[37:49]** from from just broad opening eye things.

**[37:49]** from from just broad opening eye things. Um well honestly I'm just going to let

**[37:51]** Um well honestly I'm just going to let

**[37:51]** Um well honestly I'm just going to let So we asked Jensen for one question.

**[37:53]** So we asked Jensen for one question.

**[37:53]** So we asked Jensen for one question. He's an overachiever so he sent in two.

**[37:55]** He's an overachiever so he sent in two.

**[37:55]** He's an overachiever so he sent in two. So let's play a second video.

**[37:59]** So let's play a second video.

**[37:59]** So let's play a second video. AI native engineers in the audience they


### [38:00 - 39:00]

**[38:01]** AI native engineers in the audience they

**[38:01]** AI native engineers in the audience they are probably thinking um in the coming

**[38:03]** are probably thinking um in the coming

**[38:03]** are probably thinking um in the coming years your openi will have agis and uh

**[38:06]** years your openi will have agis and uh

**[38:06]** years your openi will have agis and uh they will be building domain specific

**[38:08]** they will be building domain specific

**[38:08]** they will be building domain specific agents on top of the agis from openi and

**[38:11]** agents on top of the agis from openi and

**[38:11]** agents on top of the agis from openi and so some of the some of the questions

**[38:12]** so some of the some of the questions

**[38:12]** so some of the some of the questions that I would have on my mind would be uh

**[38:15]** that I would have on my mind would be uh

**[38:15]** that I would have on my mind would be uh how do you think their development

**[38:17]** how do you think their development

**[38:17]** how do you think their development workflow would change uh as uh openai's

**[38:20]** workflow would change uh as uh openai's

**[38:20]** workflow would change uh as uh openai's agis become much more capable and yet

**[38:22]** agis become much more capable and yet

**[38:22]** agis become much more capable and yet they would still have um plumbing

**[38:24]** they would still have um plumbing

**[38:24]** they would still have um plumbing workflows uh pipelines that they would

**[38:26]** workflows uh pipelines that they would

**[38:26]** workflows uh pipelines that they would create flywheels that they would create

**[38:27]** create flywheels that they would create

**[38:27]** create flywheels that they would create for their domain specific uh agents

**[38:30]** for their domain specific uh agents

**[38:30]** for their domain specific uh agents These agents would of course be able to

**[38:31]** These agents would of course be able to

**[38:31]** These agents would of course be able to reason, plan, use tools, have memory,

**[38:33]** reason, plan, use tools, have memory,

**[38:33]** reason, plan, use tools, have memory, short-term, long-term memory and um and

**[38:35]** short-term, long-term memory and um and

**[38:35]** short-term, long-term memory and um and they'll be amazing amazing agents, but

**[38:37]** they'll be amazing amazing agents, but

**[38:37]** they'll be amazing amazing agents, but how does it change uh the development

**[38:39]** how does it change uh the development

**[38:39]** how does it change uh the development process in the coming years?

**[38:42]** process in the coming years?

**[38:42]** process in the coming years? Yeah, I think that this is a really

**[38:43]** Yeah, I think that this is a really

**[38:43]** Yeah, I think that this is a really fascinating question, right? I think you

**[38:45]** fascinating question, right? I think you

**[38:45]** fascinating question, right? I think you can find a wide spectrum of very

**[38:48]** can find a wide spectrum of very

**[38:48]** can find a wide spectrum of very strongly held opinion that is all

**[38:49]** strongly held opinion that is all

**[38:49]** strongly held opinion that is all mutually contradictory. Um I think my

**[38:52]** mutually contradictory. Um I think my

**[38:52]** mutually contradictory. Um I think my perspective is that first of all, it's

**[38:54]** perspective is that first of all, it's

**[38:54]** perspective is that first of all, it's all on the table, right? Maybe we reach

**[38:55]** all on the table, right? Maybe we reach

**[38:56]** all on the table, right? Maybe we reach a world where it's just like the AIs are

**[38:57]** a world where it's just like the AIs are

**[38:57]** a world where it's just like the AIs are so capable um that you know we all you


### [39:00 - 40:00]

**[39:00]** so capable um that you know we all you

**[39:00]** so capable um that you know we all you know just let let them write all the

**[39:01]** know just let let them write all the

**[39:01]** know just let let them write all the code. Maybe there's a world where that

**[39:04]** code. Maybe there's a world where that

**[39:04]** code. Maybe there's a world where that you have like one AI in the sky. Maybe

**[39:06]** you have like one AI in the sky. Maybe

**[39:06]** you have like one AI in the sky. Maybe it's that you actually have a bunch of

**[39:08]** it's that you actually have a bunch of

**[39:08]** it's that you actually have a bunch of domain specific agents that require a

**[39:10]** domain specific agents that require a

**[39:10]** domain specific agents that require a bunch of of specific work in order to

**[39:12]** bunch of of specific work in order to

**[39:12]** bunch of of specific work in order to make that make it happen. I think the

**[39:14]** make that make it happen. I think the

**[39:14]** make that make it happen. I think the evidence has really been shifting

**[39:15]** evidence has really been shifting

**[39:15]** evidence has really been shifting towards this like menagery of different

**[39:17]** towards this like menagery of different

**[39:17]** towards this like menagery of different models. Um and I think that's that's

**[39:18]** models. Um and I think that's that's

**[39:18]** models. Um and I think that's that's actually really exciting right that

**[39:20]** actually really exciting right that

**[39:20]** actually really exciting right that there's different inference costs just

**[39:22]** there's different inference costs just

**[39:22]** there's different inference costs just even from a systems perspective. um that

**[39:24]** even from a systems perspective. um that

**[39:24]** even from a systems perspective. um that there's different trade-offs like

**[39:26]** there's different trade-offs like

**[39:26]** there's different trade-offs like distillation works so well. Um so

**[39:28]** distillation works so well. Um so

**[39:28]** distillation works so well. Um so there's actually a lot of power to be

**[39:30]** there's actually a lot of power to be

**[39:30]** there's actually a lot of power to be had by models that are actually able to

**[39:32]** had by models that are actually able to

**[39:32]** had by models that are actually able to use other models. And so I think that

**[39:33]** use other models. And so I think that

**[39:34]** use other models. And so I think that that that is going to open up just a ton

**[39:35]** that that is going to open up just a ton

**[39:35]** that that is going to open up just a ton of opportunity because you know we're

**[39:37]** of opportunity because you know we're

**[39:37]** of opportunity because you know we're heading to a world where the economy is

**[39:39]** heading to a world where the economy is

**[39:39]** heading to a world where the economy is fundamentally powered by AI. We're not

**[39:41]** fundamentally powered by AI. We're not

**[39:41]** fundamentally powered by AI. We're not there yet but you can see it right on

**[39:43]** there yet but you can see it right on

**[39:43]** there yet but you can see it right on the horizon. They're working on it all.

**[39:45]** the horizon. They're working on it all.

**[39:45]** the horizon. They're working on it all. Exactly. I mean that's what people in

**[39:46]** Exactly. I mean that's what people in

**[39:46]** Exactly. I mean that's what people in this room are building that that is what

**[39:47]** this room are building that that is what

**[39:48]** this room are building that that is what you are doing. And the the economy is a

**[39:50]** you are doing. And the the economy is a

**[39:50]** you are doing. And the the economy is a very big thing. there's a lot of

**[39:51]** very big thing. there's a lot of

**[39:52]** very big thing. there's a lot of diversity in it and it's also not static

**[39:54]** diversity in it and it's also not static

**[39:54]** diversity in it and it's also not static right that I think when people think

**[39:56]** right that I think when people think

**[39:56]** right that I think when people think about what AI can do for us um it's very

**[39:58]** about what AI can do for us um it's very

**[39:58]** about what AI can do for us um it's very easy to only look at well what are we

**[39:59]** easy to only look at well what are we

**[39:59]** easy to only look at well what are we doing now and how does AI slot in and


### [40:00 - 41:00]

**[40:01]** doing now and how does AI slot in and

**[40:01]** doing now and how does AI slot in and you know the percentage of human versus

**[40:03]** you know the percentage of human versus

**[40:03]** you know the percentage of human versus AI but that's not the point right the

**[40:05]** AI but that's not the point right the

**[40:05]** AI but that's not the point right the point is how do we get 10x more activity

**[40:07]** point is how do we get 10x more activity

**[40:07]** point is how do we get 10x more activity 10x more economic output 10x more

**[40:09]** 10x more economic output 10x more

**[40:09]** 10x more economic output 10x more benefit to everyone um and I think that

**[40:12]** benefit to everyone um and I think that

**[40:12]** benefit to everyone um and I think that the direction we're heading is one where

**[40:15]** the direction we're heading is one where

**[40:15]** the direction we're heading is one where the models will get much more capable

**[40:17]** the models will get much more capable

**[40:17]** the models will get much more capable there'll be much better fundamental

**[40:19]** there'll be much better fundamental

**[40:19]** there'll be much better fundamental technology and there's just going to be

**[40:20]** technology and there's just going to be

**[40:20]** technology and there's just going to be like way more things we want to do with

**[40:22]** like way more things we want to do with

**[40:22]** like way more things we want to do with it and the barrier to entry will be

**[40:23]** it and the barrier to entry will be

**[40:23]** it and the barrier to entry will be lower than ever. And so things like

**[40:25]** lower than ever. And so things like

**[40:25]** lower than ever. And so things like healthcare um that you can't just you

**[40:28]** healthcare um that you can't just you

**[40:28]** healthcare um that you can't just you know the the it requires responsibility

**[40:29]** know the the it requires responsibility

**[40:29]** know the the it requires responsibility to go in and think about how to do it

**[40:31]** to go in and think about how to do it

**[40:31]** to go in and think about how to do it right. Things like education where

**[40:32]** right. Things like education where

**[40:32]** right. Things like education where there's multiple stakeholders you know

**[40:33]** there's multiple stakeholders you know

**[40:33]** there's multiple stakeholders you know the parent the teacher the student um

**[40:36]** the parent the teacher the student um

**[40:36]** the parent the teacher the student um each of these requires domain expertise

**[40:40]** each of these requires domain expertise

**[40:40]** each of these requires domain expertise requires careful thought requires a lot

**[40:42]** requires careful thought requires a lot

**[40:42]** requires careful thought requires a lot of work. Um and so I think that there is

**[40:45]** of work. Um and so I think that there is

**[40:45]** of work. Um and so I think that there is going to be just like so much

**[40:46]** going to be just like so much

**[40:46]** going to be just like so much opportunity for people to build. Um, and

**[40:48]** opportunity for people to build. Um, and

**[40:48]** opportunity for people to build. Um, and so I'm just so excited to see everyone

**[40:49]** so I'm just so excited to see everyone

**[40:50]** so I'm just so excited to see everyone in this room because that's the right

**[40:51]** in this room because that's the right

**[40:51]** in this room because that's the right kind of energy. Thank you for

**[40:53]** kind of energy. Thank you for

**[40:53]** kind of energy. Thank you for encouraging us and being an inspiration.

**[40:54]** encouraging us and being an inspiration.

**[40:54]** encouraging us and being an inspiration. Thank you so much. Great everybody.

**[40:57]** Thank you so much. Great everybody.

**[40:57]** Thank you so much. Great everybody. Thank you.

**[40:59]** Thank you.

**[40:59]** Thank you. [Music]


