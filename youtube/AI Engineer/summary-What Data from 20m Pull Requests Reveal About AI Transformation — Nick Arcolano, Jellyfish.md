# What Data from 20m Pull Requests Reveal About AI Transformation â€” Nick Arcolano, Jellyfish

**Video URL:** https://www.youtube.com/watch?v=WqZq8L-v9pA

---

## Executive Summary

Nicholas Arcolano, Head of Research at Jellyfish, presents data-driven insights from analyzing 20 million pull requests across 200,000 developers from 1,000 companies spanning June 2024 to present. The talk reveals significant trends in AI coding tool adoption, productivity gains, side effects, and the critical impact of code architecture on AI transformation success. Key findings include: median AI adoption rates have grown from 22% to 90%, teams see a 2x increase in PR throughput on average, but highly distributed architectures show minimal productivity gains due to context limitations.

---

## Key Topics

### [Introduction and Research Questions](https://www.youtube.com/watch?v=WqZq8L-v9pA&t=0s)
**[00:00 - 01:01]**

- Nicholas Arcolano introduces himself as head of research at Jellyfish
- Four big questions companies are asking about AI transformation:
  1. What does good adoption of AI coding tools look like?
  2. What productivity gains should be expected?
  3. What are the side effects?
  4. If AI isn't delivering as advertised, what's going on and what can you do?
- Promises data-backed insights to answer these questions

**Key Points:**
- Many AI native companies being founded, existing companies transforming to be AI native
- Best way to get answers is with data
- Will cover findings from 15-20 minute talk

---

### [Data Set Overview](https://www.youtube.com/watch?v=WqZq8L-v9pA&t=63s)
**[01:03 - 02:14]**

- Jellyfish provides analytics and insights for software engineering leaders
- Data sources combined:
  - AI coding tools usage (Copilot, Cursor, Claude Code)
  - Autonomous coding agents (Devon, Codeex)
  - PR review bots
  - Source control platforms (GitHub)
  - Task management platforms (Linear, Jira)
- Dataset: 20 million pull requests from ~200,000 developers across ~1,000 companies
- Data collection period: June 2024 to present

---

### [Question 1: What Does Good Adoption Look Like?](https://www.youtube.com/watch?v=WqZq8L-v9pA&t=134s)
**[02:14 - 05:38]**

#### Lines of Code Generated by AI
- June 2024: Only 2% of companies generating 50%+ of code with AI
- Current: Nearly half of companies generating 50%+ of code with AI
- Steady growth in AI-generated code over time

#### Developer Adoption Rates
- AI adoption rate defined as: fraction of time developers use AI tools when coding
- June 2024: Median adoption rate was 22%
- Current: Median adoption rates close to 90%
- 100% adoption means using AI every time you code
- Reality: Many teams still have technical, organizational, and cultural barriers

#### Autonomous Coding Agents
- Only 44% of companies have done anything with autonomous agents in past 3 months
- Vast majority is trialing and experimentation (not full-scale production)
- Less than 2% of millions of PRs merged were from autonomous agents
- Still very early days for autonomous agents

**Key Points:**
- Lines of code isn't a great metric but widely discussed
- Developer adoption rate correlates most directly with good productivity outcomes
- Autonomous agents not yet delivering at scale

---

### [Question 2: What Productivity Gains Should You Expect?](https://www.youtube.com/watch?v=WqZq8L-v9pA&t=338s)
**[05:38 - 08:30]**

#### PR Throughput Gains
- Metric: How many pull requests does average engineer merge per week?
- Absolute levels vary by work scoping and architecture
- Average trend: 2x increase in PR throughput when going from 0% to 100% AI adoption
- Clear correlation between AI adoption and PR throughput

#### Cycle Time Improvements
- Cycle time measured: first commit in PR until merged
- Average trend: 24% decrease in cycle times from 0% to 100% AI adoption
- More work happening and happening faster
- Interesting distribution patterns:
  - Lower band: tasks taking less than a day
  - Middle band: tasks taking about two days
  - Long tail: tasks taking much longer

**Key Points:**
- Big gains from interactive coding agents even without autonomous agents
- Productivity measured in both quantity (throughput) and speed (cycle time)
- Measuring change in PR throughput is better than absolute numbers

---

### [Question 3: What Are the Side Effects?](https://www.youtube.com/watch?v=WqZq8L-v9pA&t=510s)
**[08:30 - 10:58]**

#### PRs Are Getting Bigger
- Teams fully adopting AI push PRs that are 18% larger in net lines of code
- Size change primarily from additions rather than deletions (net new code)
- Average number of files touched remains the same
- Change is more thorough/verbose code within same files, not touching more files

#### Quality Impact
- No big effects on quality currently observed
- Bug tickets created: no statistically significant relationship with AI adoption rate
- PR reverts: no statistically significant relationship with AI adoption rate
- Bugs resolved: Actually increased
  - Teams disproportionately using AI to tackle bug tickets in backlog
  - More bugs addressed by AI, but not necessarily caused by AI
  - Makes sense: bugs are well-scoped, verifiable tasks AI can succeed at

**Key Points:**
- No free lunch, but side effects are manageable
- Quality anxiety can ease up - not seeing big issues yet
- Will continue monitoring especially as asynchronous agent usage grows

---

### [Question 4: What If You're Not Seeing Results?](https://www.youtube.com/watch?v=WqZq8L-v9pA&t=658s)
**[10:58 - 16:38]**

#### Focus on Adoption First
- Most important: get folks using tools at scale
- High adoption is prerequisite for seeing gains

#### Code Architecture Impact
- **Key metric: Active repos per engineer** (how many distinct repos typical engineer pushes code to per week)
- Scale-independent metric - removes correlation with company size
- Tells you about the shape of code engineers work with

#### Four Architecture Regimes
1. **Centralized** (monorepos, monolithic services)
2. **Balanced** (moderate distribution)
3. **Distributed** (multiple repos per engineer)
4. **Highly Distributed** (many repos per engineer, polyrepos, microservices)

#### Productivity by Architecture Type
- **Centralized & Balanced**: ~4x PR throughput gains (much better than average)
- **Distributed**: ~2x gains (aligned with global average)
- **Highly Distributed**: Essentially no correlation, even slightly negative trend

#### Why Highly Distributed Struggles
- **Problem of context**: Most tools work best with one repo at a time
- Combining context across repos is challenging for humans and AI
- Relationships between repos/systems often not documented clearly
- Locked in senior engineers' heads, not accessible to tools
- Will take time to invest in context engineering

#### Future Outlook
- Many say microservices are right way for AI-native development
- Could see a flip where highly distributed becomes most productive
- But right now, centralized architectures see best results
- In highly distributed architectures, takes more PRs overall due to migrations and cross-repo coordination

**Key Points:**
- Code architecture is a particularly interesting factor affecting results
- Context is the key challenge for distributed architectures
- Track change in PR throughput, not absolute numbers (varies by architecture)

---

### [Recap and Key Takeaways](https://www.youtube.com/watch?v=WqZq8L-v9pA&t=998s)
**[16:38 - 17:56]**

1. **AI coding tools being used in a big way** - Autonomous agents still early days
2. **Big productivity gains** - More code shipped faster with interactive AI tools
3. **Expect 2x PR throughput** - Or more with full adoption
4. **Expect bigger PRs** - 18% larger on average
5. **Quality anxiety can ease** - No big issues observed yet
6. **Your mileage may vary** - Code architecture significantly impacts results
7. **Think about code architecture** - Consider how it might hold you back and what you can do about context limitations

**Final Message:**
Start thinking about your code architecture, how it might be holding you back, what you can do to compensate for context limitations, and how to unlock sweet AI productivity gains.

---

**Speaker:** Nicholas Arcolano, Head of Research at Jellyfish

**Duration:** ~18 minutes
