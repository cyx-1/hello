# A New Kind of AI Is Emerging And Its Better Than LLMS

**Video URL:** https://youtu.be/Cis57hC3KcM?si=JZjcY_i8oC8JfctT

---

## Full Transcript

### [00:00 - 01:00]

**[00:02]** So Meta's AI chief released a new paper.

**[00:02]** So Meta's AI chief released a new paper. And is this the beginning of the end for

**[00:04]** And is this the beginning of the end for

**[00:04]** And is this the beginning of the end for LM? Let's talk about it. So most of you

**[00:07]** LM? Let's talk about it. So most of you

**[00:07]** LM? Let's talk about it. So most of you guys know that Meta's AI chief scientist

**[00:08]** guys know that Meta's AI chief scientist

**[00:08]** guys know that Meta's AI chief scientist Yan Lun reportedly left Meta or is

**[00:11]** Yan Lun reportedly left Meta or is

**[00:11]** Yan Lun reportedly left Meta or is leaving Meta to build his own AI

**[00:12]** leaving Meta to build his own AI

**[00:12]** leaving Meta to build his own AI startup. But before that, he actually

**[00:14]** startup. But before that, he actually

**[00:14]** startup. But before that, he actually made a really interesting paper that I

**[00:17]** made a really interesting paper that I

**[00:17]** made a really interesting paper that I want to talk about. So the paper that he

**[00:19]** want to talk about. So the paper that he

**[00:19]** want to talk about. So the paper that he made with a bunch of different

**[00:20]** made with a bunch of different

**[00:20]** made with a bunch of different researchers from Meta is called VLJ. So

**[00:24]** researchers from Meta is called VLJ. So

**[00:24]** researchers from Meta is called VLJ. So this is a vision language model built on

**[00:27]** this is a vision language model built on

**[00:27]** this is a vision language model built on joint embedding predictive architecture

**[00:28]** joint embedding predictive architecture

**[00:28]** joint embedding predictive architecture which is Jepper and this is I guess you

**[00:30]** which is Jepper and this is I guess you

**[00:30]** which is Jepper and this is I guess you could say an extension of the VJA

**[00:32]** could say an extension of the VJA

**[00:32]** could say an extension of the VJA architecture. So this is really cool

**[00:35]** architecture. So this is really cool

**[00:35]** architecture. So this is really cool because this is from Meta's fair lab of

**[00:37]** because this is from Meta's fair lab of

**[00:37]** because this is from Meta's fair lab of course you know Lean Land is the one

**[00:39]** course you know Lean Land is the one

**[00:39]** course you know Lean Land is the one leading this and the you know ridiculous

**[00:41]** leading this and the you know ridiculous

**[00:41]** leading this and the you know ridiculous thing about this well not ridiculous but

**[00:43]** thing about this well not ridiculous but

**[00:43]** thing about this well not ridiculous but the super super interesting that I found

**[00:44]** the super super interesting that I found

**[00:44]** the super super interesting that I found about this is that unlike models like

**[00:46]** about this is that unlike models like

**[00:46]** about this is that unlike models like Chachi that generate answers word by

**[00:48]** Chachi that generate answers word by

**[00:48]** Chachi that generate answers word by word VLJ does something completely

**[00:51]** word VLJ does something completely

**[00:51]** word VLJ does something completely different. This is a non-generative

**[00:53]** different. This is a non-generative

**[00:53]** different. This is a non-generative model. So this predicts meaning directly

**[00:57]** model. So this predicts meaning directly

**[00:57]** model. So this predicts meaning directly and it's not via text. So this model

**[00:59]** and it's not via text. So this model

**[00:59]** and it's not via text. So this model builds an internal understanding of what


### [01:00 - 02:00]

**[01:01]** builds an internal understanding of what

**[01:01]** builds an internal understanding of what it sees, images, video, and then

**[01:03]** it sees, images, video, and then

**[01:03]** it sees, images, video, and then converts that understanding into words

**[01:05]** converts that understanding into words

**[01:05]** converts that understanding into words if needed. Now, because it learns in a

**[01:07]** if needed. Now, because it learns in a

**[01:07]** if needed. Now, because it learns in a semantic space instead of token space,

**[01:09]** semantic space instead of token space,

**[01:09]** semantic space instead of token space, it's faster, more efficient, and uses

**[01:11]** it's faster, more efficient, and uses

**[01:11]** it's faster, more efficient, and uses about half the parameters of traditional

**[01:13]** about half the parameters of traditional

**[01:13]** about half the parameters of traditional vision language models while often

**[01:15]** vision language models while often

**[01:15]** vision language models while often performing better. And this is crazy

**[01:18]** performing better. And this is crazy

**[01:18]** performing better. And this is crazy because what this means for robotics

**[01:20]** because what this means for robotics

**[01:20]** because what this means for robotics agent is super crazy. So let's get into

**[01:22]** agent is super crazy. So let's get into

**[01:22]** agent is super crazy. So let's get into this. So one of the things I wanted to

**[01:23]** this. So one of the things I wanted to

**[01:23]** this. So one of the things I wanted to you know really point out here to show

**[01:25]** you know really point out here to show

**[01:25]** you know really point out here to show you guys how you know different this

**[01:26]** you guys how you know different this

**[01:26]** you guys how you know different this architecture is is that it talks about

**[01:28]** architecture is is that it talks about

**[01:28]** architecture is is that it talks about the fact that this is a non-generative

**[01:30]** the fact that this is a non-generative

**[01:30]** the fact that this is a non-generative system. So if you know what a generative

**[01:32]** system. So if you know what a generative

**[01:32]** system. So if you know what a generative system is usually this means a

**[01:34]** system is usually this means a

**[01:34]** system is usually this means a generative model like chat GPT GPT4 this

**[01:37]** generative model like chat GPT GPT4 this

**[01:37]** generative model like chat GPT GPT4 this produces tokens or words you know one at

**[01:39]** produces tokens or words you know one at

**[01:39]** produces tokens or words you know one at a time you know you go from left to

**[01:40]** a time you know you go from left to

**[01:40]** a time you know you go from left to right and every output must be fully

**[01:42]** right and every output must be fully

**[01:42]** right and every output must be fully written to exist. So to answer what's

**[01:45]** written to exist. So to answer what's

**[01:45]** written to exist. So to answer what's happening in this video, a generative

**[01:46]** happening in this video, a generative

**[01:46]** happening in this video, a generative model is going to be like, okay, I'm

**[01:47]** model is going to be like, okay, I'm

**[01:47]** model is going to be like, okay, I'm going to decide the first word, then the

**[01:49]** going to decide the first word, then the

**[01:49]** going to decide the first word, then the second, then the third until it finishes

**[01:51]** second, then the third until it finishes

**[01:51]** second, then the third until it finishes the entire sentence. It literally, you

**[01:52]** the entire sentence. It literally, you

**[01:52]** the entire sentence. It literally, you know, it can't know the final answer

**[01:54]** know, it can't know the final answer

**[01:54]** know, it can't know the final answer until it finishes generating it, which

**[01:56]** until it finishes generating it, which

**[01:56]** until it finishes generating it, which is very slow and very painful. But a

**[01:58]** is very slow and very painful. But a

**[01:58]** is very slow and very painful. But a non-generative system means here is that


### [02:00 - 03:00]

**[02:00]** non-generative system means here is that

**[02:00]** non-generative system means here is that it does not need to talk to think. So

**[02:02]** it does not need to talk to think. So

**[02:02]** it does not need to talk to think. So VJA essentially what it does is that it

**[02:04]** VJA essentially what it does is that it

**[02:04]** VJA essentially what it does is that it does not generate words by default. It

**[02:05]** does not generate words by default. It

**[02:05]** does not generate words by default. It doesn't predict the next token. It

**[02:07]** doesn't predict the next token. It

**[02:07]** doesn't predict the next token. It doesn't need sentences to exist.

**[02:09]** doesn't need sentences to exist.

**[02:09]** doesn't need sentences to exist. Instead, it predicts a meaning vector

**[02:10]** Instead, it predicts a meaning vector

**[02:10]** Instead, it predicts a meaning vector directly. So think of the differences

**[02:12]** directly. So think of the differences

**[02:12]** directly. So think of the differences like this. generative AI is let me

**[02:14]** like this. generative AI is let me

**[02:14]** like this. generative AI is let me explain what I think while I'm still

**[02:15]** explain what I think while I'm still

**[02:15]** explain what I think while I'm still figuring it out and non-generative AI

**[02:17]** figuring it out and non-generative AI

**[02:17]** figuring it out and non-generative AI says you know I already know and I'll

**[02:18]** says you know I already know and I'll

**[02:18]** says you know I already know and I'll only explain if you ask and compared to

**[02:21]** only explain if you ask and compared to

**[02:21]** only explain if you ask and compared to and remember this is the entire reason

**[02:23]** and remember this is the entire reason

**[02:23]** and remember this is the entire reason that Yanlakan cares about this so much

**[02:25]** that Yanlakan cares about this so much

**[02:25]** that Yanlakan cares about this so much is because he has been saying for so

**[02:27]** is because he has been saying for so

**[02:27]** is because he has been saying for so long that language is not intelligence

**[02:29]** long that language is not intelligence

**[02:29]** long that language is not intelligence his belief is that intelligence equals

**[02:31]** his belief is that intelligence equals

**[02:31]** his belief is that intelligence equals understanding the world and language is

**[02:33]** understanding the world and language is

**[02:33]** understanding the world and language is simply just an output format but Vla

**[02:35]** simply just an output format but Vla

**[02:36]** simply just an output format but Vla reflects that philosophy exactly so this

**[02:38]** reflects that philosophy exactly so this

**[02:38]** reflects that philosophy exactly so this is why this video is talking about what

**[02:41]** is why this video is talking about what

**[02:41]** is why this video is talking about what this might be after LLMs where you're

**[02:43]** this might be after LLMs where you're

**[02:43]** this might be after LLMs where you're thinking in language, reasoning in

**[02:45]** thinking in language, reasoning in

**[02:45]** thinking in language, reasoning in tokens, [music] and where you're

**[02:47]** tokens, [music] and where you're

**[02:47]** tokens, [music] and where you're thinking in the latent space, reasoning

**[02:49]** thinking in the latent space, reasoning

**[02:49]** thinking in the latent space, reasoning in meaning, and language is actually

**[02:51]** in meaning, and language is actually

**[02:51]** in meaning, and language is actually optional. This is the paradigm shift

**[02:52]** optional. This is the paradigm shift

**[02:52]** optional. This is the paradigm shift that this paper is talking about. And I

**[02:54]** that this paper is talking about. And I

**[02:54]** that this paper is talking about. And I think that maybe, just maybe, if this

**[02:56]** think that maybe, just maybe, if this

**[02:56]** think that maybe, just maybe, if this gains more traction, this could be post

**[02:57]** gains more traction, this could be post

**[02:57]** gains more traction, this could be post LLMs. So, essentially what you're

**[02:59]** LLMs. So, essentially what you're

**[02:59]** LLMs. So, essentially what you're looking at in this video is where you


### [03:00 - 04:00]

**[03:01]** looking at in this video is where you

**[03:01]** looking at in this video is where you have a map of the internal understanding

**[03:05]** have a map of the internal understanding

**[03:05]** have a map of the internal understanding over time. So, each dot is essentially

**[03:07]** over time. So, each dot is essentially

**[03:07]** over time. So, each dot is essentially what the AI thinks is happening at that

**[03:09]** what the AI thinks is happening at that

**[03:09]** what the AI thinks is happening at that moment. So you can see the red ones,

**[03:11]** moment. So you can see the red ones,

**[03:11]** moment. So you can see the red ones, those are basically the instant guesses,

**[03:13]** those are basically the instant guesses,

**[03:13]** those are basically the instant guesses, but the blue is essentially the

**[03:14]** but the blue is essentially the

**[03:14]** but the blue is essentially the stabilized understanding. So you have to

**[03:16]** stabilized understanding. So you have to

**[03:16]** stabilized understanding. So you have to understand that what you're seeing on

**[03:17]** understand that what you're seeing on

**[03:17]** understand that what you're seeing on the left is essentially the vision

**[03:19]** the left is essentially the vision

**[03:19]** the left is essentially the vision model, what it would be able to see.

**[03:20]** model, what it would be able to see.

**[03:20]** model, what it would be able to see. Now, now what most people are going to

**[03:22]** Now, now what most people are going to

**[03:22]** Now, now what most people are going to ask here is how is this even different

**[03:23]** ask here is how is this even different

**[03:23]** ask here is how is this even different from a cheap vision model just

**[03:25]** from a cheap vision model just

**[03:25]** from a cheap vision model just describing exactly what the video is

**[03:26]** describing exactly what the video is

**[03:26]** describing exactly what the video is doing. Well, the short answer is that

**[03:28]** doing. Well, the short answer is that

**[03:28]** doing. Well, the short answer is that cheap models, they talk, but VLJ is

**[03:30]** cheap models, they talk, but VLJ is

**[03:30]** cheap models, they talk, but VLJ is understanding. So we need to break down

**[03:32]** understanding. So we need to break down

**[03:32]** understanding. So we need to break down exactly what that means. So the lowcost

**[03:34]** exactly what that means. So the lowcost

**[03:34]** exactly what that means. So the lowcost vision model, the describer is basically

**[03:36]** vision model, the describer is basically

**[03:36]** vision model, the describer is basically a cheap basic vision model that works

**[03:37]** a cheap basic vision model that works

**[03:38]** a cheap basic vision model that works like this. You have the frame, then you

**[03:39]** like this. You have the frame, then you

**[03:39]** like this. You have the frame, then you have the label, then you have the frame,

**[03:41]** have the label, then you have the frame,

**[03:41]** have the label, then you have the frame, then label, then frame, then label. So,

**[03:42]** then label, then frame, then label. So,

**[03:42]** then label, then frame, then label. So, it looks at each frame, it guesses what

**[03:44]** it looks at each frame, it guesses what

**[03:44]** it looks at each frame, it guesses what it sees, and it spits out the text

**[03:45]** it sees, and it spits out the text

**[03:45]** it sees, and it spits out the text immediately. So, this is, you know, what

**[03:47]** immediately. So, this is, you know, what

**[03:47]** immediately. So, this is, you know, what does that look like? Hand, bottle,

**[03:48]** does that look like? Hand, bottle,

**[03:48]** does that look like? Hand, bottle, picking up canister, and it's jumpy,

**[03:50]** picking up canister, and it's jumpy,

**[03:50]** picking up canister, and it's jumpy, inconsistent with no memory, and it's

**[03:52]** inconsistent with no memory, and it's

**[03:52]** inconsistent with no memory, and it's basically reacting and not

**[03:53]** basically reacting and not

**[03:53]** basically reacting and not understanding. But this is where we have

**[03:55]** understanding. But this is where we have

**[03:55]** understanding. But this is where we have VLJ. So, Vlja does this instead. It's

**[03:58]** VLJ. So, Vlja does this instead. It's

**[03:58]** VLJ. So, Vlja does this instead. It's got a video stream, of course, and it's


### [04:00 - 05:00]

**[04:00]** got a video stream, of course, and it's

**[04:00]** got a video stream, of course, and it's got continuous meaning, and then it's

**[04:01]** got continuous meaning, and then it's

**[04:02]** got continuous meaning, and then it's the event. So this tracks the meaning

**[04:04]** the event. So this tracks the meaning

**[04:04]** the event. So this tracks the meaning over time building a stable

**[04:06]** over time building a stable

**[04:06]** over time building a stable understanding and it only labels the

**[04:08]** understanding and it only labels the

**[04:08]** understanding and it only labels the action once it's confident. That's why

**[04:10]** action once it's confident. That's why

**[04:10]** action once it's confident. That's why you see red dot which is an instant

**[04:11]** you see red dot which is an instant

**[04:11]** you see red dot which is an instant guess. Well, it might be wrong. It might

**[04:13]** guess. Well, it might be wrong. It might

**[04:13]** guess. Well, it might be wrong. It might be bottle. But then the blue dot is a

**[04:15]** be bottle. But then the blue dot is a

**[04:15]** be bottle. But then the blue dot is a stabilized meaning it's a canister. So

**[04:16]** stabilized meaning it's a canister. So

**[04:16]** stabilized meaning it's a canister. So the reason that this actually matters a

**[04:18]** the reason that this actually matters a

**[04:18]** the reason that this actually matters a lot is because the cheap model is going

**[04:19]** lot is because the cheap model is going

**[04:19]** lot is because the cheap model is going to say I see a bottle. I see a bottle. I

**[04:21]** to say I see a bottle. I see a bottle. I

**[04:21]** to say I see a bottle. I see a bottle. I see a bottle. But then VLJ is going to

**[04:23]** see a bottle. But then VLJ is going to

**[04:23]** see a bottle. But then VLJ is going to actually understand the action and say

**[04:25]** actually understand the action and say

**[04:25]** actually understand the action and say the action is picking up a canister. So

**[04:27]** the action is picking up a canister. So

**[04:27]** the action is picking up a canister. So the killer difference is of course time.

**[04:29]** the killer difference is of course time.

**[04:29]** the killer difference is of course time. Lowcost models think in single frames

**[04:31]** Lowcost models think in single frames

**[04:31]** Lowcost models think in single frames and they have no real sense of before

**[04:33]** and they have no real sense of before

**[04:33]** and they have no real sense of before and after. VLJ thinks in temporal

**[04:35]** and after. VLJ thinks in temporal

**[04:35]** and after. VLJ thinks in temporal meaning and it knows when an action

**[04:37]** meaning and it knows when an action

**[04:37]** meaning and it knows when an action starts, continues and ends. That's why

**[04:39]** starts, continues and ends. That's why

**[04:39]** starts, continues and ends. That's why it's extremely useful for robotics,

**[04:41]** it's extremely useful for robotics,

**[04:41]** it's extremely useful for robotics, wearables, agents, real world planning.

**[04:43]** wearables, agents, real world planning.

**[04:43]** wearables, agents, real world planning. And why the dot cloud matters is that

**[04:45]** And why the dot cloud matters is that

**[04:45]** And why the dot cloud matters is that you know it's showing you know meaning

**[04:47]** you know it's showing you know meaning

**[04:47]** you know it's showing you know meaning drifting slightly from frame to frame

**[04:48]** drifting slightly from frame to frame

**[04:48]** drifting slightly from frame to frame then locking in once enough evidence

**[04:50]** then locking in once enough evidence

**[04:50]** then locking in once enough evidence exists. And this is something that you

**[04:52]** exists. And this is something that you

**[04:52]** exists. And this is something that you know the tokenbased models they can't

**[04:53]** know the tokenbased models they can't

**[04:53]** know the tokenbased models they can't really do efficiently because number one

**[04:55]** really do efficiently because number one

**[04:55]** really do efficiently because number one they need to you know keep generating

**[04:57]** they need to you know keep generating

**[04:57]** they need to you know keep generating text and number two they can't hold

**[04:59]** text and number two they can't hold

**[04:59]** text and number two they can't hold silent semantic state. So you know if


### [05:00 - 06:00]

**[05:01]** silent semantic state. So you know if

**[05:01]** silent semantic state. So you know if you think about it a cheap model is

**[05:03]** you think about it a cheap model is

**[05:03]** you think about it a cheap model is basically like a CCTV motion detector

**[05:05]** basically like a CCTV motion detector

**[05:05]** basically like a CCTV motion detector shouting guesses but a VLJ is a human

**[05:07]** shouting guesses but a VLJ is a human

**[05:07]** shouting guesses but a VLJ is a human watching and saying ah okay he's he's

**[05:09]** watching and saying ah okay he's he's

**[05:09]** watching and saying ah okay he's he's picking something up. So then of course

**[05:11]** picking something up. So then of course

**[05:11]** picking something up. So then of course you might want to understand the diagram

**[05:12]** you might want to understand the diagram

**[05:12]** you might want to understand the diagram of the architecture. So this is the VLJ

**[05:15]** of the architecture. So this is the VLJ

**[05:15]** of the architecture. So this is the VLJ model architecture. So if you wanted to

**[05:17]** model architecture. So if you wanted to

**[05:17]** model architecture. So if you wanted to know how this works, this is basically

**[05:19]** know how this works, this is basically

**[05:19]** know how this works, this is basically the architecture. But honestly, it was a

**[05:21]** the architecture. But honestly, it was a

**[05:21]** the architecture. But honestly, it was a little bit confusing. So I decided to

**[05:22]** little bit confusing. So I decided to

**[05:22]** little bit confusing. So I decided to just get a simpler description. So I

**[05:25]** just get a simpler description. So I

**[05:25]** just get a simpler description. So I actually used GPT image 1.5 to get this

**[05:28]** actually used GPT image 1.5 to get this

**[05:28]** actually used GPT image 1.5 to get this image because this is actually pretty

**[05:30]** image because this is actually pretty

**[05:30]** image because this is actually pretty good. And if you know this is too much,

**[05:32]** good. And if you know this is too much,

**[05:32]** good. And if you know this is too much, I also have this one right here. So

**[05:34]** I also have this one right here. So

**[05:34]** I also have this one right here. So language is optional, understanding is

**[05:36]** language is optional, understanding is

**[05:36]** language is optional, understanding is not. So basically, you know, the X

**[05:38]** not. So basically, you know, the X

**[05:38]** not. So basically, you know, the X encoder is the visual input. So it's

**[05:40]** encoder is the visual input. So it's

**[05:40]** encoder is the visual input. So it's going to be the video frames. The

**[05:41]** going to be the video frames. The

**[05:41]** going to be the video frames. The predictor is basically the brain. The

**[05:43]** predictor is basically the brain. The

**[05:43]** predictor is basically the brain. The Yen encoder is the textual query which

**[05:45]** Yen encoder is the textual query which

**[05:45]** Yen encoder is the textual query which is what you'd be asking it. And then of

**[05:47]** is what you'd be asking it. And then of

**[05:47]** is what you'd be asking it. And then of course you've got the encoded meanings

**[05:48]** course you've got the encoded meanings

**[05:48]** course you've got the encoded meanings from the word which is the Y decoder.

**[05:50]** from the word which is the Y decoder.

**[05:50]** from the word which is the Y decoder. Then of course you've got your comparing

**[05:52]** Then of course you've got your comparing

**[05:52]** Then of course you've got your comparing the thoughts which is a training loss

**[05:53]** the thoughts which is a training loss

**[05:53]** the thoughts which is a training loss which essentially means that you know

**[05:55]** which essentially means that you know

**[05:55]** which essentially means that you know it's getting better over time. And then

**[05:56]** it's getting better over time. And then

**[05:56]** it's getting better over time. And then of course you got the final output which

**[05:58]** of course you got the final output which

**[05:58]** of course you got the final output which is the correct answer which is the


### [06:00 - 07:00]

**[06:00]** is the correct answer which is the

**[06:00]** is the correct answer which is the actual meaning. Now if we look at the

**[06:02]** actual meaning. Now if we look at the

**[06:02]** actual meaning. Now if we look at the tests of this is currently the best. So

**[06:05]** tests of this is currently the best. So

**[06:05]** tests of this is currently the best. So we're looking at the scoreboard which is

**[06:06]** we're looking at the scoreboard which is

**[06:06]** we're looking at the scoreboard which is where we can see the other ones the

**[06:08]** where we can see the other ones the

**[06:08]** where we can see the other ones the different AI models. We can see that

**[06:09]** different AI models. We can see that

**[06:10]** different AI models. We can see that clip sig LP and P core. They're older

**[06:12]** clip sig LP and P core. They're older

**[06:12]** clip sig LP and P core. They're older well-known vision models and compared to

**[06:14]** well-known vision models and compared to

**[06:14]** well-known vision models and compared to VLJ base this is and VJA SFT which is

**[06:17]** VLJ base this is and VJA SFT which is

**[06:18]** VLJ base this is and VJA SFT which is you know fine-tuning and then we can see

**[06:19]** you know fine-tuning and then we can see

**[06:19]** you know fine-tuning and then we can see that VJER is a really really incredible

**[06:22]** that VJER is a really really incredible

**[06:22]** that VJER is a really really incredible improvement and one of the things I

**[06:24]** improvement and one of the things I

**[06:24]** improvement and one of the things I think you know a lot of people are going

**[06:25]** think you know a lot of people are going

**[06:25]** think you know a lot of people are going to miss is that of course you're

**[06:27]** to miss is that of course you're

**[06:27]** to miss is that of course you're probably going to miss the fact that VLJ

**[06:29]** probably going to miss the fact that VLJ

**[06:29]** probably going to miss the fact that VLJ is super super small so you know how

**[06:31]** is super super small so you know how

**[06:32]** is super super small so you know how generative models just you know tokens

**[06:33]** generative models just you know tokens

**[06:33]** generative models just you know tokens on tokens and tokens but if you're

**[06:35]** on tokens and tokens but if you're

**[06:35]** on tokens and tokens but if you're thinking about something that actually

**[06:36]** thinking about something that actually

**[06:36]** thinking about something that actually reasons like a human you can see that

**[06:38]** reasons like a human you can see that

**[06:38]** reasons like a human you can see that the number of parameters and number of

**[06:40]** the number of parameters and number of

**[06:40]** the number of parameters and number of samples seen you can see that VL jpa is

**[06:42]** samples seen you can see that VL jpa is

**[06:42]** samples seen you can see that VL jpa is 1.6 billion parameters and 2 billion

**[06:45]** 1.6 billion parameters and 2 billion

**[06:45]** 1.6 billion parameters and 2 billion parameters you know in terms of the

**[06:46]** parameters you know in terms of the

**[06:46]** parameters you know in terms of the sample scene. So it's remarkably more

**[06:49]** sample scene. So it's remarkably more

**[06:49]** sample scene. So it's remarkably more efficient than the other things that

**[06:50]** efficient than the other things that

**[06:50]** efficient than the other things that we're you know looking at. So I think

**[06:52]** we're you know looking at. So I think

**[06:52]** we're you know looking at. So I think it's I think it's pretty incredible how

**[06:54]** it's I think it's pretty incredible how

**[06:54]** it's I think it's pretty incredible how that is. I mean if we you know continue

**[06:56]** that is. I mean if we you know continue

**[06:56]** that is. I mean if we you know continue to look over here you can see that the

**[06:58]** to look over here you can see that the

**[06:58]** to look over here you can see that the zero shot video captioning. So this is


### [07:00 - 08:00]

**[07:00]** zero shot video captioning. So this is

**[07:00]** zero shot video captioning. So this is where it's showing with the same data

**[07:01]** where it's showing with the same data

**[07:01]** where it's showing with the same data and same setup VOJepper actually learns

**[07:04]** and same setup VOJepper actually learns

**[07:04]** and same setup VOJepper actually learns faster and it reaches higher caption

**[07:05]** faster and it reaches higher caption

**[07:05]** faster and it reaches higher caption quality and predicting meaning you know

**[07:07]** quality and predicting meaning you know

**[07:07]** quality and predicting meaning you know learns faster than predicting words.

**[07:09]** learns faster than predicting words.

**[07:09]** learns faster than predicting words. Then of course you've got chart two

**[07:10]** Then of course you've got chart two

**[07:10]** Then of course you've got chart two which is zeroot video classification and

**[07:12]** which is zeroot video classification and

**[07:12]** which is zeroot video classification and it's the same thing VLJ pulls quickly

**[07:15]** it's the same thing VLJ pulls quickly

**[07:15]** it's the same thing VLJ pulls quickly ahead and the visual language models

**[07:16]** ahead and the visual language models

**[07:16]** ahead and the visual language models improve very slowly. So even without

**[07:18]** improve very slowly. So even without

**[07:18]** improve very slowly. So even without fine-tuning VJ understands videos better

**[07:21]** fine-tuning VJ understands videos better

**[07:21]** fine-tuning VJ understands videos better and this kills the idea that you need

**[07:23]** and this kills the idea that you need

**[07:23]** and this kills the idea that you need token generation to understand things

**[07:25]** token generation to understand things

**[07:25]** token generation to understand things and it you know it's clear it's clear

**[07:26]** and it you know it's clear it's clear

**[07:26]** and it you know it's clear it's clear that you know Yandan is on to something.

**[07:29]** that you know Yandan is on to something.

**[07:29]** that you know Yandan is on to something. So once again if we look at the right

**[07:31]** So once again if we look at the right

**[07:31]** So once again if we look at the right size remember once I said that again.

**[07:32]** size remember once I said that again.

**[07:32]** size remember once I said that again. Now remember once I said that if you

**[07:34]** Now remember once I said that if you

**[07:34]** Now remember once I said that if you look at the actual size of the models

**[07:35]** look at the actual size of the models

**[07:35]** look at the actual size of the models you can see that once again visual

**[07:36]** you can see that once again visual

**[07:36]** you can see that once again visual language models are you know much larger

**[07:39]** language models are you know much larger

**[07:39]** language models are you know much larger and much less efficient and vjer only

**[07:41]** and much less efficient and vjer only

**[07:42]** and much less efficient and vjer only needs like 0.5 billion parameters in

**[07:44]** needs like 0.5 billion parameters in

**[07:44]** needs like 0.5 billion parameters in terms of their predictor and so there's

**[07:46]** terms of their predictor and so there's

**[07:46]** terms of their predictor and so there's no heavy decoder during training. So

**[07:48]** no heavy decoder during training. So

**[07:48]** no heavy decoder during training. So VJepper is going to get better with

**[07:49]** VJepper is going to get better with

**[07:49]** VJepper is going to get better with results with half the trainable

**[07:50]** results with half the trainable

**[07:50]** results with half the trainable parameters which is pretty insane in

**[07:52]** parameters which is pretty insane in

**[07:52]** parameters which is pretty insane in machine learning terms. And of course

**[07:54]** machine learning terms. And of course

**[07:54]** machine learning terms. And of course here we have Yan Lerna talking about

**[07:56]** here we have Yan Lerna talking about

**[07:56]** here we have Yan Lerna talking about this stuff. I mean, this was I think

**[07:57]** this stuff. I mean, this was I think

**[07:57]** this stuff. I mean, this was I think around two to three weeks ago.

**[07:59]** around two to three weeks ago.

**[07:59]** around two to three weeks ago. >> Four-year-old has seen as much visual


### [08:00 - 09:00]

**[08:02]** >> Four-year-old has seen as much visual

**[08:02]** >> Four-year-old has seen as much visual data as the biggest LLM trained on the

**[08:05]** data as the biggest LLM trained on the

**[08:05]** data as the biggest LLM trained on the entire text ever produced. And so what

**[08:07]** entire text ever produced. And so what

**[08:08]** entire text ever produced. And so what that tells you is that there is way more

**[08:11]** that tells you is that there is way more

**[08:11]** that tells you is that there is way more um information in the real world, but

**[08:13]** um information in the real world, but

**[08:13]** um information in the real world, but it's also much more complicated. It's

**[08:16]** it's also much more complicated. It's

**[08:16]** it's also much more complicated. It's noisy. It's high dimensional. It's

**[08:18]** noisy. It's high dimensional. It's

**[08:18]** noisy. It's high dimensional. It's continuous. And basically the methods

**[08:20]** continuous. And basically the methods

**[08:20]** continuous. And basically the methods that are employed to train LLMs do not

**[08:23]** that are employed to train LLMs do not

**[08:23]** that are employed to train LLMs do not work in the real world. That explains

**[08:26]** work in the real world. That explains

**[08:26]** work in the real world. That explains why we have LLMs that can pass the bar

**[08:29]** why we have LLMs that can pass the bar

**[08:29]** why we have LLMs that can pass the bar exam or solve equations or compute

**[08:32]** exam or solve equations or compute

**[08:32]** exam or solve equations or compute integrals like college students and

**[08:34]** integrals like college students and

**[08:34]** integrals like college students and solve math problems. But we still don't

**[08:36]** solve math problems. But we still don't

**[08:36]** solve math problems. But we still don't have a domestic robot. They can, you

**[08:39]** have a domestic robot. They can, you

**[08:39]** have a domestic robot. They can, you know, do the chores in the house. We

**[08:40]** know, do the chores in the house. We

**[08:40]** know, do the chores in the house. We don't we don't even have level five

**[08:42]** don't we don't even have level five

**[08:42]** don't we don't even have level five self-driving cars. I mean, we have them,

**[08:44]** self-driving cars. I mean, we have them,

**[08:44]** self-driving cars. I mean, we have them, but but we cheat. So, um I mean, we

**[08:47]** but but we cheat. So, um I mean, we

**[08:47]** but but we cheat. So, um I mean, we certainly don't have self-driving cars

**[08:48]** certainly don't have self-driving cars

**[08:48]** certainly don't have self-driving cars that can learn to drive in 20 hours of

**[08:51]** that can learn to drive in 20 hours of

**[08:51]** that can learn to drive in 20 hours of practice like any teenager. And then of

**[08:53]** practice like any teenager. And then of

**[08:53]** practice like any teenager. And then of course I actually went on Yelican's

**[08:55]** course I actually went on Yelican's

**[08:55]** course I actually went on Yelican's Twitter and I saw him uh reposting this

**[08:57]** Twitter and I saw him uh reposting this

**[08:58]** Twitter and I saw him uh reposting this from Sonia Joseph. Now this is someone

**[08:59]** from Sonia Joseph. Now this is someone

**[08:59]** from Sonia Joseph. Now this is someone of course that works at Meta and she


### [09:00 - 10:00]

**[09:01]** of course that works at Meta and she

**[09:01]** of course that works at Meta and she essentially said that we don't simulate

**[09:02]** essentially said that we don't simulate

**[09:02]** essentially said that we don't simulate every atom to model intelligence. We

**[09:04]** every atom to model intelligence. We

**[09:04]** every atom to model intelligence. We don't use quantum field theory to model

**[09:06]** don't use quantum field theory to model

**[09:06]** don't use quantum field theory to model road traffic. Jeepa taught me the

**[09:07]** road traffic. Jeepa taught me the

**[09:07]** road traffic. Jeepa taught me the importance of learning physics at the

**[09:08]** importance of learning physics at the

**[09:08]** importance of learning physics at the right level of abstraction. Thank you

**[09:10]** right level of abstraction. Thank you

**[09:10]** right level of abstraction. Thank you Landin and the Jeppa team. It was a

**[09:12]** Landin and the Jeppa team. It was a

**[09:12]** Landin and the Jeppa team. It was a privilege to work with you. So I'll

**[09:13]** privilege to work with you. So I'll

**[09:13]** privilege to work with you. So I'll definitely take a look at this. The

**[09:14]** definitely take a look at this. The

**[09:14]** definitely take a look at this. The thesis behind Japa is that our current

**[09:16]** thesis behind Japa is that our current

**[09:16]** thesis behind Japa is that our current models are not predicting causal

**[09:18]** models are not predicting causal

**[09:18]** models are not predicting causal dynamics. And if you both predict in

**[09:21]** dynamics. And if you both predict in

**[09:21]** dynamics. And if you both predict in latent space and predict the future,

**[09:23]** latent space and predict the future,

**[09:23]** latent space and predict the future, then you're more likely to abstract away

**[09:25]** then you're more likely to abstract away

**[09:25]** then you're more likely to abstract away all these pixel level details. For

**[09:27]** all these pixel level details. For

**[09:27]** all these pixel level details. For [music] example, when we model even this

**[09:29]** [music] example, when we model even this

**[09:29]** [music] example, when we model even this conversation right now, we don't have to

**[09:31]** conversation right now, we don't have to

**[09:31]** conversation right now, we don't have to model it down to the level of atoms.

**[09:33]** model it down to the level of atoms.

**[09:33]** model it down to the level of atoms. That would be so computationally costly

**[09:35]** That would be so computationally costly

**[09:35]** That would be so computationally costly and so efficient. We model things at the

**[09:37]** and so efficient. We model things at the

**[09:37]** and so efficient. We model things at the representation that's suited for our

**[09:39]** representation that's suited for our

**[09:39]** representation that's suited for our goal. So similarly, JEPA is optimi

**[09:43]** goal. So similarly, JEPA is optimi

**[09:43]** goal. So similarly, JEPA is optimi optimized to have [music] physical

**[09:45]** optimized to have [music] physical

**[09:45]** optimized to have [music] physical representations at the level of

**[09:47]** representations at the level of

**[09:47]** representations at the level of abstraction it needs. It enables it to

**[09:48]** abstraction it needs. It enables it to

**[09:48]** abstraction it needs. It enables it to plan in the physical world and be able

**[09:51]** plan in the physical world and be able

**[09:51]** plan in the physical world and be able to do a counterfactual reasoning about

**[09:52]** to do a counterfactual reasoning about

**[09:52]** to do a counterfactual reasoning about objects that are moving around behind

**[09:54]** objects that are moving around behind

**[09:54]** objects that are moving around behind Japa.

**[09:55]** Japa.

**[09:55]** Japa. >> Now I did see a few comments on Reddit

**[09:57]** >> Now I did see a few comments on Reddit

**[09:57]** >> Now I did see a few comments on Reddit talking about the video saying that most

**[09:59]** talking about the video saying that most

**[09:59]** talking about the video saying that most of the actions that it detects are wrong


### [10:00 - 11:00]

**[10:00]** of the actions that it detects are wrong

**[10:00]** of the actions that it detects are wrong though. If you stop the video at any

**[10:02]** though. If you stop the video at any

**[10:02]** though. If you stop the video at any time to actually read what it says, it's

**[10:03]** time to actually read what it says, it's

**[10:03]** time to actually read what it says, it's really bad. And someone also says, well,

**[10:06]** really bad. And someone also says, well,

**[10:06]** really bad. And someone also says, well, the guy, the same guy or the same person

**[10:08]** the guy, the same guy or the same person

**[10:08]** the guy, the same guy or the same person says that I stopped it like five times

**[10:09]** says that I stopped it like five times

**[10:09]** says that I stopped it like five times and they were all wrong. Made up a side

**[10:11]** and they were all wrong. Made up a side

**[10:11]** and they were all wrong. Made up a side of pizza, made up something else. But I

**[10:13]** of pizza, made up something else. But I

**[10:13]** of pizza, made up something else. But I think the most important thing here is

**[10:14]** think the most important thing here is

**[10:14]** think the most important thing here is not that it's going to be 100% right. I

**[10:16]** not that it's going to be 100% right. I

**[10:16]** not that it's going to be 100% right. I think the most important thing is that

**[10:17]** think the most important thing is that

**[10:17]** think the most important thing is that it's actually moving us in the right

**[10:18]** it's actually moving us in the right

**[10:18]** it's actually moving us in the right direction of where AI models should

**[10:21]** direction of where AI models should

**[10:21]** direction of where AI models should actually be and not just getting

**[10:23]** actually be and not just getting

**[10:23]** actually be and not just getting completely distracted by chat bots.


