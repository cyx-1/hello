# The Truth About The AI Bubble

**Video URL:** https://youtu.be/cqrJzG03ENE?si=-HWgLv1k14VRzFuA

---

## Full Transcript

### [00:00 - 01:00]

**[00:01]** I think perhaps the thing that most

**[00:01]** I think perhaps the thing that most surprised me is the extent to which I

**[00:03]** surprised me is the extent to which I

**[00:03]** surprised me is the extent to which I feel like the AI economy stabilized. We

**[00:06]** feel like the AI economy stabilized. We

**[00:06]** feel like the AI economy stabilized. We have like the model layer companies and

**[00:07]** have like the model layer companies and

**[00:07]** have like the model layer companies and the application layer companies and the

**[00:09]** the application layer companies and the

**[00:09]** the application layer companies and the infrastructure layer companies. Seems

**[00:10]** infrastructure layer companies. Seems

**[00:10]** infrastructure layer companies. Seems like everyone is going to make a a lot

**[00:12]** like everyone is going to make a a lot

**[00:12]** like everyone is going to make a a lot of money and there's kind of like a

**[00:13]** of money and there's kind of like a

**[00:13]** of money and there's kind of like a relative playbook for how to build an AI

**[00:15]** relative playbook for how to build an AI

**[00:15]** relative playbook for how to build an AI native company on top of the models.

**[00:17]** native company on top of the models.

**[00:17]** native company on top of the models. Many episodes ago, we talked about how

**[00:18]** Many episodes ago, we talked about how

**[00:18]** Many episodes ago, we talked about how it was felt easier than ever to pivot

**[00:20]** it was felt easier than ever to pivot

**[00:20]** it was felt easier than ever to pivot and find a startup idea because if you

**[00:22]** and find a startup idea because if you

**[00:22]** and find a startup idea because if you could just survive, if you could just

**[00:24]** could just survive, if you could just

**[00:24]** could just survive, if you could just wait a few months, there was likely

**[00:25]** wait a few months, there was likely

**[00:25]** wait a few months, there was likely going to be some like big announcement

**[00:27]** going to be some like big announcement

**[00:27]** going to be some like big announcement that would completely make a new set of

**[00:29]** that would completely make a new set of

**[00:29]** that would completely make a new set of ideas possible. And so like finding

**[00:31]** ideas possible. And so like finding

**[00:31]** ideas possible. And so like finding ideas is sort of returning to sort of

**[00:33]** ideas is sort of returning to sort of

**[00:33]** ideas is sort of returning to sort of normal levels of difficulty.

**[00:44]** Welcome back to another episode of the

**[00:44]** Welcome back to another episode of the light cone. Today we're talking about

**[00:45]** light cone. Today we're talking about

**[00:45]** light cone. Today we're talking about the most surprising things that we saw

**[00:47]** the most surprising things that we saw

**[00:48]** the most surprising things that we saw this year in 2025.

**[00:50]** this year in 2025.

**[00:50]** this year in 2025. Diana, you found a pretty crazy one.

**[00:53]** Diana, you found a pretty crazy one.

**[00:53]** Diana, you found a pretty crazy one. It's sort of a changing of the guard

**[00:54]** It's sort of a changing of the guard

**[00:54]** It's sort of a changing of the guard almost in who is the preferred LLM at YC

**[00:58]** almost in who is the preferred LLM at YC

**[00:58]** almost in who is the preferred LLM at YC during the YC batch.


### [01:00 - 02:00]

**[01:00]** during the YC batch.

**[01:00]** during the YC batch. >> Yes. In fact, we just wrapped up the

**[01:02]** >> Yes. In fact, we just wrapped up the

**[01:02]** >> Yes. In fact, we just wrapped up the winter 26 selection cycle for companies.

**[01:06]** winter 26 selection cycle for companies.

**[01:06]** winter 26 selection cycle for companies. And one of the questions we asked to all

**[01:08]** And one of the questions we asked to all

**[01:08]** And one of the questions we asked to all the founders that apply to YC is what is

**[01:11]** the founders that apply to YC is what is

**[01:11]** the founders that apply to YC is what is your tech stack and model of choice? And

**[01:14]** your tech stack and model of choice? And

**[01:14]** your tech stack and model of choice? And one of the shocking things is that for

**[01:16]** one of the shocking things is that for

**[01:16]** one of the shocking things is that for the longest time, OpenAI was the clear

**[01:19]** the longest time, OpenAI was the clear

**[01:19]** the longest time, OpenAI was the clear winner for all of last year, last couple

**[01:22]** winner for all of last year, last couple

**[01:22]** winner for all of last year, last couple of batches. Though that number has been

**[01:24]** of batches. Though that number has been

**[01:24]** of batches. Though that number has been coming down and shockingly in this

**[01:28]** coming down and shockingly in this

**[01:28]** coming down and shockingly in this batch, the number one API is actually

**[01:32]** batch, the number one API is actually

**[01:32]** batch, the number one API is actually anthropic came out a bit more than

**[01:35]** anthropic came out a bit more than

**[01:35]** anthropic came out a bit more than OpenAI, which who would have thought. I

**[01:37]** OpenAI, which who would have thought. I

**[01:37]** OpenAI, which who would have thought. I think when we started this podcast

**[01:39]** think when we started this podcast

**[01:39]** think when we started this podcast series back then, open eye was like 90

**[01:41]** series back then, open eye was like 90

**[01:42]** series back then, open eye was like 90 plus%.

**[01:43]** plus%.

**[01:43]** plus%. And now anthropic, who would have

**[01:46]** And now anthropic, who would have

**[01:46]** And now anthropic, who would have thought?

**[01:46]** thought?

**[01:46]** thought? >> Yeah. And you know, they'd been hovering

**[01:48]** >> Yeah. And you know, they'd been hovering

**[01:48]** >> Yeah. And you know, they'd been hovering around like 20 25% for most of like 20

**[01:52]** around like 20 25% for most of like 20

**[01:52]** around like 20 25% for most of like 20 2024 and early 2025 and then only even

**[01:56]** 2024 and early 2025 and then only even

**[01:56]** 2024 and early 2025 and then only even in the last 3 to 6 months did this sort


### [02:00 - 03:00]

**[02:00]** in the last 3 to 6 months did this sort

**[02:00]** in the last 3 to 6 months did this sort of changing of the guard actually

**[02:01]** of changing of the guard actually

**[02:01]** of changing of the guard actually happen.

**[02:01]** happen.

**[02:01]** happen. >> They had this uh hockey stick with the

**[02:03]** >> They had this uh hockey stick with the

**[02:03]** >> They had this uh hockey stick with the with the growth are over 52%. Why do you

**[02:07]** with the growth are over 52%. Why do you

**[02:07]** with the growth are over 52%. Why do you think that is?

**[02:07]** think that is?

**[02:07]** think that is? >> I think there's a couple of things in

**[02:09]** >> I think there's a couple of things in

**[02:09]** >> I think there's a couple of things in terms of the text selection. I think as

**[02:11]** terms of the text selection. I think as

**[02:11]** terms of the text selection. I think as we've seen this year, there's been a lot

**[02:14]** we've seen this year, there's been a lot

**[02:14]** we've seen this year, there's been a lot of wins in terms of vibe coding tools

**[02:17]** of wins in terms of vibe coding tools

**[02:17]** of wins in terms of vibe coding tools that are getting built out out there and

**[02:19]** that are getting built out out there and

**[02:20]** that are getting built out out there and coding agents. There's so many

**[02:21]** coding agents. There's so many

**[02:21]** coding agents. There's so many categories that this ended up being a

**[02:23]** categories that this ended up being a

**[02:23]** categories that this ended up being a bigger problem space that actually is

**[02:26]** bigger problem space that actually is

**[02:26]** bigger problem space that actually is creating a lot of value. And it turns

**[02:28]** creating a lot of value. And it turns

**[02:28]** creating a lot of value. And it turns out the model that performs the best at

**[02:30]** out the model that performs the best at

**[02:30]** out the model that performs the best at it is actually uh the models from

**[02:33]** it is actually uh the models from

**[02:33]** it is actually uh the models from Antropic. And I think that's not by

**[02:36]** Antropic. And I think that's not by

**[02:36]** Antropic. And I think that's not by accident. I think from the hearing the

**[02:37]** accident. I think from the hearing the

**[02:37]** accident. I think from the hearing the conversation we had with Tom Brown not

**[02:39]** conversation we had with Tom Brown not

**[02:39]** conversation we had with Tom Brown not too long ago, he came and spoke is that

**[02:41]** too long ago, he came and spoke is that

**[02:41]** too long ago, he came and spoke is that was one of their internal evals. They on

**[02:44]** was one of their internal evals. They on

**[02:44]** was one of their internal evals. They on purpose made them their northstar and

**[02:46]** purpose made them their northstar and

**[02:46]** purpose made them their northstar and you can see it in the model taste as a

**[02:48]** you can see it in the model taste as a

**[02:48]** you can see it in the model taste as a result of what's the best choice of

**[02:52]** result of what's the best choice of

**[02:52]** result of what's the best choice of model for a lot of founders building

**[02:54]** model for a lot of founders building

**[02:54]** model for a lot of founders building products is anthropic.

**[02:56]** products is anthropic.

**[02:56]** products is anthropic. >> The vast majority of the use cases

**[02:58]** >> The vast majority of the use cases

**[02:58]** >> The vast majority of the use cases people are using it for though is not

**[02:59]** people are using it for though is not

**[02:59]** people are using it for though is not coding. So I wonder if there's like a


### [03:00 - 04:00]

**[03:01]** coding. So I wonder if there's like a

**[03:01]** coding. So I wonder if there's like a bleedthrough effect where people are

**[03:02]** bleedthrough effect where people are

**[03:02]** bleedthrough effect where people are using Claude for their personal coding

**[03:05]** using Claude for their personal coding

**[03:05]** using Claude for their personal coding and then as a result they're more likely

**[03:07]** and then as a result they're more likely

**[03:07]** and then as a result they're more likely to choose it for their application even

**[03:08]** to choose it for their application even

**[03:08]** to choose it for their application even if their application is not doing coding

**[03:10]** if their application is not doing coding

**[03:10]** if their application is not doing coding at all

**[03:10]** at all

**[03:10]** at all >> because you'd be very um familiar with

**[03:12]** >> because you'd be very um familiar with

**[03:12]** >> because you'd be very um familiar with like the personality of Claude Opus or

**[03:15]** like the personality of Claude Opus or

**[03:15]** like the personality of Claude Opus or whatever they're choosing.

**[03:16]** whatever they're choosing.

**[03:16]** whatever they're choosing. >> Yeah.

**[03:17]** >> Yeah.

**[03:17]** >> Yeah. >> Sonnet I suppose.

**[03:18]** >> Sonnet I suppose.

**[03:18]** >> Sonnet I suppose. >> How about Gemini? How's Gemini doing in

**[03:20]** >> How about Gemini? How's Gemini doing in

**[03:20]** >> How about Gemini? How's Gemini doing in those rankings?

**[03:21]** those rankings?

**[03:21]** those rankings? >> Gemini is also pretty much has been

**[03:23]** >> Gemini is also pretty much has been

**[03:23]** >> Gemini is also pretty much has been climbing up pretty pretty high. I think

**[03:25]** climbing up pretty pretty high. I think

**[03:25]** climbing up pretty pretty high. I think last year was probably singledigit

**[03:27]** last year was probably singledigit

**[03:27]** last year was probably singledigit percent or even like two 3% and uh now

**[03:30]** percent or even like two 3% and uh now

**[03:30]** percent or even like two 3% and uh now for winter 26 is about 23%.

**[03:34]** for winter 26 is about 23%.

**[03:34]** for winter 26 is about 23%. And uh we personally been using also a

**[03:36]** And uh we personally been using also a

**[03:36]** And uh we personally been using also a lot of Gemini 3.0 and we've been

**[03:38]** lot of Gemini 3.0 and we've been

**[03:38]** lot of Gemini 3.0 and we've been impressed with the with the quality of

**[03:40]** impressed with the with the quality of

**[03:40]** impressed with the with the quality of it. I think it's really really working.

**[03:43]** it. I think it's really really working.

**[03:43]** it. I think it's really really working. >> I mean they have all different

**[03:44]** >> I mean they have all different

**[03:44]** >> I mean they have all different personalities, don't they?

**[03:46]** personalities, don't they?

**[03:46]** personalities, don't they? >> They got too.

**[03:46]** >> They got too.

**[03:46]** >> They got too. >> Yeah. It's uh it's kind of the classic

**[03:48]** >> Yeah. It's uh it's kind of the classic

**[03:48]** >> Yeah. It's uh it's kind of the classic where OpenAI sort of has the uh black

**[03:51]** where OpenAI sort of has the uh black

**[03:51]** where OpenAI sort of has the uh black cat energy and almost like uh Anthropic

**[03:54]** cat energy and almost like uh Anthropic

**[03:54]** cat energy and almost like uh Anthropic is kind of more the happy golucky a bit

**[03:57]** is kind of more the happy golucky a bit

**[03:57]** is kind of more the happy golucky a bit more very helpful golden retriever. At

**[03:59]** more very helpful golden retriever. At

**[03:59]** more very helpful golden retriever. At least that's what I feel when I talk to


### [04:00 - 05:00]

**[04:00]** least that's what I feel when I talk to

**[04:00]** least that's what I feel when I talk to them.

**[04:00]** them.

**[04:00]** them. >> And how about Gemini?

**[04:02]** >> And how about Gemini?

**[04:02]** >> And how about Gemini? >> It's kind of like in between.

**[04:03]** >> It's kind of like in between.

**[04:03]** >> It's kind of like in between. >> Har you prefer Gemini actually.

**[04:06]** >> Har you prefer Gemini actually.

**[04:06]** >> Har you prefer Gemini actually. >> Yeah, I switched to Gemini this year as

**[04:09]** >> Yeah, I switched to Gemini this year as

**[04:09]** >> Yeah, I switched to Gemini this year as my just go-to model. I think even before

**[04:12]** my just go-to model. I think even before

**[04:12]** my just go-to model. I think even before 2.5 Pro came out and just seemed better

**[04:15]** 2.5 Pro came out and just seemed better

**[04:15]** 2.5 Pro came out and just seemed better at reasoning. For me, it was just like

**[04:18]** at reasoning. For me, it was just like

**[04:18]** at reasoning. For me, it was just like the increasingly I replaced my Google

**[04:20]** the increasingly I replaced my Google

**[04:20]** the increasingly I replaced my Google searches with Gemini and I just sort of

**[04:22]** searches with Gemini and I just sort of

**[04:22]** searches with Gemini and I just sort of trusted that Google's I think like the

**[04:25]** trusted that Google's I think like the

**[04:25]** trusted that Google's I think like the groundings API and its ability to

**[04:26]** groundings API and its ability to

**[04:26]** groundings API and its ability to actually like use the Google index to

**[04:28]** actually like use the Google index to

**[04:28]** actually like use the Google index to give you like real time information

**[04:31]** give you like real time information

**[04:31]** give you like real time information correctly. I just found it was better

**[04:32]** correctly. I just found it was better

**[04:32]** correctly. I just found it was better than I personally I found it was better

**[04:34]** than I personally I found it was better

**[04:34]** than I personally I found it was better than all the other tools for that and it

**[04:35]** than all the other tools for that and it

**[04:35]** than all the other tools for that and it was better than Plexity on it too. Like

**[04:37]** was better than Plexity on it too. Like

**[04:37]** was better than Plexity on it too. Like Plexity would be fast but not always

**[04:39]** Plexity would be fast but not always

**[04:39]** Plexity would be fast but not always accurate and Gemini was not quite as

**[04:41]** accurate and Gemini was not quite as

**[04:41]** accurate and Gemini was not quite as fast as perplexity but was always pretty

**[04:42]** fast as perplexity but was always pretty

**[04:42]** fast as perplexity but was always pretty accurate if I asked it about something

**[04:44]** accurate if I asked it about something

**[04:44]** accurate if I asked it about something that happened today.

**[04:45]** that happened today.

**[04:46]** that happened today. >> Even if you use Gemini as the reasoning

**[04:48]** >> Even if you use Gemini as the reasoning

**[04:48]** >> Even if you use Gemini as the reasoning engine in Perplexity.

**[04:50]** engine in Perplexity.

**[04:50]** engine in Perplexity. >> I have not done that.

**[04:51]** >> I have not done that.

**[04:51]** >> I have not done that. >> Interesting. Yeah. So it's hard to know

**[04:53]** >> Interesting. Yeah. So it's hard to know

**[04:53]** >> Interesting. Yeah. So it's hard to know like how much of it is the tooling and

**[04:55]** like how much of it is the tooling and

**[04:55]** like how much of it is the tooling and how much of it is like the base LLM.

**[04:57]** how much of it is like the base LLM.

**[04:57]** how much of it is like the base LLM. >> That's fair.

**[04:57]** >> That's fair.

**[04:58]** >> That's fair. >> Yeah. I mean what are your guys' tools

**[04:59]** >> Yeah. I mean what are your guys' tools


### [05:00 - 06:00]

**[05:00]** >> Yeah. I mean what are your guys' tools of choice? I haven't switched off of

**[05:01]** of choice? I haven't switched off of

**[05:01]** of choice? I haven't switched off of Chat GPT. I mean I find the memory very

**[05:03]** Chat GPT. I mean I find the memory very

**[05:03]** Chat GPT. I mean I find the memory very sticky. It knows me. It knows my

**[05:05]** sticky. It knows me. It knows my

**[05:05]** sticky. It knows me. It knows my personality. it and knows the things

**[05:06]** personality. it and knows the things

**[05:06]** personality. it and knows the things that I think about. And so I'll use

**[05:09]** that I think about. And so I'll use

**[05:09]** that I think about. And so I'll use Perplexity for fast web searches or

**[05:11]** Perplexity for fast web searches or

**[05:12]** Perplexity for fast web searches or things that um you know I know was like

**[05:14]** things that um you know I know was like

**[05:14]** things that um you know I know was like a research task cuz I think Chetchup PT

**[05:16]** a research task cuz I think Chetchup PT

**[05:16]** a research task cuz I think Chetchup PT is still like a little bit of a step

**[05:18]** is still like a little bit of a step

**[05:18]** is still like a little bit of a step behind for searching the web. I don't

**[05:20]** behind for searching the web. I don't

**[05:20]** behind for searching the web. I don't know. I think memory is turning into um

**[05:23]** know. I think memory is turning into um

**[05:23]** know. I think memory is turning into um an actual moat for like that consumer

**[05:26]** an actual moat for like that consumer

**[05:26]** an actual moat for like that consumer experience. And I don't expect Gemini to

**[05:28]** experience. And I don't expect Gemini to

**[05:28]** experience. And I don't expect Gemini to ever have the personality that I would

**[05:31]** ever have the personality that I would

**[05:31]** ever have the personality that I would expect from Chat GPT. It just feels like

**[05:33]** expect from Chat GPT. It just feels like

**[05:33]** expect from Chat GPT. It just feels like a different like entity, you know? The

**[05:35]** a different like entity, you know? The

**[05:35]** a different like entity, you know? The thing I'm still surprised about is why

**[05:37]** thing I'm still surprised about is why

**[05:37]** thing I'm still surprised about is why there just aren't more um consumer apps

**[05:41]** there just aren't more um consumer apps

**[05:41]** there just aren't more um consumer apps around like all the various things we

**[05:43]** around like all the various things we

**[05:44]** around like all the various things we do. Like if I think back, one of the big

**[05:45]** do. Like if I think back, one of the big

**[05:46]** do. Like if I think back, one of the big changes for me this year is just the

**[05:47]** changes for me this year is just the

**[05:47]** changes for me this year is just the amount of prompting and context

**[05:49]** amount of prompting and context

**[05:49]** amount of prompting and context engineering I do for like my life. Like

**[05:52]** engineering I do for like my life. Like

**[05:52]** engineering I do for like my life. Like I we we bought a house recently and like

**[05:54]** I we we bought a house recently and like

**[05:54]** I we we bought a house recently and like the whole thing like I just had like a

**[05:55]** the whole thing like I just had like a

**[05:56]** the whole thing like I just had like a really long running chat GPT

**[05:57]** really long running chat GPT

**[05:57]** really long running chat GPT conversation stuffing it full of context

**[05:59]** conversation stuffing it full of context


### [06:00 - 07:00]

**[06:00]** conversation stuffing it full of context of like every inspection report or like

**[06:02]** of like every inspection report or like

**[06:02]** of like every inspection report or like wanting it to be like level the playing

**[06:04]** wanting it to be like level the playing

**[06:04]** wanting it to be like level the playing field between me and like the realtor to

**[06:06]** field between me and like the realtor to

**[06:06]** field between me and like the realtor to understand kind of all the dynamics and

**[06:08]** understand kind of all the dynamics and

**[06:08]** understand kind of all the dynamics and things that are going on and it just

**[06:09]** things that are going on and it just

**[06:09]** things that are going on and it just feels like there should be an app for

**[06:11]** feels like there should be an app for

**[06:12]** feels like there should be an app for that.

**[06:12]** that.

**[06:12]** that. >> But simultaneously I'm sure you took the

**[06:14]** >> But simultaneously I'm sure you took the

**[06:14]** >> But simultaneously I'm sure you took the uh PDFs and just like dropped them into

**[06:17]** uh PDFs and just like dropped them into

**[06:17]** uh PDFs and just like dropped them into Gemini and said like well summarize and

**[06:20]** Gemini and said like well summarize and

**[06:20]** Gemini and said like well summarize and tell me what's important for me. I guess

**[06:21]** tell me what's important for me. I guess

**[06:21]** tell me what's important for me. I guess I worry about I worried about I still

**[06:24]** I worry about I worried about I still

**[06:24]** I worry about I worried about I still don't trust the models enough to be

**[06:25]** don't trust the models enough to be

**[06:25]** don't trust the models enough to be accurate without lots of prompting and

**[06:27]** accurate without lots of prompting and

**[06:27]** accurate without lots of prompting and it's a high value transaction. So you

**[06:29]** it's a high value transaction. So you

**[06:29]** it's a high value transaction. So you don't want to like get incorrect data

**[06:31]** don't want to like get incorrect data

**[06:31]** don't want to like get incorrect data out of it. So I still feel like you need

**[06:33]** out of it. So I still feel like you need

**[06:33]** out of it. So I still feel like you need to put in the work and it feels like

**[06:34]** to put in the work and it feels like

**[06:34]** to put in the work and it feels like there should still be apps that just do

**[06:37]** there should still be apps that just do

**[06:37]** there should still be apps that just do all the work for you.

**[06:38]** all the work for you.

**[06:38]** all the work for you. >> Did you see Karpathy release like sort

**[06:40]** >> Did you see Karpathy release like sort

**[06:40]** >> Did you see Karpathy release like sort of a LLM arena of a sort which I mean I

**[06:43]** of a LLM arena of a sort which I mean I

**[06:43]** of a LLM arena of a sort which I mean I do by like hand right now using tabs.

**[06:45]** do by like hand right now using tabs.

**[06:45]** do by like hand right now using tabs. It's like you have uh Claude open, you

**[06:47]** It's like you have uh Claude open, you

**[06:47]** It's like you have uh Claude open, you have Gemini open, you have Chetchip open

**[06:50]** have Gemini open, you have Chetchip open

**[06:50]** have Gemini open, you have Chetchip open and you give it the same task and then

**[06:52]** and you give it the same task and then

**[06:52]** and you give it the same task and then you take the output from each and then I

**[06:54]** you take the output from each and then I

**[06:54]** you take the output from each and then I usually go to Claude at that point and

**[06:56]** usually go to Claude at that point and

**[06:56]** usually go to Claude at that point and I'm like all right Claude this is what

**[06:57]** I'm like all right Claude this is what

**[06:57]** I'm like all right Claude this is what the other ones said what do you think

**[06:59]** the other ones said what do you think

**[06:59]** the other ones said what do you think and check each other's work think that


### [07:00 - 08:00]

**[07:01]** and check each other's work think that

**[07:01]** and check each other's work think that that particular behavior at the consumer

**[07:03]** that particular behavior at the consumer

**[07:03]** that particular behavior at the consumer that level that we're doing startups are

**[07:06]** that level that we're doing startups are

**[07:06]** that level that we're doing startups are doing as well they are actually

**[07:07]** doing as well they are actually

**[07:07]** doing as well they are actually arbitrageing a lot of the models I had

**[07:09]** arbitrageing a lot of the models I had

**[07:09]** arbitrageing a lot of the models I had some conversations with a number of

**[07:11]** some conversations with a number of

**[07:11]** some conversations with a number of founders where before they might have

**[07:13]** founders where before they might have

**[07:13]** founders where before they might have been loyalist to let's say open AAI I

**[07:15]** been loyalist to let's say open AAI I

**[07:15]** been loyalist to let's say open AAI I models or entropic and I just had some

**[07:18]** models or entropic and I just had some

**[07:18]** models or entropic and I just had some conversations recently with them and

**[07:20]** conversations recently with them and

**[07:20]** conversations recently with them and these are founders that are running

**[07:22]** these are founders that are running

**[07:22]** these are founders that are running larger companies like series B level

**[07:24]** larger companies like series B level

**[07:24]** larger companies like series B level type of companies with AI they're

**[07:27]** type of companies with AI they're

**[07:27]** type of companies with AI they're actually abstracting all that away and

**[07:30]** actually abstracting all that away and

**[07:30]** actually abstracting all that away and building this orchestration layer where

**[07:32]** building this orchestration layer where

**[07:32]** building this orchestration layer where perhaps as each new model release comes

**[07:35]** perhaps as each new model release comes

**[07:35]** perhaps as each new model release comes out they can swap them in and out or

**[07:38]** out they can swap them in and out or

**[07:38]** out they can swap them in and out or they can use specific models that are

**[07:40]** they can use specific models that are

**[07:40]** they can use specific models that are better at certain things for just that.

**[07:42]** better at certain things for just that.

**[07:42]** better at certain things for just that. For example, I heard from the startup

**[07:44]** For example, I heard from the startup

**[07:44]** For example, I heard from the startup they use Gemini 3 to do the context

**[07:47]** they use Gemini 3 to do the context

**[07:47]** they use Gemini 3 to do the context engineering which they actually then fed

**[07:49]** engineering which they actually then fed

**[07:50]** engineering which they actually then fed into OpenAI to execute it and they keep

**[07:52]** into OpenAI to execute it and they keep

**[07:52]** into OpenAI to execute it and they keep swapping it as new models come up and

**[07:54]** swapping it as new models come up and

**[07:54]** swapping it as new models come up and the winner for each category or type of

**[07:57]** the winner for each category or type of

**[07:57]** the winner for each category or type of agent work is different and ultimately


### [08:00 - 09:00]

**[08:00]** agent work is different and ultimately

**[08:00]** agent work is different and ultimately they can do this because it it is all

**[08:03]** they can do this because it it is all

**[08:03]** they can do this because it it is all grounded based on the evals and the

**[08:04]** grounded based on the evals and the

**[08:04]** grounded based on the evals and the evals are all proprietary to them

**[08:06]** evals are all proprietary to them

**[08:06]** evals are all proprietary to them because they they're a vertical AI agent

**[08:08]** because they they're a vertical AI agent

**[08:08]** because they they're a vertical AI agent and they just work in a very regulated

**[08:10]** and they just work in a very regulated

**[08:10]** and they just work in a very regulated industry and they have this data set

**[08:11]** industry and they have this data set

**[08:12]** industry and they have this data set that just works the best for them. I

**[08:13]** that just works the best for them. I

**[08:13]** that just works the best for them. I think this is the new normal right now

**[08:15]** think this is the new normal right now

**[08:15]** think this is the new normal right now where people are expecting, yeah, the

**[08:17]** where people are expecting, yeah, the

**[08:17]** where people are expecting, yeah, the it's cool that the model companies,

**[08:19]** it's cool that the model companies,

**[08:19]** it's cool that the model companies, they're spending all this money and

**[08:22]** they're spending all this money and

**[08:22]** they're spending all this money and making intelligence faster and better

**[08:23]** making intelligence faster and better

**[08:23]** making intelligence faster and better and we can all benefit. Let's just do

**[08:25]** and we can all benefit. Let's just do

**[08:25]** and we can all benefit. Let's just do the best. It's almost like the era of um

**[08:28]** the best. It's almost like the era of um

**[08:28]** the best. It's almost like the era of um Intel and AMD with new architecture

**[08:30]** Intel and AMD with new architecture

**[08:30]** Intel and AMD with new architecture would come up. People could just swap

**[08:31]** would come up. People could just swap

**[08:31]** would come up. People could just swap them, right?

**[08:32]** them, right?

**[08:32]** them, right? >> Yeah. It feels at the highest level that

**[08:34]** >> Yeah. It feels at the highest level that

**[08:34]** >> Yeah. It feels at the highest level that angst around where's the value going to

**[08:36]** angst around where's the value going to

**[08:36]** angst around where's the value going to acrew? Is it going to go to the model

**[08:37]** acrew? Is it going to go to the model

**[08:37]** acrew? Is it going to go to the model companies or like the application layer?

**[08:39]** companies or like the application layer?

**[08:39]** companies or like the application layer? are either startups feels like that es

**[08:42]** are either startups feels like that es

**[08:42]** are either startups feels like that es and flows in either direction a little

**[08:43]** and flows in either direction a little

**[08:44]** and flows in either direction a little bit throughout the year to me like I

**[08:45]** bit throughout the year to me like I

**[08:45]** bit throughout the year to me like I feel there are moments where like claude

**[08:47]** feel there are moments where like claude

**[08:47]** feel there are moments where like claude code amazing launch and it was like oh

**[08:49]** code amazing launch and it was like oh

**[08:49]** code amazing launch and it was like oh okay like the model companies are

**[08:50]** okay like the model companies are

**[08:50]** okay like the model companies are actually going to play at the

**[08:51]** actually going to play at the

**[08:51]** actually going to play at the application layer but then to me at

**[08:52]** application layer but then to me at

**[08:52]** application layer but then to me at least is all vibes based like Gemini

**[08:54]** least is all vibes based like Gemini

**[08:54]** least is all vibes based like Gemini surge especially over the last few

**[08:56]** surge especially over the last few

**[08:56]** surge especially over the last few months just feels like it returns us to

**[08:57]** months just feels like it returns us to

**[08:57]** months just feels like it returns us to a world of where exactly that like the

**[08:59]** a world of where exactly that like the

**[08:59]** a world of where exactly that like the models are all essentially commoditizing


### [09:00 - 10:00]

**[09:01]** models are all essentially commoditizing

**[09:01]** models are all essentially commoditizing each other and it's just like the

**[09:03]** each other and it's just like the

**[09:03]** each other and it's just like the application layer and the startups are

**[09:05]** application layer and the startups are

**[09:05]** application layer and the startups are going to are set up to have another

**[09:06]** going to are set up to have another

**[09:06]** going to are set up to have another fantastic year if that continues. I'm

**[09:08]** fantastic year if that continues. I'm

**[09:08]** fantastic year if that continues. I'm curious what you think Jared with some

**[09:12]** curious what you think Jared with some

**[09:12]** curious what you think Jared with some a lot of um perhaps the negative

**[09:15]** a lot of um perhaps the negative

**[09:15]** a lot of um perhaps the negative comments on Twitter around is this a bit

**[09:17]** comments on Twitter around is this a bit

**[09:17]** comments on Twitter around is this a bit of a bubble AI bubble.

**[09:20]** of a bubble AI bubble.

**[09:20]** of a bubble AI bubble. >> Yeah. When I talk to undergrads this is

**[09:23]** >> Yeah. When I talk to undergrads this is

**[09:23]** >> Yeah. When I talk to undergrads this is like a common question that I get is

**[09:24]** like a common question that I get is

**[09:24]** like a common question that I get is like oh like I heard it's a big AI

**[09:27]** like oh like I heard it's a big AI

**[09:27]** like oh like I heard it's a big AI bubble because like there's all this

**[09:29]** bubble because like there's all this

**[09:29]** bubble because like there's all this like crazy roundtpping going between

**[09:30]** like crazy roundtpping going between

**[09:30]** like crazy roundtpping going between Nvidia and OpenAI and like is it great

**[09:34]** Nvidia and OpenAI and like is it great

**[09:34]** Nvidia and OpenAI and like is it great is is it all fake? Yeah.

**[09:36]** is is it all fake? Yeah.

**[09:36]** is is it all fake? Yeah. >> No, this is fantastic. Right. Like

**[09:38]** >> No, this is fantastic. Right. Like

**[09:38]** >> No, this is fantastic. Right. Like people look at the telecom bubble, it's

**[09:39]** people look at the telecom bubble, it's

**[09:39]** people look at the telecom bubble, it's like there's just, you know, billions of

**[09:40]** like there's just, you know, billions of

**[09:40]** like there's just, you know, billions of dollars, like tens of billions, hundreds

**[09:42]** dollars, like tens of billions, hundreds

**[09:42]** dollars, like tens of billions, hundreds of billions just like sort of sitting in

**[09:45]** of billions just like sort of sitting in

**[09:45]** of billions just like sort of sitting in a bunch of telecom back in like the, you

**[09:47]** a bunch of telecom back in like the, you

**[09:47]** a bunch of telecom back in like the, you know, '9s. Actually, that's why YouTube

**[09:49]** know, '9s. Actually, that's why YouTube

**[09:49]** know, '9s. Actually, that's why YouTube was able to exist, right? Like if you

**[09:51]** was able to exist, right? Like if you

**[09:52]** was able to exist, right? Like if you just have a whole bunch of extra

**[09:53]** just have a whole bunch of extra

**[09:53]** just have a whole bunch of extra bandwidth that isn't being used and is

**[09:55]** bandwidth that isn't being used and is

**[09:55]** bandwidth that isn't being used and is relatively cheap, the cost is low enough

**[09:56]** relatively cheap, the cost is low enough

**[09:56]** relatively cheap, the cost is low enough for like something like YouTube to

**[09:58]** for like something like YouTube to

**[09:58]** for like something like YouTube to exist. Like if there wasn't a glut of

**[09:59]** exist. Like if there wasn't a glut of

**[09:59]** exist. Like if there wasn't a glut of telecom, then like maybe YouTube would


### [10:00 - 11:00]

**[10:02]** telecom, then like maybe YouTube would

**[10:02]** telecom, then like maybe YouTube would have happened. It just would have

**[10:03]** have happened. It just would have

**[10:03]** have happened. It just would have happened later. And then that isn't that

**[10:05]** happened later. And then that isn't that

**[10:05]** happened later. And then that isn't that like sort of what we're talking about

**[10:06]** like sort of what we're talking about

**[10:06]** like sort of what we're talking about here? Like how do we we have to

**[10:08]** here? Like how do we we have to

**[10:08]** here? Like how do we we have to accelerate, right? We have the age of

**[10:10]** accelerate, right? We have the age of

**[10:10]** accelerate, right? We have the age of intelligence. The rocks can talk, they

**[10:12]** intelligence. The rocks can talk, they

**[10:12]** intelligence. The rocks can talk, they can think, and they can do work. And you

**[10:13]** can think, and they can do work. And you

**[10:13]** can think, and they can do work. And you just have to zap them more. And you get

**[10:16]** just have to zap them more. And you get

**[10:16]** just have to zap them more. And you get like smarter and smarter stuff. At this

**[10:18]** like smarter and smarter stuff. At this

**[10:18]** like smarter and smarter stuff. At this point, I think the argument to college

**[10:20]** point, I think the argument to college

**[10:20]** point, I think the argument to college students is actually like because there

**[10:22]** students is actually like because there

**[10:22]** students is actually like because there will be a glut there is an opportunity

**[10:24]** will be a glut there is an opportunity

**[10:24]** will be a glut there is an opportunity for you. And if there was not a gluten

**[10:27]** for you. And if there was not a gluten

**[10:27]** for you. And if there was not a gluten there wouldn't be as much competition,

**[10:29]** there wouldn't be as much competition,

**[10:29]** there wouldn't be as much competition, the prices would be higher, the margins

**[10:32]** the prices would be higher, the margins

**[10:32]** the prices would be higher, the margins lower in the stack would be higher,

**[10:34]** lower in the stack would be higher,

**[10:34]** lower in the stack would be higher, right? And then, you know, what's one of

**[10:36]** right? And then, you know, what's one of

**[10:36]** right? And then, you know, what's one of the big stories this year? Like, Nvidia

**[10:38]** the big stories this year? Like, Nvidia

**[10:38]** the big stories this year? Like, Nvidia suddenly is on the outs. Like, I think

**[10:40]** suddenly is on the outs. Like, I think

**[10:40]** suddenly is on the outs. Like, I think their stock is today is like around 170s

**[10:43]** their stock is today is like around 170s

**[10:43]** their stock is today is like around 170s or something. You know, I think I'm

**[10:44]** or something. You know, I think I'm

**[10:44]** or something. You know, I think I'm still at long-term buy and hold,

**[10:46]** still at long-term buy and hold,

**[10:46]** still at long-term buy and hold, honestly. But for the moment, people are

**[10:48]** honestly. But for the moment, people are

**[10:48]** honestly. But for the moment, people are like, "Oh, well, Gemini is so good and

**[10:50]** like, "Oh, well, Gemini is so good and

**[10:50]** like, "Oh, well, Gemini is so good and all the, you know, nobody seems to be

**[10:52]** all the, you know, nobody seems to be

**[10:52]** all the, you know, nobody seems to be Nvidia only now, and everyone's buying

**[10:54]** Nvidia only now, and everyone's buying

**[10:54]** Nvidia only now, and everyone's buying AMD and everyone's, you know, and TPUs

**[10:57]** AMD and everyone's, you know, and TPUs

**[10:57]** AMD and everyone's, you know, and TPUs are working." So, you know, at the


### [11:00 - 12:00]

**[11:00]** are working." So, you know, at the

**[11:00]** are working." So, you know, at the moment it looks like there's, you know,

**[11:01]** moment it looks like there's, you know,

**[11:01]** moment it looks like there's, you know, what does that mean? like there's

**[11:03]** what does that mean? like there's

**[11:03]** what does that mean? like there's competition and uh it means that there

**[11:06]** competition and uh it means that there

**[11:06]** competition and uh it means that there will be more compute not less and then

**[11:09]** will be more compute not less and then

**[11:09]** will be more compute not less and then that means that probably a little bit

**[11:11]** that means that probably a little bit

**[11:12]** that means that probably a little bit better things for all of the big LLM

**[11:15]** better things for all of the big LLM

**[11:15]** better things for all of the big LLM companies like sort of the you know the

**[11:16]** companies like sort of the you know the

**[11:16]** companies like sort of the you know the AI labs uh they get a little bit of

**[11:19]** AI labs uh they get a little bit of

**[11:19]** AI labs uh they get a little bit of power but you know they too are in

**[11:21]** power but you know they too are in

**[11:21]** power but you know they too are in competition with one another so then

**[11:23]** competition with one another so then

**[11:23]** competition with one another so then what does that mean well then it's you

**[11:25]** what does that mean well then it's you

**[11:25]** what does that mean well then it's you know go up another level in the stack

**[11:27]** know go up another level in the stack

**[11:27]** know go up another level in the stack right like as long as there are a great

**[11:29]** right like as long as there are a great

**[11:29]** right like as long as there are a great many AI labs that are in uh deep

**[11:31]** many AI labs that are in uh deep

**[11:31]** many AI labs that are in uh deep competition with one another then uh

**[11:34]** competition with one another then uh

**[11:34]** competition with one another then uh that's even better for that college

**[11:36]** that's even better for that college

**[11:36]** that's even better for that college student who's about to start a company

**[11:38]** student who's about to start a company

**[11:38]** student who's about to start a company at the application level.

**[11:40]** at the application level.

**[11:40]** at the application level. >> Yeah, I think that's exactly right. It's

**[11:41]** >> Yeah, I think that's exactly right. It's

**[11:41]** >> Yeah, I think that's exactly right. It's like people are asking this question

**[11:43]** like people are asking this question

**[11:43]** like people are asking this question like is it a bubble? That's maybe a

**[11:45]** like is it a bubble? That's maybe a

**[11:45]** like is it a bubble? That's maybe a question that's really relevant if

**[11:46]** question that's really relevant if

**[11:46]** question that's really relevant if you're like the equivalent of like

**[11:47]** you're like the equivalent of like

**[11:47]** you're like the equivalent of like Comcast. Like if you're Nvidia that's a

**[11:49]** Comcast. Like if you're Nvidia that's a

**[11:50]** Comcast. Like if you're Nvidia that's a very relevant question like oh are

**[11:51]** very relevant question like oh are

**[11:51]** very relevant question like oh are people overbuilding GPU capacity? But

**[11:53]** people overbuilding GPU capacity? But

**[11:53]** people overbuilding GPU capacity? But like the college students they're not

**[11:55]** like the college students they're not

**[11:55]** like the college students they're not Comcast they're actually like YouTube.

**[11:56]** Comcast they're actually like YouTube.

**[11:56]** Comcast they're actually like YouTube. If you're doing a startup in in your

**[11:58]** If you're doing a startup in in your

**[11:58]** If you're doing a startup in in your dorm room, it's like the AI equivalent

**[11:59]** dorm room, it's like the AI equivalent

**[11:59]** dorm room, it's like the AI equivalent of like YouTube and like kind of doesn't


### [12:00 - 13:00]

**[12:01]** of like YouTube and like kind of doesn't

**[12:01]** of like YouTube and like kind of doesn't really matter that much. Maybe Nvidia's

**[12:02]** really matter that much. Maybe Nvidia's

**[12:02]** really matter that much. Maybe Nvidia's stock will go down next year. I don't

**[12:04]** stock will go down next year. I don't

**[12:04]** stock will go down next year. I don't know. But like even if it does, that

**[12:07]** know. But like even if it does, that

**[12:07]** know. But like even if it does, that doesn't actually mean that it's like a

**[12:08]** doesn't actually mean that it's like a

**[12:08]** doesn't actually mean that it's like a bad time to be working on an AI startup.

**[12:10]** bad time to be working on an AI startup.

**[12:10]** bad time to be working on an AI startup. >> Yeah. To what Zach said on a podcast

**[12:11]** >> Yeah. To what Zach said on a podcast

**[12:11]** >> Yeah. To what Zach said on a podcast survey this year, I think, right? It's

**[12:12]** survey this year, I think, right? It's

**[12:12]** survey this year, I think, right? It's like Meta may end up overinvesting like

**[12:15]** like Meta may end up overinvesting like

**[12:15]** like Meta may end up overinvesting like a significant amount in like the capex

**[12:18]** a significant amount in like the capex

**[12:18]** a significant amount in like the capex and infrastructure, but like they

**[12:20]** and infrastructure, but like they

**[12:20]** and infrastructure, but like they essentially have to the big companies

**[12:21]** essentially have to the big companies

**[12:21]** essentially have to the big companies have to do it because they can't just

**[12:22]** have to do it because they can't just

**[12:22]** have to do it because they can't just like sit on the sidelines. And in the

**[12:25]** like sit on the sidelines. And in the

**[12:25]** like sit on the sidelines. And in the case like demand falls off a cliff for

**[12:27]** case like demand falls off a cliff for

**[12:27]** case like demand falls off a cliff for some reason, it's their capex, not the

**[12:30]** some reason, it's their capex, not the

**[12:30]** some reason, it's their capex, not the startup's capex. And there's still going

**[12:31]** startup's capex. And there's still going

**[12:31]** startup's capex. And there's still going to be tons of infrastructure and ideas

**[12:33]** to be tons of infrastructure and ideas

**[12:33]** to be tons of infrastructure and ideas to still continue building.

**[12:35]** to still continue building.

**[12:35]** to still continue building. >> There was this book written by this

**[12:36]** >> There was this book written by this

**[12:36]** >> There was this book written by this economist called Carlo Perez who studied

**[12:40]** economist called Carlo Perez who studied

**[12:40]** economist called Carlo Perez who studied a lot of uh tech trends and it studies a

**[12:43]** a lot of uh tech trends and it studies a

**[12:43]** a lot of uh tech trends and it studies a lot of um technology revolutions and it

**[12:45]** lot of um technology revolutions and it

**[12:45]** lot of um technology revolutions and it summarizes that there's really two

**[12:47]** summarizes that there's really two

**[12:47]** summarizes that there's really two phases. There's the phase of uh

**[12:50]** phases. There's the phase of uh

**[12:50]** phases. There's the phase of uh installation which is where a lot of the

**[12:52]** installation which is where a lot of the

**[12:52]** installation which is where a lot of the very heavy capex investment come in and

**[12:56]** very heavy capex investment come in and

**[12:56]** very heavy capex investment come in and then there's the deployment phase where

**[12:58]** then there's the deployment phase where

**[12:58]** then there's the deployment phase where really ripples it where it rips and then


### [13:00 - 14:00]

**[13:01]** really ripples it where it rips and then

**[13:01]** really ripples it where it rips and then everything explodes in terms of

**[13:02]** everything explodes in terms of

**[13:02]** everything explodes in terms of abundance and during the initial phase

**[13:05]** abundance and during the initial phase

**[13:05]** abundance and during the initial phase of installation is where it feels like a

**[13:07]** of installation is where it feels like a

**[13:07]** of installation is where it feels like a bubble. There's a bit of a frenzy

**[13:09]** bubble. There's a bit of a frenzy

**[13:09]** bubble. There's a bit of a frenzy because it starts first with a there's

**[13:10]** because it starts first with a there's

**[13:10]** because it starts first with a there's this new technology that's amazing which

**[13:12]** this new technology that's amazing which

**[13:12]** this new technology that's amazing which happened with the chatb moment in 2023.

**[13:15]** happened with the chatb moment in 2023.

**[13:15]** happened with the chatb moment in 2023. Everyone got super excited about the

**[13:17]** Everyone got super excited about the

**[13:17]** Everyone got super excited about the tech and then everyone got super hyped

**[13:19]** tech and then everyone got super hyped

**[13:19]** tech and then everyone got super hyped and got into investing into a lot of the

**[13:22]** and got into investing into a lot of the

**[13:22]** and got into investing into a lot of the infrastructure with buying a lot of GPUs

**[13:24]** infrastructure with buying a lot of GPUs

**[13:24]** infrastructure with buying a lot of GPUs and all the giant gigawatt data center

**[13:26]** and all the giant gigawatt data center

**[13:26]** and all the giant gigawatt data center built out and then people's like but

**[13:28]** built out and then people's like but

**[13:28]** built out and then people's like but what is the demand? What are going to be

**[13:29]** what is the demand? What are going to be

**[13:29]** what is the demand? What are going to be all the applications to be built out? I

**[13:31]** all the applications to be built out? I

**[13:31]** all the applications to be built out? I think right now we're in that transition

**[13:33]** think right now we're in that transition

**[13:33]** think right now we're in that transition which is actually really good news for

**[13:35]** which is actually really good news for

**[13:35]** which is actually really good news for startup founders because they are not

**[13:37]** startup founders because they are not

**[13:37]** startup founders because they are not involved into the building the data

**[13:39]** involved into the building the data

**[13:39]** involved into the building the data centers but they're going to build the

**[13:42]** centers but they're going to build the

**[13:42]** centers but they're going to build the next generation of applications in the

**[13:44]** next generation of applications in the

**[13:44]** next generation of applications in the deployment phase when it really

**[13:46]** deployment phase when it really

**[13:46]** deployment phase when it really proliferates and what happened just

**[13:49]** proliferates and what happened just

**[13:49]** proliferates and what happened just going back to the analogy with with the

**[13:50]** going back to the analogy with with the

**[13:50]** going back to the analogy with with the era of uh the internet before the 2000

**[13:54]** era of uh the internet before the 2000

**[13:54]** era of uh the internet before the 2000 there was a lot of heavy capex

**[13:55]** there was a lot of heavy capex

**[13:55]** there was a lot of heavy capex investment into the telos right those

**[13:59]** investment into the telos right those

**[13:59]** investment into the telos right those were giant projects that college


### [14:00 - 15:00]

**[14:01]** were giant projects that college

**[14:01]** were giant projects that college students wouldn't be involved, but they

**[14:03]** students wouldn't be involved, but they

**[14:03]** students wouldn't be involved, but they were very heavily invested and in some

**[14:05]** were very heavily invested and in some

**[14:05]** were very heavily invested and in some cases were overinvested. I mean, this is

**[14:07]** cases were overinvested. I mean, this is

**[14:08]** cases were overinvested. I mean, this is the whole thing with dark fiber and some

**[14:10]** the whole thing with dark fiber and some

**[14:10]** the whole thing with dark fiber and some pipes that are not used. And that's

**[14:11]** pipes that are not used. And that's

**[14:11]** pipes that are not used. And that's fine. The internet ended up being still

**[14:14]** fine. The internet ended up being still

**[14:14]** fine. The internet ended up being still a giant economic driver. And what that

**[14:17]** a giant economic driver. And what that

**[14:17]** a giant economic driver. And what that means is startups like the future

**[14:19]** means is startups like the future

**[14:19]** means is startups like the future Facebook or the future Google are yet to

**[14:21]** Facebook or the future Google are yet to

**[14:21]** Facebook or the future Google are yet to be started because those come in in the

**[14:23]** be started because those come in in the

**[14:23]** be started because those come in in the deployment phase because right now I

**[14:24]** deployment phase because right now I

**[14:24]** deployment phase because right now I think this things things are still

**[14:26]** think this things things are still

**[14:26]** think this things things are still getting built up. I I do think the

**[14:28]** getting built up. I I do think the

**[14:28]** getting built up. I I do think the foundation lab companies and GPUs very

**[14:30]** foundation lab companies and GPUs very

**[14:30]** foundation lab companies and GPUs very much are falling into the bucket of

**[14:32]** much are falling into the bucket of

**[14:32]** much are falling into the bucket of infrastructure.

**[14:33]** infrastructure.

**[14:33]** infrastructure. >> Yeah. I mean it's interesting to watch

**[14:35]** >> Yeah. I mean it's interesting to watch

**[14:35]** >> Yeah. I mean it's interesting to watch uh how the stuff is evolving a little

**[14:36]** uh how the stuff is evolving a little

**[14:36]** uh how the stuff is evolving a little bit. So do you remember summer 24 there

**[14:39]** bit. So do you remember summer 24 there

**[14:39]** bit. So do you remember summer 24 there was a company called StarCloud that came

**[14:41]** was a company called StarCloud that came

**[14:41]** was a company called StarCloud that came out and was one of the first to come out

**[14:43]** out and was one of the first to come out

**[14:43]** out and was one of the first to come out and say we're going to make data centers

**[14:45]** and say we're going to make data centers

**[14:45]** and say we're going to make data centers in space and what was the reaction when

**[14:48]** in space and what was the reaction when

**[14:48]** in space and what was the reaction when you know

**[14:49]** you know

**[14:49]** you know >> people laughed at them

**[14:50]** >> people laughed at them

**[14:50]** >> people laughed at them >> on the internet. Yes. They said that's

**[14:51]** >> on the internet. Yes. They said that's

**[14:51]** >> on the internet. Yes. They said that's the stupidest idea ever. you know, I

**[14:53]** the stupidest idea ever. you know, I

**[14:53]** the stupidest idea ever. you know, I guess 18 months later, uh, suddenly

**[14:55]** guess 18 months later, uh, suddenly

**[14:56]** guess 18 months later, uh, suddenly Google's doing it, Elon's doing it.

**[14:58]** Google's doing it, Elon's doing it.

**[14:58]** Google's doing it, Elon's doing it. >> In every interview now, apparently, it


### [15:00 - 16:00]

**[15:01]** >> In every interview now, apparently, it

**[15:01]** >> In every interview now, apparently, it seems to be like his top talking point.

**[15:02]** seems to be like his top talking point.

**[15:02]** seems to be like his top talking point. >> Yeah. And so, I mean, why is that? Like,

**[15:04]** >> Yeah. And so, I mean, why is that? Like,

**[15:04]** >> Yeah. And so, I mean, why is that? Like, I feel like one of the aspects is that

**[15:06]** I feel like one of the aspects is that

**[15:06]** I feel like one of the aspects is that like part of the um infrastructure

**[15:09]** like part of the um infrastructure

**[15:09]** like part of the um infrastructure buildout right now that's so intense is

**[15:11]** buildout right now that's so intense is

**[15:11]** buildout right now that's so intense is like we literally don't have power

**[15:14]** like we literally don't have power

**[15:14]** like we literally don't have power generation. Boom. Supersonic instead of

**[15:16]** generation. Boom. Supersonic instead of

**[15:16]** generation. Boom. Supersonic instead of making supersonic jets right now is on

**[15:19]** making supersonic jets right now is on

**[15:19]** making supersonic jets right now is on this good quest to create enough power

**[15:21]** this good quest to create enough power

**[15:21]** this good quest to create enough power for a bunch of these AI data centers

**[15:24]** for a bunch of these AI data centers

**[15:24]** for a bunch of these AI data centers that are being built right now. They use

**[15:26]** that are being built right now. They use

**[15:26]** that are being built right now. They use jet engines and even those like are so

**[15:29]** jet engines and even those like are so

**[15:29]** jet engines and even those like are so bad, you know, the supply chain for jet

**[15:31]** bad, you know, the supply chain for jet

**[15:31]** bad, you know, the supply chain for jet engines to generate power for data

**[15:33]** engines to generate power for data

**[15:33]** engines to generate power for data centers is so backed up that you know

**[15:35]** centers is so backed up that you know

**[15:35]** centers is so backed up that you know you would have had to have ordered these

**[15:36]** you would have had to have ordered these

**[15:36]** you would have had to have ordered these things you know two or three years ago

**[15:39]** things you know two or three years ago

**[15:39]** things you know two or three years ago just to even have it in two or three

**[15:41]** just to even have it in two or three

**[15:41]** just to even have it in two or three years from now. you know, these

**[15:43]** years from now. you know, these

**[15:43]** years from now. you know, these constraints uh end up like influencing

**[15:47]** constraints uh end up like influencing

**[15:47]** constraints uh end up like influencing like fairly directly what the giant tech

**[15:50]** like fairly directly what the giant tech

**[15:50]** like fairly directly what the giant tech companies need to do to win the game

**[15:52]** companies need to do to win the game

**[15:52]** companies need to do to win the game three or five years out. Like suddenly

**[15:56]** three or five years out. Like suddenly

**[15:56]** three or five years out. Like suddenly there's not enough land. You know, in

**[15:58]** there's not enough land. You know, in

**[15:58]** there's not enough land. You know, in America, we can't build. The regulations


### [16:00 - 17:00]

**[16:00]** America, we can't build. The regulations

**[16:00]** America, we can't build. The regulations are too high. In California, we have

**[16:01]** are too high. In California, we have

**[16:02]** are too high. In California, we have SQA, which is totally abused by the

**[16:04]** SQA, which is totally abused by the

**[16:04]** SQA, which is totally abused by the environmental lobby to stop all

**[16:06]** environmental lobby to stop all

**[16:06]** environmental lobby to stop all innovation and building housing. By the

**[16:09]** innovation and building housing. By the

**[16:09]** innovation and building housing. By the way, we just don't have enough

**[16:11]** way, we just don't have enough

**[16:11]** way, we just don't have enough terrestrially like to just do the things

**[16:14]** terrestrially like to just do the things

**[16:14]** terrestrially like to just do the things that society sort of needs right now.

**[16:16]** that society sort of needs right now.

**[16:16]** that society sort of needs right now. So, you know, the escape valve is like

**[16:18]** So, you know, the escape valve is like

**[16:18]** So, you know, the escape valve is like actually let's just do it in space.

**[16:20]** actually let's just do it in space.

**[16:20]** actually let's just do it in space. >> Yeah. Come to think about we we kind of

**[16:22]** >> Yeah. Come to think about we we kind of

**[16:22]** >> Yeah. Come to think about we we kind of have the trifecta of YC companies that

**[16:23]** have the trifecta of YC companies that

**[16:24]** have the trifecta of YC companies that are solving the data center buildout

**[16:25]** are solving the data center buildout

**[16:25]** are solving the data center buildout problem.

**[16:25]** problem.

**[16:25]** problem. >> Well, you need fusion energy.

**[16:27]** >> Well, you need fusion energy.

**[16:27]** >> Well, you need fusion energy. >> Yeah. Yeah. Well, we have the company

**[16:29]** >> Yeah. Yeah. Well, we have the company

**[16:29]** >> Yeah. Yeah. Well, we have the company that's solving the no land problem by

**[16:32]** that's solving the no land problem by

**[16:32]** that's solving the no land problem by building the data centers in space. We

**[16:34]** building the data centers in space. We

**[16:34]** building the data centers in space. We have Boom and Helion which are solving

**[16:36]** have Boom and Helion which are solving

**[16:36]** have Boom and Helion which are solving that we don't have enough energy

**[16:37]** that we don't have enough energy

**[16:37]** that we don't have enough energy problem. Just fun today, uh, a space

**[16:40]** problem. Just fun today, uh, a space

**[16:40]** problem. Just fun today, uh, a space fusion company that just graduated

**[16:42]** fusion company that just graduated

**[16:42]** fusion company that just graduated called Zephr Fusion. And they actually

**[16:44]** called Zephr Fusion. And they actually

**[16:44]** called Zephr Fusion. And they actually had a great seed round out of Demo Day.

**[16:46]** had a great seed round out of Demo Day.

**[16:46]** had a great seed round out of Demo Day. They're in their 40s. They're national

**[16:48]** They're in their 40s. They're national

**[16:48]** They're in their 40s. They're national lab engineers who their entire careers

**[16:51]** lab engineers who their entire careers

**[16:51]** lab engineers who their entire careers they were building, you know, Tokamax

**[16:53]** they were building, you know, Tokamax

**[16:53]** they were building, you know, Tokamax and Fusion Energy. And they came into

**[16:56]** and Fusion Energy. And they came into

**[16:56]** and Fusion Energy. And they came into the lab one day. They looked at the

**[16:58]** the lab one day. They looked at the

**[16:58]** the lab one day. They looked at the physics. They, you know, looked at the


### [17:00 - 18:00]

**[17:00]** physics. They, you know, looked at the

**[17:00]** physics. They, you know, looked at the math and the models and they said, "You

**[17:03]** math and the models and they said, "You

**[17:03]** math and the models and they said, "You know what? If we did this in space, it

**[17:05]** know what? If we did this in space, it

**[17:06]** know what? If we did this in space, it would actually pencil." And so that's

**[17:07]** would actually pencil." And so that's

**[17:07]** would actually pencil." And so that's they're on like this sort of grand next

**[17:10]** they're on like this sort of grand next

**[17:10]** they're on like this sort of grand next 5 10 year quest to actually manifest it

**[17:13]** 5 10 year quest to actually manifest it

**[17:13]** 5 10 year quest to actually manifest it to actually create it in space uh

**[17:15]** to actually create it in space uh

**[17:15]** to actually create it in space uh because the equations say that it is

**[17:17]** because the equations say that it is

**[17:17]** because the equations say that it is possible and uh if they do it it's

**[17:20]** possible and uh if they do it it's

**[17:20]** possible and uh if they do it it's actually the only path to gigawatts of

**[17:21]** actually the only path to gigawatts of

**[17:22]** actually the only path to gigawatts of energy uh up there in space. So you know

**[17:25]** energy uh up there in space. So you know

**[17:25]** energy uh up there in space. So you know it might be you know an even more

**[17:26]** it might be you know an even more

**[17:26]** it might be you know an even more perfect trifecta uh shortly. Something

**[17:29]** perfect trifecta uh shortly. Something

**[17:29]** perfect trifecta uh shortly. Something else I feel like happened over the

**[17:30]** else I feel like happened over the

**[17:30]** else I feel like happened over the course of this year is the um interest

**[17:33]** course of this year is the um interest

**[17:33]** course of this year is the um interest in starting model companies like I guess

**[17:35]** in starting model companies like I guess

**[17:35]** in starting model companies like I guess maybe at both ends. There's like the

**[17:37]** maybe at both ends. There's like the

**[17:37]** maybe at both ends. There's like the people who can raise the capital to go

**[17:39]** people who can raise the capital to go

**[17:39]** people who can raise the capital to go and actually try and build a head-on

**[17:41]** and actually try and build a head-on

**[17:41]** and actually try and build a head-on competitor to open AI which there are

**[17:42]** competitor to open AI which there are

**[17:42]** competitor to open AI which there are very very few like maybe have Ilio with

**[17:44]** very very few like maybe have Ilio with

**[17:44]** very very few like maybe have Ilio with SSI but then more so within YC people

**[17:48]** SSI but then more so within YC people

**[17:48]** SSI but then more so within YC people trying to build like smaller models. Um,

**[17:50]** trying to build like smaller models. Um,

**[17:50]** trying to build like smaller models. Um, I've certainly had more of those in the

**[17:51]** I've certainly had more of those in the

**[17:51]** I've certainly had more of those in the last few batches than before, like

**[17:53]** last few batches than before, like

**[17:53]** last few batches than before, like whether it's sort of like a models to

**[17:54]** whether it's sort of like a models to

**[17:54]** whether it's sort of like a models to run on edge devices or maybe like a

**[17:56]** run on edge devices or maybe like a

**[17:56]** run on edge devices or maybe like a voice model specific to a particular

**[17:57]** voice model specific to a particular

**[17:58]** voice model specific to a particular language. And I'm curious to see if that


### [18:00 - 19:00]

**[18:00]** language. And I'm curious to see if that

**[18:00]** language. And I'm curious to see if that trend continues going back to this early

**[18:02]** trend continues going back to this early

**[18:02]** trend continues going back to this early era of YC. Actually, we sort of saw the

**[18:04]** era of YC. Actually, we sort of saw the

**[18:04]** era of YC. Actually, we sort of saw the explosion of just startups being created

**[18:06]** explosion of just startups being created

**[18:06]** explosion of just startups being created and maybe especially SAS startups.

**[18:08]** and maybe especially SAS startups.

**[18:08]** and maybe especially SAS startups. Partly what what um fed that was just

**[18:12]** Partly what what um fed that was just

**[18:12]** Partly what what um fed that was just knowledge about startups became more

**[18:13]** knowledge about startups became more

**[18:13]** knowledge about startups became more dispersed. there wasn't like a cannon of

**[18:15]** dispersed. there wasn't like a cannon of

**[18:15]** dispersed. there wasn't like a cannon of library information on the internet

**[18:17]** library information on the internet

**[18:17]** library information on the internet about like how to start a startup, how

**[18:18]** about like how to start a startup, how

**[18:18]** about like how to start a startup, how to build software and then over like a

**[18:20]** to build software and then over like a

**[18:20]** to build software and then over like a decade that just became more common

**[18:22]** decade that just became more common

**[18:22]** decade that just became more common place and that just exploded like

**[18:24]** place and that just exploded like

**[18:24]** place and that just exploded like society's knowledge of startups and how

**[18:26]** society's knowledge of startups and how

**[18:26]** society's knowledge of startups and how to build things and it's may feels like

**[18:29]** to build things and it's may feels like

**[18:29]** to build things and it's may feels like maybe we're going through that moment in

**[18:30]** maybe we're going through that moment in

**[18:30]** maybe we're going through that moment in sort of the AI research meets like

**[18:33]** sort of the AI research meets like

**[18:34]** sort of the AI research meets like actually building things

**[18:35]** actually building things

**[18:35]** actually building things >> with with training models. I think we

**[18:36]** >> with with training models. I think we

**[18:36]** >> with with training models. I think we are absolutely going through that right

**[18:37]** are absolutely going through that right

**[18:37]** are absolutely going through that right now. Yes. where where it's going from

**[18:39]** now. Yes. where where it's going from

**[18:39]** now. Yes. where where it's going from being a very rare skill set to a more

**[18:42]** being a very rare skill set to a more

**[18:42]** being a very rare skill set to a more common one

**[18:42]** common one

**[18:42]** common one >> cuz like open AI a decade ago was like a

**[18:45]** >> cuz like open AI a decade ago was like a

**[18:45]** >> cuz like open AI a decade ago was like a rare like you needed you need like a a

**[18:47]** rare like you needed you need like a a

**[18:47]** rare like you needed you need like a a unique combination of skills right you

**[18:49]** unique combination of skills right you

**[18:49]** unique combination of skills right you need like your researcher brain your

**[18:51]** need like your researcher brain your

**[18:51]** need like your researcher brain your sort of like engineering brain maybe

**[18:53]** sort of like engineering brain maybe

**[18:53]** sort of like engineering brain maybe like your sort of finance business brain

**[18:55]** like your sort of finance business brain

**[18:56]** like your sort of finance business brain >> wait so did you just describe Ilia Greg


### [19:00 - 20:00]

**[19:00]** >> wait so did you just describe Ilia Greg

**[19:00]** >> wait so did you just describe Ilia Greg and Sam

**[19:00]** and Sam

**[19:00]** and Sam >> you got it

**[19:03]** >> you got it

**[19:03]** >> you got it >> there was like a rare team right there

**[19:05]** >> there was like a rare team right there

**[19:05]** >> there was like a rare team right there just wasn't that configuration of team

**[19:06]** just wasn't that configuration of team

**[19:06]** just wasn't that configuration of team around very much and Now a decade later,

**[19:10]** around very much and Now a decade later,

**[19:10]** around very much and Now a decade later, there's like a plethora of people who

**[19:12]** there's like a plethora of people who

**[19:12]** there's like a plethora of people who have like the research background, the

**[19:14]** have like the research background, the

**[19:14]** have like the research background, the engineering background, um the startup

**[19:17]** engineering background, um the startup

**[19:17]** engineering background, um the startup capital raising um background or at

**[19:19]** capital raising um background or at

**[19:19]** capital raising um background or at least going to be taught how to do all

**[19:20]** least going to be taught how to do all

**[19:20]** least going to be taught how to do all of that kind of stuff. And I'm curious

**[19:22]** of that kind of stuff. And I'm curious

**[19:22]** of that kind of stuff. And I'm curious if that would just mean we'll just see

**[19:23]** if that would just mean we'll just see

**[19:23]** if that would just mean we'll just see more

**[19:25]** more

**[19:25]** more applied AI company starting and maybe

**[19:27]** applied AI company starting and maybe

**[19:27]** applied AI company starting and maybe there'll be like even more models to

**[19:29]** there'll be like even more models to

**[19:30]** there'll be like even more models to choose from for all the various specific

**[19:31]** choose from for all the various specific

**[19:31]** choose from for all the various specific tasks.

**[19:31]** tasks.

**[19:32]** tasks. >> I think so. So I think the other thing

**[19:33]** >> I think so. So I think the other thing

**[19:33]** >> I think so. So I think the other thing that's even contributing and making this

**[19:35]** that's even contributing and making this

**[19:35]** that's even contributing and making this a ever even bigger snowball is because

**[19:38]** a ever even bigger snowball is because

**[19:38]** a ever even bigger snowball is because of RL. I think there's all these new

**[19:41]** of RL. I think there's all these new

**[19:41]** of RL. I think there's all these new open source models that people are doing

**[19:44]** open source models that people are doing

**[19:44]** open source models that people are doing the fine tune on top of it with a

**[19:47]** the fine tune on top of it with a

**[19:47]** the fine tune on top of it with a particular RL environment and task. So

**[19:50]** particular RL environment and task. So

**[19:50]** particular RL environment and task. So it is very possible that you can create

**[19:53]** it is very possible that you can create

**[19:53]** it is very possible that you can create the best domain specific let's say

**[19:56]** the best domain specific let's say

**[19:56]** the best domain specific let's say healthcare

**[19:58]** healthcare

**[19:58]** healthcare model train on a generic open source


### [20:00 - 21:00]

**[20:00]** model train on a generic open source

**[20:00]** model train on a generic open source model by just doing fine-tuning on it

**[20:03]** model by just doing fine-tuning on it

**[20:03]** model by just doing fine-tuning on it and doing arl it beats the regular big

**[20:06]** and doing arl it beats the regular big

**[20:06]** and doing arl it beats the regular big model actually I've heard and seen a

**[20:08]** model actually I've heard and seen a

**[20:08]** model actually I've heard and seen a number of startups where their domain

**[20:10]** number of startups where their domain

**[20:10]** number of startups where their domain specific model beats u openai let's say

**[20:13]** specific model beats u openai let's say

**[20:13]** specific model beats u openai let's say on healthcare there's this particular yc

**[20:15]** on healthcare there's this particular yc

**[20:15]** on healthcare there's this particular yc startup that told me that they collected

**[20:16]** startup that told me that they collected

**[20:16]** startup that told me that they collected the best data set for for healthcare

**[20:19]** the best data set for for healthcare

**[20:19]** the best data set for for healthcare care and they ended up performing better

**[20:21]** care and they ended up performing better

**[20:21]** care and they ended up performing better than OpenAI and a lot of the benchmarks

**[20:23]** than OpenAI and a lot of the benchmarks

**[20:23]** than OpenAI and a lot of the benchmarks for for healthcare with only 8 billion

**[20:25]** for for healthcare with only 8 billion

**[20:25]** for for healthcare with only 8 billion parameters.

**[20:26]** parameters.

**[20:26]** parameters. >> I guess what's funny is uh you do need

**[20:28]** >> I guess what's funny is uh you do need

**[20:28]** >> I guess what's funny is uh you do need to have a post-raining infrastructure.

**[20:30]** to have a post-raining infrastructure.

**[20:30]** to have a post-raining infrastructure. You we've also had YC companies where uh

**[20:32]** You we've also had YC companies where uh

**[20:32]** You we've also had YC companies where uh they had something that beat OpenAI uh

**[20:35]** they had something that beat OpenAI uh

**[20:35]** they had something that beat OpenAI uh you know GPT 3.5 and they were doing

**[20:37]** you know GPT 3.5 and they were doing

**[20:37]** you know GPT 3.5 and they were doing fine-tuning with RL but then uh yeah G

**[20:41]** fine-tuning with RL but then uh yeah G

**[20:41]** fine-tuning with RL but then uh yeah G GPT 4.5 and then 5.1 came out and uh you

**[20:45]** GPT 4.5 and then 5.1 came out and uh you

**[20:46]** GPT 4.5 and then 5.1 came out and uh you know basically blew their finetuning out

**[20:47]** know basically blew their finetuning out

**[20:48]** know basically blew their finetuning out of the water. have to keep going. Yeah.

**[20:49]** of the water. have to keep going. Yeah.

**[20:49]** of the water. have to keep going. Yeah. >> Yeah. You got to keep going. Yeah. I

**[20:51]** >> Yeah. You got to keep going. Yeah. I

**[20:51]** >> Yeah. You got to keep going. Yeah. I mean, you actually have to continue to

**[20:53]** mean, you actually have to continue to

**[20:54]** mean, you actually have to continue to uh get to the edge. Anything else uh

**[20:56]** uh get to the edge. Anything else uh

**[20:56]** uh get to the edge. Anything else uh that really sort of stood out from this

**[20:59]** that really sort of stood out from this

**[20:59]** that really sort of stood out from this past year that jumps out to you?


### [21:00 - 22:00]

**[21:00]** past year that jumps out to you?

**[21:00]** past year that jumps out to you? >> Uh it's funny. We started the year with

**[21:02]** >> Uh it's funny. We started the year with

**[21:02]** >> Uh it's funny. We started the year with one of our episodes that got a lot of

**[21:04]** one of our episodes that got a lot of

**[21:04]** one of our episodes that got a lot of views around vibe coding. I think we

**[21:07]** views around vibe coding. I think we

**[21:07]** views around vibe coding. I think we were talking about it more as observing

**[21:09]** were talking about it more as observing

**[21:09]** were talking about it more as observing a behavior that was happening from our

**[21:10]** a behavior that was happening from our

**[21:10]** a behavior that was happening from our founders. And I was surprised to see

**[21:14]** founders. And I was surprised to see

**[21:14]** founders. And I was surprised to see that this became like a giant category.

**[21:16]** that this became like a giant category.

**[21:16]** that this became like a giant category. There's lots of companies that are

**[21:18]** There's lots of companies that are

**[21:18]** There's lots of companies that are winning. I mean, we have Replet, there's

**[21:20]** winning. I mean, we have Replet, there's

**[21:20]** winning. I mean, we have Replet, there's Emergence, there's a bunch of them.

**[21:23]** Emergence, there's a bunch of them.

**[21:23]** Emergence, there's a bunch of them. >> Verun moan had gone over to Google. He

**[21:25]** >> Verun moan had gone over to Google. He

**[21:25]** >> Verun moan had gone over to Google. He released anti-gravity. And uh did you

**[21:28]** released anti-gravity. And uh did you

**[21:28]** released anti-gravity. And uh did you guys see the video? I actually am sort

**[21:29]** guys see the video? I actually am sort

**[21:29]** guys see the video? I actually am sort of curious whether they actually used

**[21:31]** of curious whether they actually used

**[21:31]** of curious whether they actually used Nanobanana or any of these videogen

**[21:34]** Nanobanana or any of these videogen

**[21:34]** Nanobanana or any of these videogen things cuz it's like a little too

**[21:36]** things cuz it's like a little too

**[21:36]** things cuz it's like a little too perfect. But Google has the budget to do

**[21:38]** perfect. But Google has the budget to do

**[21:38]** perfect. But Google has the budget to do the high production value video. But

**[21:40]** the high production value video. But

**[21:40]** the high production value video. But it's, you know, Verun at the keyboard

**[21:42]** it's, you know, Verun at the keyboard

**[21:42]** it's, you know, Verun at the keyboard and then, you know, Sergey is like right

**[21:45]** and then, you know, Sergey is like right

**[21:45]** and then, you know, Sergey is like right behind him. So it was like it was very

**[21:46]** behind him. So it was like it was very

**[21:46]** behind him. So it was like it was very cinematic. Anyway, I think Sundar was uh

**[21:49]** cinematic. Anyway, I think Sundar was uh

**[21:49]** cinematic. Anyway, I think Sundar was uh you know also not only talking about um

**[21:52]** you know also not only talking about um

**[21:52]** you know also not only talking about um space data centers. Uh he was also

**[21:55]** space data centers. Uh he was also

**[21:55]** space data centers. Uh he was also talking about vibe coding and I knew

**[21:57]** talking about vibe coding and I knew

**[21:57]** talking about vibe coding and I knew that I was a little bit trolling back

**[21:59]** that I was a little bit trolling back

**[21:59]** that I was a little bit trolling back but knowing what we know. I mean yes


### [22:00 - 23:00]

**[22:02]** but knowing what we know. I mean yes

**[22:02]** but knowing what we know. I mean yes vibe coding is not you know completely

**[22:06]** vibe coding is not you know completely

**[22:06]** vibe coding is not you know completely usable and trustable for uh you know

**[22:09]** usable and trustable for uh you know

**[22:09]** usable and trustable for uh you know 100% of your coding period like this.

**[22:12]** 100% of your coding period like this.

**[22:12]** 100% of your coding period like this. You know, it is not true that you can

**[22:15]** You know, it is not true that you can

**[22:15]** You know, it is not true that you can like shipund 100 100% solid production

**[22:19]** like shipund 100 100% solid production

**[22:19]** like shipund 100 100% solid production code today as of 2020 and the end of

**[22:22]** code today as of 2020 and the end of

**[22:22]** code today as of 2020 and the end of 2025. Yeah, I was thinking about things

**[22:24]** 2025. Yeah, I was thinking about things

**[22:24]** 2025. Yeah, I was thinking about things that surprised me in 2025. And I think

**[22:26]** that surprised me in 2025. And I think

**[22:26]** that surprised me in 2025. And I think perhaps the thing that most surprised me

**[22:28]** perhaps the thing that most surprised me

**[22:28]** perhaps the thing that most surprised me is the extent to which I feel like the

**[22:30]** is the extent to which I feel like the

**[22:30]** is the extent to which I feel like the AI economy stabilized. Like I feel like

**[22:32]** AI economy stabilized. Like I feel like

**[22:32]** AI economy stabilized. Like I feel like when we did this episode at the end of

**[22:34]** when we did this episode at the end of

**[22:34]** when we did this episode at the end of 2024, it felt like we were still in the

**[22:36]** 2024, it felt like we were still in the

**[22:36]** 2024, it felt like we were still in the middle of a period of incredibly rapid

**[22:37]** middle of a period of incredibly rapid

**[22:37]** middle of a period of incredibly rapid change where the ground was shifting

**[22:38]** change where the ground was shifting

**[22:38]** change where the ground was shifting under our feet and like nobody knew when

**[22:41]** under our feet and like nobody knew when

**[22:41]** under our feet and like nobody knew when the other shoe might drop and like what

**[22:43]** the other shoe might drop and like what

**[22:43]** the other shoe might drop and like what exactly was going to happen with

**[22:44]** exactly was going to happen with

**[22:44]** exactly was going to happen with startups and AI and the economy. Now I

**[22:45]** startups and AI and the economy. Now I

**[22:46]** startups and AI and the economy. Now I feel like we've kind of settled into

**[22:47]** feel like we've kind of settled into

**[22:47]** feel like we've kind of settled into like a fairly stable AI economy where we

**[22:50]** like a fairly stable AI economy where we

**[22:50]** like a fairly stable AI economy where we have like the model layer companies and

**[22:51]** have like the model layer companies and

**[22:51]** have like the model layer companies and the application layer companies and seem

**[22:53]** the application layer companies and seem

**[22:53]** the application layer companies and seem and the infrastructure layer companies

**[22:54]** and the infrastructure layer companies

**[22:54]** and the infrastructure layer companies seems like everyone is going to make a a

**[22:56]** seems like everyone is going to make a a

**[22:56]** seems like everyone is going to make a a lot of money and there's kind of like a

**[22:58]** lot of money and there's kind of like a

**[22:58]** lot of money and there's kind of like a relative playbook for how to build an AI


### [23:00 - 24:00]

**[23:00]** relative playbook for how to build an AI

**[23:00]** relative playbook for how to build an AI native company on top of the models. I

**[23:01]** native company on top of the models. I

**[23:01]** native company on top of the models. I feel like things really kind of matured

**[23:03]** feel like things really kind of matured

**[23:03]** feel like things really kind of matured in that way

**[23:04]** in that way

**[23:04]** in that way >> which feels is all downstream of like

**[23:05]** >> which feels is all downstream of like

**[23:05]** >> which feels is all downstream of like the models themselves have incrementally

**[23:07]** the models themselves have incrementally

**[23:07]** the models themselves have incrementally improved this year but there haven't

**[23:09]** improved this year but there haven't

**[23:09]** improved this year but there haven't been like major steps forward that have

**[23:11]** been like major steps forward that have

**[23:11]** been like major steps forward that have shaken everything up which is has a

**[23:13]** shaken everything up which is has a

**[23:13]** shaken everything up which is has a knock on effect. Many episodes ago we

**[23:14]** knock on effect. Many episodes ago we

**[23:14]** knock on effect. Many episodes ago we talked about how it was felt easier than

**[23:16]** talked about how it was felt easier than

**[23:16]** talked about how it was felt easier than ever to pivot and find a startup idea

**[23:18]** ever to pivot and find a startup idea

**[23:18]** ever to pivot and find a startup idea because if you could just survive if you

**[23:20]** because if you could just survive if you

**[23:20]** because if you could just survive if you could just wait a few months there was

**[23:21]** could just wait a few months there was

**[23:21]** could just wait a few months there was likely going to be some like big

**[23:23]** likely going to be some like big

**[23:23]** likely going to be some like big announcement that would completely make

**[23:25]** announcement that would completely make

**[23:25]** announcement that would completely make a new set of ideas possible and create

**[23:27]** a new set of ideas possible and create

**[23:27]** a new set of ideas possible and create more opportunities to build things. It

**[23:29]** more opportunities to build things. It

**[23:29]** more opportunities to build things. It certainly feels like that has slowed

**[23:30]** certainly feels like that has slowed

**[23:30]** certainly feels like that has slowed down and so like finding ideas is sort

**[23:33]** down and so like finding ideas is sort

**[23:33]** down and so like finding ideas is sort of returning to sort of normal levels of

**[23:35]** of returning to sort of normal levels of

**[23:35]** of returning to sort of normal levels of difficulty in my experience in office

**[23:37]** difficulty in my experience in office

**[23:37]** difficulty in my experience in office hours.

**[23:37]** hours.

**[23:37]** hours. >> I agree.

**[23:38]** >> I agree.

**[23:38]** >> I agree. >> I'll tell you what's not a surprise. Do

**[23:40]** >> I'll tell you what's not a surprise. Do

**[23:40]** >> I'll tell you what's not a surprise. Do you remember that report AI 2027 where

**[23:43]** you remember that report AI 2027 where

**[23:43]** you remember that report AI 2027 where it was just sort of like this doomer

**[23:44]** it was just sort of like this doomer

**[23:44]** it was just sort of like this doomer piece that said like oh well society is

**[23:46]** piece that said like oh well society is

**[23:46]** piece that said like oh well society is going to start falling apart in 2027.

**[23:49]** going to start falling apart in 2027.

**[23:49]** going to start falling apart in 2027. But you know at some point they quietly

**[23:50]** But you know at some point they quietly

**[23:50]** But you know at some point they quietly revised it to say that it wasn't 2027

**[23:53]** revised it to say that it wasn't 2027

**[23:53]** revised it to say that it wasn't 2027 but they kept the title. Maybe it's not

**[23:55]** but they kept the title. Maybe it's not

**[23:55]** but they kept the title. Maybe it's not a surprise. Like I was always a little

**[23:57]** a surprise. Like I was always a little

**[23:57]** a surprise. Like I was always a little bit of a skeptic. um of like this fast


### [24:00 - 25:00]

**[24:00]** bit of a skeptic. um of like this fast

**[24:00]** bit of a skeptic. um of like this fast takeoff argument because even with the

**[24:03]** takeoff argument because even with the

**[24:03]** takeoff argument because even with the scaling law, it is uh log linear. So it

**[24:07]** scaling law, it is uh log linear. So it

**[24:07]** scaling law, it is uh log linear. So it is slower. It requires like 10x more

**[24:10]** is slower. It requires like 10x more

**[24:10]** is slower. It requires like 10x more compute and it's still sort of, you

**[24:13]** compute and it's still sort of, you

**[24:13]** compute and it's still sort of, you know, topping out, right? And uh that's

**[24:15]** know, topping out, right? And uh that's

**[24:15]** know, topping out, right? And uh that's one form of good news. Another form of

**[24:17]** one form of good news. Another form of

**[24:18]** one form of good news. Another form of it's weird to call this good news, but

**[24:20]** it's weird to call this good news, but

**[24:20]** it's weird to call this good news, but human beings uh don't like change.

**[24:23]** human beings uh don't like change.

**[24:23]** human beings uh don't like change. in our previous episode where we sort of

**[24:25]** in our previous episode where we sort of

**[24:25]** in our previous episode where we sort of blew up that uh MIT report that said

**[24:27]** blew up that uh MIT report that said

**[24:27]** blew up that uh MIT report that said that you know 98% or 90% of uh

**[24:31]** that you know 98% or 90% of uh

**[24:31]** that you know 98% or 90% of uh enterprise AI projects fail. Well, it

**[24:33]** enterprise AI projects fail. Well, it

**[24:33]** enterprise AI projects fail. Well, it turns out that 90% of uh enterprises

**[24:36]** turns out that 90% of uh enterprises

**[24:36]** turns out that 90% of uh enterprises don't know how to do you know it, let

**[24:39]** don't know how to do you know it, let

**[24:39]** don't know how to do you know it, let alone AI. It's weird to say that that's

**[24:41]** alone AI. It's weird to say that that's

**[24:41]** alone AI. It's weird to say that that's a good thing, but in the context of fast

**[24:43]** a good thing, but in the context of fast

**[24:43]** a good thing, but in the context of fast takeoff, like that is a real break on

**[24:46]** takeoff, like that is a real break on

**[24:46]** takeoff, like that is a real break on the ability of this new really insane

**[24:49]** the ability of this new really insane

**[24:49]** the ability of this new really insane technology from actually permeating

**[24:51]** technology from actually permeating

**[24:51]** technology from actually permeating society. I love to accelerate but like

**[24:53]** society. I love to accelerate but like

**[24:53]** society. I love to accelerate but like it's weird to say like oh well actually

**[24:55]** it's weird to say like oh well actually

**[24:55]** it's weird to say like oh well actually in this case maybe that's a good thing

**[24:57]** in this case maybe that's a good thing

**[24:57]** in this case maybe that's a good thing right like it is a shockingly powerful

**[24:59]** right like it is a shockingly powerful


### [25:00 - 26:00]

**[25:00]** right like it is a shockingly powerful technology but you know between being

**[25:02]** technology but you know between being

**[25:02]** technology but you know between being log linear scaling and human beings

**[25:06]** log linear scaling and human beings

**[25:06]** log linear scaling and human beings really don't like change like

**[25:07]** really don't like change like

**[25:07]** really don't like change like organizationally speaking society will

**[25:10]** organizationally speaking society will

**[25:10]** organizationally speaking society will absorb this technology everyone will

**[25:13]** absorb this technology everyone will

**[25:13]** absorb this technology everyone will have enough time to sort of process it

**[25:15]** have enough time to sort of process it

**[25:15]** have enough time to sort of process it like culture will catch up governments

**[25:17]** like culture will catch up governments

**[25:18]** like culture will catch up governments will be able to respond to it not in

**[25:20]** will be able to respond to it not in

**[25:20]** will be able to respond to it not in like a frantic SP 1047 sort of like, you

**[25:24]** like a frantic SP 1047 sort of like, you

**[25:24]** like a frantic SP 1047 sort of like, you know, let's stop all the compute past 10

**[25:26]** know, let's stop all the compute past 10

**[25:26]** know, let's stop all the compute past 10 the 26, right? Like just these knee-jerk

**[25:29]** the 26, right? Like just these knee-jerk

**[25:29]** the 26, right? Like just these knee-jerk responses to technology. We're excited

**[25:31]** responses to technology. We're excited

**[25:31]** responses to technology. We're excited about um the ARC AGI prize is uh you

**[25:35]** about um the ARC AGI prize is uh you

**[25:35]** about um the ARC AGI prize is uh you know going to come in and do the winter

**[25:37]** know going to come in and do the winter

**[25:37]** know going to come in and do the winter 26 batch as a nonprofit. The funny thing

**[25:39]** 26 batch as a nonprofit. The funny thing

**[25:39]** 26 batch as a nonprofit. The funny thing about that is like yeah, maybe there's

**[25:42]** about that is like yeah, maybe there's

**[25:42]** about that is like yeah, maybe there's um a team right now that is climbing the

**[25:45]** um a team right now that is climbing the

**[25:45]** um a team right now that is climbing the leaderboard of ARC AGI and they're going

**[25:47]** leaderboard of ARC AGI and they're going

**[25:47]** leaderboard of ARC AGI and they're going to accelerate this thing again. And

**[25:49]** to accelerate this thing again. And

**[25:49]** to accelerate this thing again. And something that surprised me sort of

**[25:50]** something that surprised me sort of

**[25:50]** something that surprised me sort of relate to that with the startups is I

**[25:51]** relate to that with the startups is I

**[25:51]** relate to that with the startups is I remember around this time last year we

**[25:53]** remember around this time last year we

**[25:53]** remember around this time last year we were talking about how companies are

**[25:54]** were talking about how companies are

**[25:54]** were talking about how companies are getting to a million dollars AR and

**[25:56]** getting to a million dollars AR and

**[25:56]** getting to a million dollars AR and raising series A's without hiring like

**[25:58]** raising series A's without hiring like

**[25:58]** raising series A's without hiring like some cases not hiring anyone just the

**[25:59]** some cases not hiring anyone just the

**[25:59]** some cases not hiring anyone just the founders maybe hiring one person which


### [26:00 - 27:00]

**[26:01]** founders maybe hiring one person which

**[26:01]** founders maybe hiring one person which just felt very unusual. I feel like a

**[26:04]** just felt very unusual. I feel like a

**[26:04]** just felt very unusual. I feel like a year on

**[26:06]** year on

**[26:06]** year on that hasn't translated into okay and

**[26:08]** that hasn't translated into okay and

**[26:08]** that hasn't translated into okay and then they went and hit like 10 million

**[26:10]** then they went and hit like 10 million

**[26:10]** then they went and hit like 10 million AR or like they they scaled without

**[26:12]** AR or like they they scaled without

**[26:12]** AR or like they they scaled without adding any more people to

**[26:14]** adding any more people to

**[26:14]** adding any more people to >> No, they turned around and started and

**[26:16]** >> No, they turned around and started and

**[26:16]** >> No, they turned around and started and started hiring like actual teams.

**[26:17]** started hiring like actual teams.

**[26:17]** started hiring like actual teams. >> Yeah. like post series 8, it actually

**[26:20]** >> Yeah. like post series 8, it actually

**[26:20]** >> Yeah. like post series 8, it actually largely feels like the playbook is the

**[26:22]** largely feels like the playbook is the

**[26:22]** largely feels like the playbook is the same and the companies might be smaller

**[26:25]** same and the companies might be smaller

**[26:26]** same and the companies might be smaller for the same amount of revenue, but it

**[26:27]** for the same amount of revenue, but it

**[26:27]** for the same amount of revenue, but it feels it's entirely because they hit the

**[26:29]** feels it's entirely because they hit the

**[26:30]** feels it's entirely because they hit the revenue so fast and there's still just

**[26:31]** revenue so fast and there's still just

**[26:31]** revenue so fast and there's still just bottleneck on how long it takes to hire

**[26:33]** bottleneck on how long it takes to hire

**[26:33]** bottleneck on how long it takes to hire people versus they have demand for less

**[26:35]** people versus they have demand for less

**[26:35]** people versus they have demand for less people.

**[26:35]** people.

**[26:36]** people. >> I still think there is like a you know

**[26:37]** >> I still think there is like a you know

**[26:37]** >> I still think there is like a you know some effect but it is not like open and

**[26:40]** some effect but it is not like open and

**[26:40]** some effect but it is not like open and shut. It is not like you don't have to

**[26:42]** shut. It is not like you don't have to

**[26:42]** shut. It is not like you don't have to hire executives anymore. I think they're

**[26:44]** hire executives anymore. I think they're

**[26:44]** hire executives anymore. I think they're like there might be a case of two fuagra

**[26:47]** like there might be a case of two fuagra

**[26:47]** like there might be a case of two fuagra startups like one being Harvey and the

**[26:49]** startups like one being Harvey and the

**[26:49]** startups like one being Harvey and the other one being open evidence right

**[26:51]** other one being open evidence right

**[26:51]** other one being open evidence right Harvey the founders are incredible uh

**[26:53]** Harvey the founders are incredible uh

**[26:53]** Harvey the founders are incredible uh they were you know very early and then

**[26:56]** they were you know very early and then

**[26:56]** they were you know very early and then there's this sort of idea of like for

**[26:58]** there's this sort of idea of like for

**[26:58]** there's this sort of idea of like for VCs you could just go down Sand Hill


### [27:00 - 28:00]

**[27:00]** VCs you could just go down Sand Hill

**[27:00]** VCs you could just go down Sand Hill Road and like the fixes in like you just

**[27:02]** Road and like the fixes in like you just

**[27:02]** Road and like the fixes in like you just sort of block out all of them and then

**[27:04]** sort of block out all of them and then

**[27:04]** sort of block out all of them and then all the people you know there maybe 30

**[27:06]** all the people you know there maybe 30

**[27:06]** all the people you know there maybe 30 people who could write checks of like 10

**[27:08]** people who could write checks of like 10

**[27:08]** people who could write checks of like 10 to$und00 million and if you just sort of

**[27:11]** to$und00 million and if you just sort of

**[27:11]** to$und00 million and if you just sort of get all of their money like there's sort

**[27:12]** get all of their money like there's sort

**[27:12]** get all of their money like there's sort of no one who can actually come in and

**[27:15]** of no one who can actually come in and

**[27:15]** of no one who can actually come in and do the next series A and then basically

**[27:17]** do the next series A and then basically

**[27:17]** do the next series A and then basically you're safe like you have capital as a

**[27:19]** you're safe like you have capital as a

**[27:19]** you're safe like you have capital as a budgeon is capital as a moat in that

**[27:21]** budgeon is capital as a moat in that

**[27:21]** budgeon is capital as a moat in that case right so yeah Harvey is interesting

**[27:23]** case right so yeah Harvey is interesting

**[27:23]** case right so yeah Harvey is interesting because you know uh Lorra is coming fast

**[27:25]** because you know uh Lorra is coming fast

**[27:25]** because you know uh Lorra is coming fast for them and obviously we have some skin

**[27:27]** for them and obviously we have some skin

**[27:28]** for them and obviously we have some skin in the game on Lora but we think that

**[27:29]** in the game on Lora but we think that

**[27:29]** in the game on Lora but we think that they have uh as good a shot at any

**[27:32]** they have uh as good a shot at any

**[27:32]** they have uh as good a shot at any >> I guess that's one trend that we saw in

**[27:33]** >> I guess that's one trend that we saw in

**[27:33]** >> I guess that's one trend that we saw in 2025 is that there was like a first wave

**[27:35]** 2025 is that there was like a first wave

**[27:35]** 2025 is that there was like a first wave of like AI hative companies like Harvey

**[27:38]** of like AI hative companies like Harvey

**[27:38]** of like AI hative companies like Harvey >> who might have wasted a lot of money on

**[27:39]** >> who might have wasted a lot of money on

**[27:39]** >> who might have wasted a lot of money on finetuning actually

**[27:40]** finetuning actually

**[27:40]** finetuning actually >> totally that like broke out really in

**[27:43]** >> totally that like broke out really in

**[27:43]** >> totally that like broke out really in 2023 and kind of did a victory lap that

**[27:45]** 2023 and kind of did a victory lap that

**[27:45]** 2023 and kind of did a victory lap that you know oh we've won the the space and

**[27:47]** you know oh we've won the the space and

**[27:47]** you know oh we've won the the space and now we're seeing a second wave of

**[27:48]** now we're seeing a second wave of

**[27:48]** now we're seeing a second wave of companies like Lora and Giga and it

**[27:50]** companies like Lora and Giga and it

**[27:50]** companies like Lora and Giga and it turns out that like oh actually like it

**[27:53]** turns out that like oh actually like it

**[27:53]** turns out that like oh actually like it isn't so simple.

**[27:54]** isn't so simple.

**[27:54]** isn't so simple. >> Yeah. The weird beneficiary of um you

**[27:56]** >> Yeah. The weird beneficiary of um you

**[27:56]** >> Yeah. The weird beneficiary of um you know burning some non-trivial

**[27:59]** know burning some non-trivial

**[27:59]** know burning some non-trivial double-digit percentage of your capital


### [28:00 - 29:00]

**[28:01]** double-digit percentage of your capital

**[28:01]** double-digit percentage of your capital stack on fine-tuning that buys you no

**[28:04]** stack on fine-tuning that buys you no

**[28:04]** stack on fine-tuning that buys you no advantage is like basically the

**[28:05]** advantage is like basically the

**[28:06]** advantage is like basically the investors are the only winners there

**[28:07]** investors are the only winners there

**[28:07]** investors are the only winners there because they just own more of your

**[28:08]** because they just own more of your

**[28:08]** because they just own more of your company, you know.

**[28:09]** company, you know.

**[28:09]** company, you know. >> Yeah. Yeah, at least as it relates to

**[28:10]** >> Yeah. Yeah, at least as it relates to

**[28:10]** >> Yeah. Yeah, at least as it relates to like the the hiring and team size, I

**[28:12]** like the the hiring and team size, I

**[28:12]** like the the hiring and team size, I feel like of the two camps, one being

**[28:15]** feel like of the two camps, one being

**[28:15]** feel like of the two camps, one being the AI is going to make everything more

**[28:16]** the AI is going to make everything more

**[28:16]** the AI is going to make everything more efficient. You will need less people and

**[28:18]** efficient. You will need less people and

**[28:18]** efficient. You will need less people and the other AI is going to reduce the cost

**[28:20]** the other AI is going to reduce the cost

**[28:20]** the other AI is going to reduce the cost of like producing the time to produce

**[28:22]** of like producing the time to produce

**[28:22]** of like producing the time to produce things and so then the expectations from

**[28:24]** things and so then the expectations from

**[28:24]** things and so then the expectations from your users and customers will just go up

**[28:26]** your users and customers will just go up

**[28:26]** your users and customers will just go up and you'll need to keep hiring more

**[28:27]** and you'll need to keep hiring more

**[28:27]** and you'll need to keep hiring more people to satisfy like the growing

**[28:29]** people to satisfy like the growing

**[28:29]** people to satisfy like the growing expectations. I feel like this year has

**[28:31]** expectations. I feel like this year has

**[28:31]** expectations. I feel like this year has been more in that second camp and I

**[28:33]** been more in that second camp and I

**[28:33]** been more in that second camp and I think that is what's driving the fact

**[28:34]** think that is what's driving the fact

**[28:34]** think that is what's driving the fact that the companies are still just hiring

**[28:36]** that the companies are still just hiring

**[28:36]** that the companies are still just hiring as many people as they were preai. is

**[28:38]** as many people as they were preai. is

**[28:38]** as many people as they were preai. is just like the bar for what the soft what

**[28:40]** just like the bar for what the soft what

**[28:40]** just like the bar for what the soft what their customers expect and they're all

**[28:42]** their customers expect and they're all

**[28:42]** their customers expect and they're all in, you know, like Lora's racing with

**[28:44]** in, you know, like Lora's racing with

**[28:44]** in, you know, like Lora's racing with Harvey, Giga's racing with Sierra. Like

**[28:46]** Harvey, Giga's racing with Sierra. Like

**[28:46]** Harvey, Giga's racing with Sierra. Like they're all still competing for the same

**[28:49]** they're all still competing for the same

**[28:49]** they're all still competing for the same set of customers and they still

**[28:50]** set of customers and they still

**[28:50]** set of customers and they still ultimately are bottlenecked on like

**[28:52]** ultimately are bottlenecked on like

**[28:52]** ultimately are bottlenecked on like people and like I don't think anyone's

**[28:54]** people and like I don't think anyone's

**[28:54]** people and like I don't think anyone's bottlenecked on ideas, but they're

**[28:56]** bottlenecked on ideas, but they're

**[28:56]** bottlenecked on ideas, but they're bottlenecked on like people who can

**[28:57]** bottlenecked on like people who can

**[28:57]** bottlenecked on like people who can execute really well. I think that's like

**[28:59]** execute really well. I think that's like

**[28:59]** execute really well. I think that's like still it's exciting feels like an


### [29:00 - 30:00]

**[29:00]** still it's exciting feels like an

**[29:00]** still it's exciting feels like an exciting phase. I agree with you that

**[29:02]** exciting phase. I agree with you that

**[29:02]** exciting phase. I agree with you that like the uh era of the one person

**[29:06]** like the uh era of the one person

**[29:06]** like the uh era of the one person running a trillion dollar company is not

**[29:08]** running a trillion dollar company is not

**[29:08]** running a trillion dollar company is not here.

**[29:08]** here.

**[29:08]** here. >> Not yet.

**[29:09]** >> Not yet.

**[29:09]** >> Not yet. >> Yeah. But I think it's going to trend

**[29:11]** >> Yeah. But I think it's going to trend

**[29:11]** >> Yeah. But I think it's going to trend that way eventually. That'll be a wild

**[29:13]** that way eventually. That'll be a wild

**[29:13]** that way eventually. That'll be a wild time. Maybe that's a prediction for

**[29:15]** time. Maybe that's a prediction for

**[29:15]** time. Maybe that's a prediction for >> 2026. Yeah.

**[29:16]** >> 2026. Yeah.

**[29:16]** >> 2026. Yeah. >> You think it's coming in?

**[29:17]** >> You think it's coming in?

**[29:17]** >> You think it's coming in? >> I mean, I don't think it'll happen in

**[29:18]** >> I mean, I don't think it'll happen in

**[29:18]** >> I mean, I don't think it'll happen in 2026 either, honestly. I mean, I think

**[29:20]** 2026 either, honestly. I mean, I think

**[29:20]** 2026 either, honestly. I mean, I think you will have many stories of companies

**[29:24]** you will have many stories of companies

**[29:24]** you will have many stories of companies run by, you know, under a hundred people

**[29:26]** run by, you know, under a hundred people

**[29:26]** run by, you know, under a hundred people that are making hundreds of millions of

**[29:28]** that are making hundreds of millions of

**[29:28]** that are making hundreds of millions of dollars. So I mean Gamma was interesting

**[29:30]** dollars. So I mean Gamma was interesting

**[29:30]** dollars. So I mean Gamma was interesting to see like uh one of the biggest things

**[29:32]** to see like uh one of the biggest things

**[29:32]** to see like uh one of the biggest things that they said in their launch that I

**[29:34]** that they said in their launch that I

**[29:34]** that they said in their launch that I think is a very good trend is they said

**[29:36]** think is a very good trend is they said

**[29:36]** think is a very good trend is they said they got to hundred million dollars in

**[29:39]** they got to hundred million dollars in

**[29:39]** they got to hundred million dollars in ARR with only 50 employees. So which is

**[29:42]** ARR with only 50 employees. So which is

**[29:42]** ARR with only 50 employees. So which is a very different it's you know such an

**[29:43]** a very different it's you know such an

**[29:44]** a very different it's you know such an inversion right like normally you have

**[29:45]** inversion right like normally you have

**[29:45]** inversion right like normally you have the big banner and the like little X

**[29:48]** the big banner and the like little X

**[29:48]** the big banner and the like little X thing you know image and it's like oh

**[29:50]** thing you know image and it's like oh

**[29:50]** thing you know image and it's like oh yeah like we raised all this money and

**[29:52]** yeah like we raised all this money and

**[29:52]** yeah like we raised all this money and look at all the people who work for us.

**[29:54]** look at all the people who work for us.

**[29:54]** look at all the people who work for us. It's a good trend to have the reverse

**[29:56]** It's a good trend to have the reverse

**[29:56]** It's a good trend to have the reverse flex which is like look at all this

**[29:58]** flex which is like look at all this

**[29:58]** flex which is like look at all this revenue and look how few people work for


### [30:00 - 31:00]

**[30:00]** revenue and look how few people work for

**[30:00]** revenue and look how few people work for us. Well, that's all we have time for

**[30:02]** us. Well, that's all we have time for

**[30:02]** us. Well, that's all we have time for this time. We just wanted to wish you a

**[30:04]** this time. We just wanted to wish you a

**[30:04]** this time. We just wanted to wish you a really happy holidays and happy new year

**[30:06]** really happy holidays and happy new year

**[30:06]** really happy holidays and happy new year from all of us to you and yours. See you

**[30:09]** from all of us to you and yours. See you

**[30:09]** from all of us to you and yours. See you next time.


